---
marp: true
# footer: '**机器学习：第1章-概述**'
---
<!-- backgroundColor: light -->
# 《机器学习》课程简介
## 教师：肖宇
## 授课时间：
## 答疑时间：
## 助教：


---
# 第1章内容提要
- 什么是机器学习？
- 机器学习的发展历程
- 机器学习和数据挖掘
- 机器学习和统计学
- 机器学习的分类
- 机器学习方法三要素
- 参考资料

---
# 什么是机器学习？
## 维基百科概念
>  机器学习是近20多年兴起的一门多领域交叉学科，涉及**概率论、统计学、逼近论、凸分析、算法复杂度理论**等多门学科。机器学习理论主要是设计和分析一些让计算机可以自动“学习”的算法。机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。因为学习算法中涉及了大量的统计学理论，机器学习与统计推断学联系尤为密切，也被称为**统计学习理论**。算法设计方面，机器学习理论关注可以实现的，行之有效的学习算法。很多推论问题属于无程序可循难度，所以部分的机器学习研究是开发容易处理的近似算法。

---
# 什么是机器学习？
## 机器学习的定义
>- 机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能。
>- 机器学习是用数据或以往的经验，以此优化计算机程序的性能标准。
>- A computer program is said to learn from **experience E** with respect to some class of **tasks T** and **performance measure P**, if its performance at tasks in T, as measured by P, improves with experience E.

---
# 什么是机器学习？
## 机器学习的应用
- 数据挖掘
- 计算机视觉
- 自然语言处理
- 生物特征识别
- 搜索引擎
- 医学诊断
- 检测信用卡欺诈
- 语音和手写识别
- 机器人
- ...

---
# 机器学习的发展历程
## “黑暗时代”，人工智能的诞生（1943年~1956年）
- Warren McCulloch和Walter Pitts在1943年发表了人工智能领域的开篇之作，提出了人工神经网络模型。
- John von Neumann。他在1930年加入了普林斯顿大学，在数学物理系任教，和阿兰·图灵是同事。
- Marvin Minsky和Dean Edmonds建造第一台神经网络计算机。
- 1956年：John McCarthy从普林斯顿大学毕业后去达特茅斯学院工作，说服了Marvin Minsky和Claude Shannon在达特茅斯学院组织一个暑期研讨会，召集了对机器智能、人工神经网络和自动理论感兴趣的研究者，参加由IBM赞助的研讨会。

---
# 机器学习的发展历程
![width:800px](pictures/1.1.jpg)

---
# 机器学习的发展历程
## 新的方向：
- 集成学习
- 可扩展机器学习（对大数据集、高维数据的学习等）
- 强化学习
- 迁移学习
- 概率网络
- 深度学习

---
# 机器学习和数据挖掘
- 机器学习是数据挖掘的重要工具。
- 数据挖掘不仅仅要研究、拓展、应用一些机器学习方法，还要通过许多非机器学习技术解决数据仓储、大规模数据、数据噪音等等更为实际的问题。
- 机器学习的涉及面更宽，常用在数据挖掘上的方法通常只是“从数据学习”，然则机器学习不仅仅可以用在数据挖掘上，一些机器学习的子领域甚至与数据挖掘关系不大，例如增强学习与自动控制等等。
- 数据挖掘试图从海量数据中找出有用的知识。
- 大体上看，数据挖掘可以视为机器学习和数据库的交叉，它主要利用机器学习界提供的技术来分析海量数据，利用数据库界提供的技术来管理海量数据。

---
# 机器学习和数据挖掘
![width: 900px](/pictures/1.3.jpg)

---
# 为什么要研究大数据机器学习？
## 例如: “尿布→啤酒”关联规则
> 实际上，在面对少量数据时关联分析并不难，可以直接使用统计学中有关相关性的知识，这也正是机器学习界没有研究关联分析的一个重要原因。关联分析的困难其实完全是由海量数据造成的，因为数据量的增加会直接造成挖掘效率的下降，当数据量增加到一定程度，问题的难度就会产生质变，例如，在关联分析中必须考虑因数据太大而无法承受多次扫描数据库的开销、可能产生在存储和计算上都无法接受的大量中间结果等。

---
# 机器学习和统计学
## 机器学习
>机器学习(machine learning)致力于研究通过**计算**的手段，利用经验改善系统自身的性能。在计算机系统中，“经验”通常以“数据”形式存在，因此，机器学习所研究的主要内容，是关于在计算机上从数据中产生“模型”（model）的算法，即“学习算法”。如果说计算机科学是研究关于“算法”的学问，那么类似地，可以说机器学习是研究关于“学习算法”的学问。（周志华，《机器学习》，p1）
## 统计学习
>统计学习(statistical learning)是关于**计算机**基于**数据**构建**概率统计模型**并运用模型对数据进行预测与分析的一门学科。统计学习也称为统计机器学习。(李航，《统计学习方法》，p1)

---
# 机器学习和统计学

## 机器学习
>强调了通过机器设备（如计算机）进行学习以提升系统，由机器实施“计算”，因此重点在于“算法”。

## 统计学
>强调了通过统计理论与模型（如线性回归）学习以提升系统，重点在于“模型”。

## 两者是统一的
>都是为了构建一个可通过经验进行自我提升的系统，“模型”和“算法”都是这个过程中涉及的不同或缺组成部分。

---
# 机器学习和统计学
|machine learning| statistics |
|---:|---:|
|instance | data point |
|feature | covariate|
|label | response|
|weights | parameters|
|learning | fitting/estimation |
|supervised learning | regression/classification|
|unsupervised learning | density estimation/clustering |

---
# 机器学习和统计学
Simon Blomberg:
>From R’s fortunes package: To paraphrase provocatively, ‘machine learning is statistics minus any checking of models and assumptions’.

Andrew Gelman:
>In that case, maybe we should get rid of checking of models and assumptions more often. Then maybe we’d be able to solve some of the problems that the machine learning people can solve but we can’t!

---
# 机器学习和统计学
- 研究方法差异
  - 统计学研究形式化和推导
  - 机器学习更容忍一些新方法
- 维度差异
  - 统计学强调低维空间问题的统计推导（confidence intervals, hypothesis tests, optimal estimators）
  - 机器学习强调高维预测问题
- 统计学和机器学习各自更关心的领域：
  - 统计学: survival analysis, spatial analysis, multiple testing, minimax theory, deconvolution,  semiparametric inference, bootstrapping, time series.
  - 机器学习: online learning, semisupervised learning, manifold learning, active learning, boosting.

---
# 机器学习的分类
机器学习是一个范围宽阔、内容繁多、应用广泛的领域，并不存在一个统一的理论体系涵盖所有内容。下面从几个角度对机器学习方法进行分类。
## 基本分类
- 监督学习
- 无监督学习
- 强化学习
- 半监督学习
- 主动学习

---
# 机器学习的分类
## 按模型分类
- 概率模型与非概率（确定性）模型
  - 概率模型$P(y|x)$: 决策树、朴素贝叶斯、隐马儿可夫模型、条件随机场、概率潜在语义分析、潜在狄利克雷分配、高斯混合模型
  - 确定性模型$y=f(x)$: 感知机、支持向量机、k近邻、AdaBoost、k均值、潜在与意义分析、神经网络
  - Logit回归即可看作概率模型，又可看作确定性模型
  - 两类模型的区别在于模型的内在结构，而不在于映射关系。概率模型一定可以表示为联合概率分布的形式，而针对非概率模型则不一定存在这一的联合概率分布

---
# 机器学习的分类
## 按模型分类
- 线性模型与非线性模型
  - 如果函数$y=f(x)$是线性函数，则称模型为线性模型，否则称模型为非线性模型。
  - 线性模型：感知机、线性支持向量机、k近邻、k均值、潜在语义分析
  - 非线性模型：核函数支持向量机、AdaBoost、神经网络
- 参数化模型和非参数化模型
  - 参数化模型假设模型参数的维度不变，模型可由有限维参数完全刻画；非参数化模型参数维度不定或无穷大，随着训练数据量的增加而增大。
  - 参数化模型：感知机、朴素贝叶斯、Logit回归、高斯混合模型
  - 非参数化模型：决策树、支持向量机、AdaBoost、k近邻、潜在语义分析、概率潜在语义分析、潜在狄利克雷分配

---
# 机器学习的分类
## 按算法分类
- 在线学习
- 批量学习

## 按技巧分类
- 贝叶斯学习
- 核方法

---
# 监督学习
**(1) 输入空间、特征空间和输出空间**
- 输入实例$x$的特征向量：
$$
x=(x^{(1)},x^{(2)},x^{(3)}, ..., x^{(n)})^T
$$
注意$x^{(i)}$与$x_i$的区别，后者表示多个特征变量的第i个
$$
x_i=(x_i^{(1)},x_i^{(2)},...,x_i^{(n)})^T
$$
- 训练集
$$
T=\{(x_1,y_1),(x_2, y_2),..., (x_N,y_N)\}
$$
- 输入变量和输出变量：分类问题、回归问题、标注问题

---
# 监督学习
**(2) 联合概率分布**
- 假设输入与输出的随机变量$X$和$Y$遵循联合概率分布$P(X,Y)$
- $P(X,Y)$为分布函数或分布密度函数
- 对于学习系统来说，联合概率分布是未知的
- 训练数据和测试数据被看作是依联合概率分布$P(X,Y)$独立同分布产生的。

**(3) 假设空间**
- 监督学习目的是学习一个由输入到输出的映射，称为模型
- 模式的集合就是假设空间（`hypothesis space`）
- 概率模型:条件概率分布$P(Y|X)$, 决策函数：$Y=f(X)$

---
# 监督学习
**(4) 问题的形式化**
给定一个训练集
$$
T=\{(x_1,y_1),(x_2, y_2),..., (x_N,y_N)\}
$$
其中$(x_i, y_i), i=1,2,...,N$称为样本或样本点。$x_i\in \chi \in \mathbb{R^n}$是输入的观测值，也称为输入或实例，$y_i\in \mathbf{y}$ 是输出的观测值，也称为输出。

监督学习分为**学习**和**预测**两个过程，分别由学习系统和预测系统完成：
- 学习系统基于给定的训练数据集和一定的准则训练得到条件概率分布$\hat{P}(Y|X)$或决策函数$Y=\hat{f}(X)$以描述输入与输出随机变量之间的映射关系。
- 预测系统基于学习得到的概率分布（或决策函数）和输入$x_{j}$得到对应的输出$y_{j}$。

---
# 监督学习
在监督学习中，假设训练数据和测试数据是依联合概率分布$P(X,Y)$独立同分布产生的。

学习系统试图通过训练集中的样本$(x_i, y_i)$带来的信息学习模型。对输入$x_i$，一个具体的模型$y=f(x)$可以产生一个输出$f(x_i)$，而训练数据集中对应的输出是$y_i$。如果这个模型有很好地预测能力，训练样本输出$y_i$和模型输出$f(x_i)$之间的差应该足够小。

学习系统通过不断地尝试，选取最好的模型，以便对训练数据集有足够好的预测，同时对未知的测试数据集的预测也有尽可能好的推广。

监督学习包括：回归和分类

---
# 无监督学习
无监督学习是指从无标注数据中学习预测模型的机器学习问题，本质是学习数据中的统计规律或潜在结构。

模型的输入和输出的所有可能取值的集合分别称为输入空间和输出空间。输入空间和输出空间可以是有限元素集合，也可以是欧式空间。

每个输入是一个实例，由特征向量表示。每一个输出是对输入的分析结果，由输入的类别、转换或概率表示。模型可以实现对数据的聚类、降维和概率估计。

---
# 无监督学习
假设$\chi$是输入空间，$\Zeta$是隐式结构空间。要学习的模型可以表示为函数$z=g(x)$，条件概率分布$P(z|x)$，或者条件概率分布$P(x|z)$的形式，其中$x\in \chi$是输入，$z\in \Zeta$是输出。

包含所有可能的模型的集合称为假设空间。无监督学习旨在从假设空间中学出在给定评价标准下的最优模型。

---
# 无监督学习
![width: 900px](/pictures/1.4.jpg)

---
# 无监督学习
在学习过程中，学习系统从训练数据集学习得到一个最优模型，表示为函数$z=\hat{g}(x)$，条件概率分布$\hat{P}(z|x)$或者条件概率分布$\hat{P}(x|z)$。在预测过程中，预测系统对于给定的输入$x_{N+1}$，由模型$z_{N+1}=\hat{g}(x_{N+1})$或者$z_{N+1}=\text{arg max }\hat{P}(z|x_{N+1})$给出相应的输出$z_{N+1}$，进行聚类或降维；或由模型$\hat{P}(x|z)$给出输入的概率$\hat{P}(x_{N+1}|z_{N+1})$，进行概率估计。

---
# 强化学习
强化学习(reinforcement learning)是指智能系统在于环境的连续互动中学习最优行为策略的机器学习问题。强化学习的本质是学习最优的序贯决策。

在每一步$t$，智能系统从环境中观测一个状态$s_t$与一个奖励$r_t$，采取一个动作$a_t$。环境根据智能系统的选择的动作，决定$t+1$的$s_{t+1}$和$r_{t+1}$。要学习的策略表示为给定的状态下采取的动作。

注意，智能系统的目标是长期奖励的最大化。强化学习过程中，系统不断地试错，以达到学习最优策略的目的。

---
# 强化学习
强化学习的马儿可夫决策过程是状态、奖励、动作序列上的随机过程，有五元组$<S,A,P,r,\gamma>$组成。
- $S$是有限状态的集合
- $A$是有限动作的集合
- $P$是状态转移概率函数: $P(s'|s,a)=P(s_{t+1}=s'|s_t=s,a_t=a)$
- $r$是奖励函数: $r(s,a)=E(r_{t+1}|s_t=s,a_t=a)$
- $\gamma$是衰减系数: $\gamma\in [0,1]$

> 马儿可夫决策过程具有马儿可夫性，下一个状态只依赖于前一个状态和动作，由$P(s'|s,a)$表示。下一个奖励依赖于前一个状态与动作，有奖励函数$r(s,a)$表示。

---
# 强化学习
- 马儿可夫策略$\pi$定义为给定状态下动作的函数$a=f(s)$或者条件概率分布$P(a|s)$

- 价值函数定义为策略$\pi$从某一个状态$s$开始的长期累积奖励的数学期望:
$$
v_{\pi}(s)=E_{\pi}\left[r_{t+1}+\gamma r_{t+2}+\gamma^2r_{t+3}+...|s_t=s \right]
$$

- 动作价值函数定义为策略$\pi$的从某一个状态$s$和动作$a$开始的长期累积奖励的属性期望：
$$
q_{\pi}(s,a)=E_{\pi}\left[r_{t+1}+\gamma r_{t+2}+\gamma^2r_{t+3}+...|s_t=s, a_t=a \right]
$$

>强化学习的目标是在所有可能的策略中选出价值函数最大的策略$\pi^*$，而在实际学习中往往从具体的策略出发，不断优化已有策略。

---
# 强化学习
强化学习方法包含无模型方法和有模型方法，无模型的方法又包括基于策略的和基于价值的方法。
- 无模型
  - 基于策略的: 试图求解最优策略$\pi^*$，表示为函数$a=f^*(s)$或者条件概率分布$P^*(a|s)$。学习通常从一个具体策略开始，通过搜索更优的策略进行。
  - 基于价值的: 试图求解最优价值函数，特别是最优动作价值函数$q^{*}(s,a)$。学习通常从一个具体价值函数开始，通过搜索更优的价值函数进行。
- 有模型
  - 通过学习马尔可夫决策过程的模型，包括转移概率函数和奖励函数
  - 通过模型对环境的反馈进行预测
  - 求解价值函数最大的策略$\pi^*$

---
# 半监督学习
- 少量标注数据，大量未标注数据
- 利用未标注数据的信息，辅助标注数据，进行监督学习
- 较低成本

# 主动学习
- 机器主动给出实例，教师进行标注
- 利用标注数据学习预测模型


---
# 贝叶斯学习
在概率模型的学习和推理中，利用贝叶斯定理，计算在给定数据条件下模型的条件概率，即后验概率，并应用这个原理进行模型的估计，以及对数据的预测。将模型、未观测要素及其参数用变量表示，使用模型的先验分布是贝叶斯学习的特点。

假设随机变量D表示数据，随机变量$\theta$表示模型参数。根据贝叶斯定理
$$
P(\theta|D)=\frac{P(\theta)P(D|\theta)}{P(d)}
$$
其中$P(\theta)$是先验概率，$P(D|\theta)$是似然函数。

模型估计时，估计整个后验分布$P(\theta|D)$。如果需要给出一个模型，通常取后验概率最大的模型。预测时，计算数据对后验概率分布的期望值：
$$
P(x|D)=\int P(X|\theta,D)P(\theta|D)d \theta
$$

---
# 核方法
核方法是使用核函数表示和学习非线性模型，将线性模型学习方法扩展到非线性模型的一种机器学习方法，可以用于监督学习和无监督学习。

注意，核方法不显式地定义输入空间到特征空间的映射，而是直接定义核函数，即映射之后在特征空间的内积。

假设$x_1,x_2$是输入空间的任意两个实例，内积为$\left<x_1, x_2\right>$，输入空间到特征空间的映射为$φ$，核方法在输入空间中定义核函数 $K(x_1, x_2)$，使其满足 $K(x_1,x_2) = \left<φ(x_1), φ(x_2)\right>$

核函数支持向量机、核PCA、核k均值均属于核方法。



---
# 机器学习方法三要素
机器学习方法都是由模型、策略和算法构成的，可以简单地表示为：
$$方法=模型+策略+算法$$

---
# 机器学习方法三要素

## 1. 模型


---
# 学习资源
1. 李航，《统计学习方法》，第二版
2. 周志华，《机器学习》
3. [斯坦福机器学习](http://v.163.com/special/opencourse/machinelearning.html)
4. CMU 机器学习课程
   - http://www.cs.cmu.edu/~epxing/Class/10715/
   - http://www.cs.cmu.edu/~epxing/Class/10708/
   - http://www.cs.cmu.edu/~epxing/Class/10701