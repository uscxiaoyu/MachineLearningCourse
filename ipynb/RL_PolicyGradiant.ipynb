{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Filter deprecation warnings that are not critical (specifically for np.bool8)\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、带基线的`REINFORCE`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(REINFORCEModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 128)\n",
    "        self.actor = nn.Linear(128, 2)  # 2个动作\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        action_probs = F.softmax(self.actor(x), dim=-1)\n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCETrainer:\n",
    "    def __init__(self, env, model, gamma=0.99, lr=3e-3, max_steps=1000, update_target_interval=10):\n",
    "        self.env = env  # gym环境\n",
    "        self.model = model  # 策略网络\n",
    "        self.target_model = copy.deepcopy(model)  # 创建目标网络的深拷贝\n",
    "        self.target_model.load_state_dict(model.state_dict())  # 将参数复制到目标网络\n",
    "        self.target_model.eval()\n",
    "        self.gamma = gamma\n",
    "        self.max_steps = max_steps\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=lr)  # 优化器\n",
    "        self.update_target_interval = update_target_interval  # 更新目标网络的间隔\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            saved_log_probs = []  # 保存log概率\n",
    "            rewards = []  # 保存奖励\n",
    "            for _ in range(self.max_steps):  # model和环境交互max_steps步\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                action_probs = self.model(state_tensor)\n",
    "                m = torch.distributions.Categorical(action_probs)\n",
    "                action = m.sample()\n",
    "                saved_log_probs.append(m.log_prob(action))\n",
    "                \n",
    "                state, reward, done, _, _ = self.env.step(action.item())\n",
    "                rewards.append(reward)\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            R = 0\n",
    "            policy_loss = []\n",
    "            returns = []\n",
    "            for r in reversed(rewards):  # 计算每一步的回报\n",
    "                R = r + self.gamma * R\n",
    "                returns.insert(0, R)\n",
    "            \n",
    "            returns = torch.tensor(returns)  # 计算回报的均值和标准差\n",
    "            returns = (returns - returns.mean()) / (returns.std() + 1e-5)  # 防止除0\n",
    "            for log_prob, R in zip(saved_log_probs, returns):  # 计算损失\n",
    "                policy_loss.append(-log_prob * R)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            policy_loss = torch.cat(policy_loss).sum()  # 求和\n",
    "            policy_loss.backward()  # 反向传播\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if (episode + 1) % self.update_target_interval == 0:\n",
    "                self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "            if (episode + 1) % 100 == 0:\n",
    "                print(f'Episode {episode + 1}: Last total rewards: {sum(rewards)}')\n",
    "\n",
    "    def evaluate(self, num_episodes=10):\n",
    "        total_rewards = []\n",
    "        for i in range(num_episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            for _ in range(self.max_steps):\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                with torch.no_grad():\n",
    "                    action_probs = self.model(state_tensor)\n",
    "                    \n",
    "                action = torch.argmax(action_probs).item()\n",
    "                state, reward, done, _, _ = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "            total_rewards.append(episode_reward)\n",
    "            print(f'Episode {i + 1}: Total Reward = {episode_reward}')\n",
    "        return sum(total_rewards)/len(total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100: Last total rewards: 195.0\n",
      "Episode 200: Last total rewards: 1000.0\n",
      "Episode 300: Last total rewards: 1000.0\n",
      "Episode 400: Last total rewards: 147.0\n",
      "Episode 500: Last total rewards: 1000.0\n",
      "Episode 600: Last total rewards: 1000.0\n",
      "Episode 700: Last total rewards: 1000.0\n",
      "Episode 800: Last total rewards: 1000.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "model = REINFORCEModel()\n",
    "trainer = REINFORCETrainer(env, model, update_target_interval=25)\n",
    "trainer.train(num_episodes=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 1000.0\n",
      "Episode 2: Total Reward = 1000.0\n",
      "Episode 3: Total Reward = 1000.0\n",
      "Episode 4: Total Reward = 1000.0\n",
      "Episode 5: Total Reward = 1000.0\n",
      "Episode 6: Total Reward = 1000.0\n",
      "Episode 7: Total Reward = 1000.0\n",
      "Episode 8: Total Reward = 1000.0\n",
      "Episode 9: Total Reward = 1000.0\n",
      "Episode 10: Total Reward = 1000.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(num_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、`A2C`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要使用A2C（Advantage Actor-Critic）算法来训练控制CartPole游戏的智能体，我们首先需要建立一个Actor-Critic网络架构。A2C算法结合了策略梯度方法和值函数方法的优势，使用一个策略网络（Actor）来选择动作，同时使用一个价值网络（Critic）来评估采取某个动作后的状态价值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "以下是使用PyTorch实现A2C来控制CartPole游戏的完整步骤：\n",
    "\n",
    "\n",
    "### 1. 构建Actor-Critic网络\n",
    "\n",
    "我们将创建一个网络，它有两个输出头：一个用于Actor，输出动作概率；另一个用于Critic，输出状态值评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 128)\n",
    "        self.actor = nn.Linear(128, 2)  # 2个动作\n",
    "        self.critic = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        action_probs = F.softmax(self.actor(x), dim=-1)\n",
    "        state_values = self.critic(x)\n",
    "        return action_probs, state_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 定义训练过程\n",
    "\n",
    "在A2C的训练过程中，我们需要计算优势函数来更新策略，并同时更新价值函数。\n",
    "\n",
    "- 不使用目标网络\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticTrainer:\n",
    "    def __init__(self, env, model, gamma=0.99, lr=3e-3, max_steps=1000):\n",
    "        self.env = env\n",
    "        self.model = model  # 策略网络\n",
    "        self.gamma = gamma  # 折扣因子\n",
    "        self.max_steps = max_steps  # 最大步数\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=lr)  # 优化器\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            log_probs = []\n",
    "            values = []\n",
    "            rewards = []\n",
    "            masks = []\n",
    "            entropy = 0\n",
    "            step = 0\n",
    "            while True:  # 利用策略网络控制游戏，完成一个回合交互\n",
    "                if state is None or len(state) == 0:\n",
    "                    print(\"State is empty or None, breaking the loop.\")\n",
    "                    break\n",
    "                \n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)  # 确保state非空\n",
    "                probs, value = self.model(state_tensor)  # 获取策略网络概率分布和状态值\n",
    "                m = torch.distributions.Categorical(probs)  # 根据策略生成动作概率分布\n",
    "                action = m.sample()  # 采样动作\n",
    "                next_state, reward, done, _, _ = self.env.step(action.item())  # 执行动作\n",
    "\n",
    "                log_prob = m.log_prob(action)  # 计算log概率\n",
    "                entropy += m.entropy().mean()  # 计算累计熵\n",
    "\n",
    "                log_probs.append(log_prob)  # 记录log概率\n",
    "                values.append(value)  # 记录状态值\n",
    "                rewards.append(torch.tensor([reward], dtype=torch.float32))  # 记录奖励\n",
    "                masks.append(torch.tensor([1 - done], dtype=torch.float32))  # 记录mask\n",
    "\n",
    "                state = next_state  # 更新状态\n",
    "                step += 1\n",
    "                if done or step >= self.max_steps:  # 如果回合结束或者超过最大步数，结束\n",
    "                    break\n",
    "\n",
    "            # 玩完一个回合，再更新策略网络\n",
    "            Qvals = []  # 计算Q值\n",
    "            Qval = 0  # 初始化Q值\n",
    "            for r, m in zip(reversed(rewards), reversed(masks)):  # 从后往前计算Q值\n",
    "                Qval = r + self.gamma * Qval * m  # hay_y计算公式\n",
    "                Qvals.insert(0, Qval)  # 插入到Q值列表的第一个位置\n",
    "            \n",
    "            Qvals = torch.cat(Qvals).detach()  # 转换为张量\n",
    "            log_probs = torch.cat(log_probs)  # log_prob转换为张量\n",
    "            values = torch.cat(values)  # values转换为张量\n",
    "            \n",
    "            advantage = Qvals - values  # 计算优势\n",
    "\n",
    "            # Calculate losses\n",
    "            actor_loss = -(log_probs * advantage.detach()).mean()  # 计算actor损失\n",
    "            critic_loss = advantage.pow(2).mean()  # 计算critic损失\n",
    "\n",
    "            ac_loss = actor_loss + critic_loss - 0.001 * entropy  # 计算总损失\n",
    "\n",
    "            # Perform backprop\n",
    "            self.optimizer.zero_grad()  # 梯度清零\n",
    "            ac_loss.backward()  # 反向传播\n",
    "            self.optimizer.step()  # 更新参数\n",
    "\n",
    "            if (episode + 1) % 100 == 0:  # 每100个episode打印一次\n",
    "                print(f'Episode {episode + 1}: Last total rewards: {torch.tensor(rewards).sum().item()}')\n",
    "\n",
    "    def evaluate(self, num_episodes=10):\n",
    "        '''\n",
    "        计算num_episodes个回合的平均奖励\n",
    "        '''\n",
    "        total_rewards = []  # 记录每个episode的总奖励\n",
    "        for i in range(num_episodes):\n",
    "            state, _ = self.env.reset()  # 重置环境\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            step = 0\n",
    "            while not done:\n",
    "                state = torch.FloatTensor(state).unsqueeze(0)\n",
    "                with torch.no_grad():\n",
    "                    probs, _ = self.model(state)\n",
    "                    \n",
    "                action = torch.argmax(probs).item()\n",
    "                state, reward, done, _, _ = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "                step += 1\n",
    "                if step >= self.max_steps:\n",
    "                    break\n",
    "                \n",
    "            total_rewards.append(episode_reward)\n",
    "            print(f'Episode {i + 1}: Total Reward = {episode_reward}')\n",
    "            \n",
    "        return sum(total_rewards)/len(total_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用目标网络训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticTrainerWithTargetNet:\n",
    "    def __init__(self, env, model, gamma=0.99, lr=3e-3, max_steps=1000, update_target_interval=10):\n",
    "        self.env = env\n",
    "        self.model = model\n",
    "        self.target_model = copy.deepcopy(model)  # 创建目标网络的深拷贝\n",
    "        self.target_model.load_state_dict(model.state_dict())\n",
    "        self.target_model.eval()\n",
    "        self.gamma = gamma\n",
    "        self.max_steps = max_steps\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        self.update_target_interval = update_target_interval  # 目标网络更新间隔\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            log_probs = []\n",
    "            values = []\n",
    "            rewards = []\n",
    "            masks = []\n",
    "            entropy = 0\n",
    "            step = 0\n",
    "            while True:\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                probs, value = self.model(state_tensor)\n",
    "                m = torch.distributions.Categorical(probs)\n",
    "                action = m.sample()\n",
    "                next_state, reward, done, _, _ = self.env.step(action.item())\n",
    "\n",
    "                log_prob = m.log_prob(action)\n",
    "                entropy += m.entropy().mean()\n",
    "\n",
    "                log_probs.append(log_prob)\n",
    "                values.append(value)\n",
    "                rewards.append(torch.tensor([reward], dtype=torch.float32))\n",
    "                masks.append(torch.tensor([1 - done], dtype=torch.float32))\n",
    "\n",
    "                state = next_state\n",
    "                step += 1\n",
    "                if done or step >= self.max_steps:\n",
    "                    break\n",
    "\n",
    "            Qvals = []\n",
    "            Qval = 0\n",
    "            for r, m in zip(reversed(rewards), reversed(masks)):\n",
    "                Qval = r + self.gamma * Qval * m\n",
    "                Qvals.insert(0, Qval)\n",
    "\n",
    "            Qvals = torch.cat(Qvals).detach()\n",
    "            values = torch.cat(values)\n",
    "            advantage = Qvals - values\n",
    "\n",
    "            actor_loss = -(torch.cat(log_probs) * advantage.detach()).mean()\n",
    "            critic_loss = advantage.pow(2).mean()\n",
    "            ac_loss = actor_loss + critic_loss - 0.001 * entropy\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            ac_loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if (episode + 1) % self.update_target_interval == 0:\n",
    "                self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "            if (episode + 1) % 100 == 0:\n",
    "                print(f'Episode {episode + 1}: Last total rewards: {sum(rewards).item()}')\n",
    "    \n",
    "    def evaluate(self, num_episodes=10):\n",
    "        '''\n",
    "        计算num_episodes个回合的平均奖励\n",
    "        '''\n",
    "        total_rewards = []  # 记录每个episode的总奖励\n",
    "        for i in range(num_episodes):\n",
    "            state, _ = self.env.reset()  # 重置环境\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            step = 0\n",
    "            while not done:\n",
    "                state = torch.FloatTensor(state).unsqueeze(0)\n",
    "                with torch.no_grad():\n",
    "                    probs, _ = self.model(state)\n",
    "                    \n",
    "                action = torch.argmax(probs).item()\n",
    "                state, reward, done, _, _ = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "                step += 1\n",
    "                if step >= self.max_steps:\n",
    "                    break\n",
    "                \n",
    "            total_rewards.append(episode_reward)\n",
    "            print(f'Episode {i + 1}: Total Reward = {episode_reward}')\n",
    "            \n",
    "        return sum(total_rewards)/len(total_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 运行训练过程\n",
    "\n",
    "创建环境和模型，然后启动训练。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 不使用目标网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoyu/miniforge3/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100: Last total rewards: 55.0\n",
      "Episode 200: Last total rewards: 688.0\n",
      "Episode 300: Last total rewards: 166.0\n",
      "Episode 400: Last total rewards: 1000.0\n",
      "Episode 500: Last total rewards: 917.0\n",
      "Episode 600: Last total rewards: 303.0\n",
      "Episode 700: Last total rewards: 233.0\n",
      "Episode 800: Last total rewards: 1000.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "model = ActorCritic()\n",
    "trainer = ActorCriticTrainer(env, model)\n",
    "trainer.train(num_episodes=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 1000.0\n",
      "Episode 2: Total Reward = 1000.0\n",
      "Episode 3: Total Reward = 1000.0\n",
      "Episode 4: Total Reward = 1000.0\n",
      "Episode 5: Total Reward = 1000.0\n",
      "Episode 6: Total Reward = 1000.0\n",
      "Episode 7: Total Reward = 1000.0\n",
      "Episode 8: Total Reward = 1000.0\n",
      "Episode 9: Total Reward = 1000.0\n",
      "Episode 10: Total Reward = 1000.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(num_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用目标网络训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoyu/miniforge3/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100: Last total rewards: 80.0\n",
      "Episode 200: Last total rewards: 77.0\n",
      "Episode 300: Last total rewards: 293.0\n",
      "Episode 400: Last total rewards: 246.0\n",
      "Episode 500: Last total rewards: 241.0\n",
      "Episode 600: Last total rewards: 301.0\n",
      "Episode 700: Last total rewards: 293.0\n",
      "Episode 800: Last total rewards: 809.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "model = ActorCritic()\n",
    "trainer = ActorCriticTrainerWithTargetNet(env, model, lr=5e-3, update_target_interval=25)\n",
    "trainer.train(num_episodes=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 1000.0\n",
      "Episode 2: Total Reward = 1000.0\n",
      "Episode 3: Total Reward = 1000.0\n",
      "Episode 4: Total Reward = 1000.0\n",
      "Episode 5: Total Reward = 1000.0\n",
      "Episode 6: Total Reward = 1000.0\n",
      "Episode 7: Total Reward = 1000.0\n",
      "Episode 8: Total Reward = 1000.0\n",
      "Episode 9: Total Reward = 1000.0\n",
      "Episode 10: Total Reward = 1000.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(num_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Actor-Critic方法中，Actor的目标函数 $L$ 经常被设计为期望的负对数概率乘以优势函数的形式。这个函数是：\n",
    "\n",
    " $$L(\\theta) = -\\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^T \\log \\pi_\\theta(a_t|s_t) \\cdot A^\\pi(s_t, a_t)\\right]$$\n",
    "\n",
    "其中，$\\pi_\\theta(a_t|s_t)$ 是由参数 $\\theta$ 确定的在给定状态 $s_t$ 下选择动作 $a_t$ 的策略，$A^\\pi(s_t, a_t)$ 是在状态 $s_t$ 下选择动作 $a_t$ 的优势函数。\n",
    "\n",
    "### 计算 $L$ 对 $\\theta$ 的导数\n",
    "\n",
    "要找到 $L$ 对策略参数 $\\theta$ 的导数（也称为策略梯度），我们可以使用链式法则。根据策略梯度定理，我们有：\n",
    "\n",
    "$$ \\nabla_\\theta L(\\theta) = -\\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot A^\\pi(s_t, a_t)\\right] $$\n",
    "\n",
    "进一步展开这个梯度的计算：\n",
    "\n",
    "1. **对数概率的梯度** $\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)$：\n",
    "   - 这是策略网络对其输出（动作概率）的敏感度的度量。使用链式法则，我们可以找到策略网络输出相对于其参数的梯度。\n",
    "   - 由于 $\\log \\pi_\\theta(a_t|s_t)$ 是对数似然，其对策略参数 $\\theta$ 的梯度可以直接通过自动微分（如PyTorch中的autograd）获得。\n",
    "\n",
    "2. **乘以优势函数** $A^\\pi(s_t, a_t)$：\n",
    "   - 优势函数衡量选择特定动作相对于平均情况的额外价值。它不依赖于参数 $\\theta$，因此在求导时仅作为乘法因子。\n",
    "\n",
    "因此，策略梯度实际上指示了如何调整策略参数 $\\theta$ 以增加获得高优势的动作的概率，同时减少获得低或负优势的动作的概率。这种更新方式是在学习过程中推动策略向着增加预期回报的方向演进。\n",
    "\n",
    "### 为什么使用 $-\\mathbb{E}[\\cdot]$\n",
    "\n",
    "由于我们通常使用梯度下降方法最小化损失函数，而策略梯度方法的目标是最大化总回报，因此在实际实现中，我们取目标函数的负值（即最大化 $\\mathbb{E}[\\cdot]$ 转换为最小化 $-\\mathbb{E}[\\cdot]$）。这样，就可以直接应用标准的优化算法（如SGD或Adam）来进行参数更新。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
