{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三. 对数几率回归（logit regression）\n",
    "考虑一个二分类任务，其生产标记$y\\in \\{0,1\\}$，而线性回归模型产生的预测值$z=\\mathbf{\\omega^Tx+b}$是实数，于是需将$z$转换为0/1值。直观地，可以考虑\"单位阶跃函数\"\n",
    "$$\n",
    "\\begin{equation}\n",
    "y=\\begin{cases}\n",
    "0,z<0;\\\\\n",
    "0.5,z=0;\\\\\n",
    "1,z>0.\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "即若预测值$z$大于0则判为正例。显然，单位阶越函数是不连续函数，因此退而使用有更好性质的对数几率函数（logistic function）:\n",
    "$$\n",
    "y=\\frac{1}{1+e^{-z}}.\n",
    "$$\n",
    "显然对数几率函数可以将z值转换为一个接近0或1的值，且在$z=0$附近变化很陡。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将$z=\\mathbf{\\omega^Tx+b}$代入对数几率函数，可得\n",
    "$$\n",
    "y=\\frac{1}{1+e^{-\\mathbf{\\omega^Tx+b}}}.\n",
    "$$\n",
    "进而转换为\n",
    "$$\n",
    "\\mathrm{ln}\\frac{y}{1-y}=\\mathbf{\\omega^Tx+b}.\n",
    "$$\n",
    "若将$y$视为样本$\\mathbf{x}$作为正例的可能性，则$1-y$是其反例可能性，两者比值为\n",
    "$$\n",
    "\\frac{y}{1-y}\n",
    "$$\n",
    "称为几率（odd），反映了x作为正例的相对可能性。对几率取自然对数则可得对数几率（log odds, 也称为logit）\n",
    "$$\n",
    "\\mathrm{ln}\\frac{y}{1-y}\n",
    "$$\n",
    "通过“极大似然法”来估计$\\omega$和$b$，给定数据集$\\{(x_i,y_i)\\}^m_{i=1}$，最大化对数似然率\n",
    "$$\n",
    "\\text{max  } \\mathbf{l(w,b)}=\\sum_{i=1}^m \\mathrm{ln}p(y_i|\\mathbf{x_i;w,b})\n",
    "$$\n",
    "即令每个样本属于其真实标记的概率越大越好。上式又等价于最小化负对数似然率\n",
    "$$\n",
    "(\\omega, b)^* = \\text{argmin  } \\mathbf{l(w,b)}=\\sum_{i=1}^m\\left(-y_i(\\omega^Tx_i+b)+\\mathbf{ln}(1+e^{\\omega^Tx_i+b})\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对应的矩阵形式为\n",
    "$$\n",
    "\\mathbf{-(X^T\\omega+b)^TY + \\left(\\ln(1+e^{X^T\\omega+b})\\right)^T1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython import display\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 垃圾邮件数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dataset/smsspamcollection/SMSSpamCollection', delimiter='\\t', header=None, names=['category', 'message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category                                            message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "垃圾邮件数量: 747 \n",
      "正常邮件数量: 4825 \n"
     ]
    }
   ],
   "source": [
    "print('垃圾邮件数量: %d ' % np.sum(df.category == 'spam'))\n",
    "print('正常邮件数量: %d ' % np.sum(df.category == 'ham'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 使用scikit-learn获取tfidf分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.message.values\n",
    "y = df.category.values\n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()  # tfidf值\n",
    "X_train = vectorizer.fit_transform(X_train_raw)\n",
    "X_test = vectorizer.transform(X_test_raw)\n",
    "# vectorizer.vocabulary_  # 词典 \n",
    "# vectorizer.get_feature_names()  # 特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression(solver='lbfgs')  # logit分类器\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: ham, True: ham, Message: Alright, we're all set here, text the man\n",
      "Predicted: ham, True: ham, Message: Yup. Thk of u oso boring wat.\n",
      "Predicted: ham, True: ham, Message: This weekend is fine (an excuse not to do too much decorating)\n",
      "Predicted: ham, True: ham, Message: If you hear a loud scream in about &lt;#&gt; minutes its cause my Gyno will be shoving things up me that don't belong :/\n",
      "Predicted: ham, True: ham, Message: * Am on a train back from northampton so i'm afraid not!\n",
      "Predicted: ham, True: ham, Message: I got it before the new year cos yetunde said she wanted to surprise you with it but when i didnt see money i returned it mid january before the  &lt;#&gt; day return period ended.\n",
      "Predicted: ham, True: ham, Message: Hi babe its me thanks for coming even though it didnt go that well!i just wanted my bed! Hope to see you soon love and kisses xxx\n",
      "Predicted: ham, True: ham, Message: I am in a marriage function\n",
      "Predicted: ham, True: ham, Message: 1Apple/Day=No Doctor. 1Tulsi Leaf/Day=No Cancer. 1Lemon/Day=No Fat. 1Cup Milk/day=No Bone Problms 3 Litres Watr/Day=No Diseases Snd ths 2 Whom U Care..:-)\n",
      "Predicted: ham, True: ham, Message: Yeah work is fine, started last week, all the same stuff as before, dull but easy and guys are fun!\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(X_test)\n",
    "for i, prediction in enumerate(predictions[:10]):\n",
    "    print('Predicted: %s, True: %s, Message: %s' % (prediction, y_test[i], X_test_raw[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(classifier.predict(X_train) != y_train)  # 训练集预测出错数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(classifier.predict(X_test) != y_test)  # 测试集预测出错数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(z):\n",
    "    if z < 0:\n",
    "        return 0\n",
    "    elif z == 0:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def g(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 基于`torch`实现Logit回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数的向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_f = np.vectorize(f)\n",
    "v_g = np.vectorize(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(-5, 5, num=200)\n",
    "y1 = v_f(z)\n",
    "y2 = v_g(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.set_matplotlib_formats('svg')\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(z, y1, 'r-', label='Heaviside')\n",
    "ax.plot(z, y2, 'g-', label='logit')\n",
    "ax.scatter([0], [0.5], s=50, alpha=0.5)\n",
    "ax.set_xlim([-5, 5])\n",
    "ax.set_xlabel(\"z\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmod函数\n",
    "def logit(w, x, b):\n",
    "    return torch.sigmoid(x@w + b)  # 1 / (1 + torch.exp(-(x@w + b)))\n",
    "\n",
    "# 负对数似然率函数\n",
    "def neglikelihood(x, y, w, b):\n",
    "    z = x@w + b\n",
    "    llike = -y.reshape(1, -1)@z + torch.ones_like(z).reshape(1, -1)@torch.log(1 + torch.exp(z))\n",
    "    return llike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(w, b, feature, true_label):\n",
    "    z = logit(w, feature, b)\n",
    "    y = (z >= 0.5).float()\n",
    "    return torch.sum(y == true_label).numpy() / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w = torch.FloatTensor([1, -2]).reshape(-1, 1)\n",
    "true_b = torch.FloatTensor([1])\n",
    "x = torch.randn(size=(1000, 2)).float()\n",
    "\n",
    "# 生成数据集\n",
    "z = logit(true_w, x, true_b)  # 为正例的概率\n",
    "y = z >= 0.5  # 生成True或False\n",
    "y = y.float()  # 注意要转换为浮点数，否则后面迭代时报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, neglikelihood: 413.5895\n",
      "    train accuracy: 0.9575, test accuracy: 0.965\n",
      "epoch 2, neglikelihood: 306.1644\n",
      "    train accuracy: 0.99375, test accuracy: 0.995\n",
      "epoch 3, neglikelihood: 257.0077\n",
      "    train accuracy: 1.0, test accuracy: 1.0\n",
      "epoch 4, neglikelihood: 227.7614\n",
      "    train accuracy: 1.0, test accuracy: 1.0\n",
      "epoch 5, neglikelihood: 207.6567\n",
      "    train accuracy: 1.0, test accuracy: 1.0\n",
      "epoch 6, neglikelihood: 192.8063\n",
      "    train accuracy: 1.0, test accuracy: 1.0\n",
      "epoch 7, neglikelihood: 181.2397\n",
      "    train accuracy: 1.0, test accuracy: 1.0\n",
      "epoch 8, neglikelihood: 171.8677\n",
      "    train accuracy: 1.0, test accuracy: 1.0\n",
      "epoch 9, neglikelihood: 164.0813\n",
      "    train accuracy: 1.0, test accuracy: 1.0\n",
      "epoch 10, neglikelihood: 157.4612\n",
      "    train accuracy: 1.0, test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "x_train = x[:int(len(x)*0.8)]\n",
    "x_test = x[int(len(x)*0.8):]\n",
    "\n",
    "y_train = y[:int(len(y)*0.8)]\n",
    "y_test = y[int(len(y)*0.8):]\n",
    "\n",
    "# 设置参数初始值\n",
    "w = torch.rand(size=true_w.shape)\n",
    "b = torch.FloatTensor([0.0])\n",
    "w.requires_grad_(True)\n",
    "b.requires_grad_(True)\n",
    "\n",
    "lr = 0.05\n",
    "num_epochs = 10\n",
    "batch_size = 10  # 构建10个批次的训练集\n",
    "dataset = TensorDataset(x_train, y_train)\n",
    "data_iter = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for t_x, t_y in data_iter:\n",
    "        l = neglikelihood(t_x, t_y, w, b)        \n",
    "        l.backward()  # 计算损失函数在 [w,b] 上的梯度\n",
    "        w.data.sub_(lr*w.grad/batch_size)\n",
    "        w.grad.data.zero_()\n",
    "        b.data.sub_(lr*b.grad/batch_size)\n",
    "        b.grad.data.zero_()\n",
    "        \n",
    "    with torch.no_grad():  # 不计算梯度，加速损失函数的运算\n",
    "        train_l = neglikelihood(x, y, w, b)  # 最近一次的负对数似然率\n",
    "        est_w = [u[0] for u in w.detach().numpy()]  # detach得到一个有着和原tensor相同数据的tensor\n",
    "        est_b = [v for v in b.detach().numpy()]\n",
    "        train_accu_ratio = precision(w, b, x_train, y_train)\n",
    "        test_accu_ratio = precision(w, b, x_test, y_test)\n",
    "        \n",
    "        print(f'epoch {epoch + 1}, neglikelihood: {train_l.numpy()[0][0]:.4f}')\n",
    "        print(f'    train accuracy: {train_accu_ratio}, test accuracy: {test_accu_ratio}')\n",
    "#         print(f'    w0: {est_w[0]:.4f}, w1: {est_w[1]:.4f},  b: {est_b[0]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于`LogisticRegression`验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression(solver='lbfgs')\n",
    "classifier.fit(x.data.numpy(), y.data.squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_y = classifier.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(p_y == y.data.squeeze().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
