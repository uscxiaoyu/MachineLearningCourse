{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\uscxi\\\\GitProjects\\\\MachineLearningCourse'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import os\n",
    "os.chdir(\"c://Users/uscxi/GitProjects/MachineLearningCourse/\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/smsspamcollection/SMSSpamCollection', delimiter='\\t', header=None, names=['category', 'message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "垃圾邮件数量: 747 \n",
      "正常邮件数量: 4825 \n"
     ]
    }
   ],
   "source": [
    "print('垃圾邮件数量: %d ' % np.sum(df.category == 'spam'))\n",
    "print('正常邮件数量: %d ' % np.sum(df.category == 'ham'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.message.values\n",
    "y = df.category.values\n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()  # tfidf值\n",
    "X_train = vectorizer.fit_transform(X_train_raw)\n",
    "X_test = vectorizer.transform(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_  # 词典 \n",
    "vectorizer.get_feature_names()  # 特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression(solver='lbfgs')\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: ham, True: ham, Message: Total disappointment, when I texted you was the craziest shit got :(\n",
      "Predicted: ham, True: ham, Message: I am in bus on the way to calicut\n",
      "Predicted: spam, True: spam, Message: U have a secret admirer. REVEAL who thinks U R So special. Call 09065174042. To opt out Reply REVEAL STOP. 1.50 per msg recd. Cust care 07821230901\n",
      "Predicted: ham, True: spam, Message: You have 1 new message. Call 0207-083-6089\n",
      "Predicted: ham, True: spam, Message: Congrats! 2 mobile 3G Videophones R yours. call 09061744553 now! videochat wid ur mates, play java games, Dload polyH music, noline rentl. bx420. ip4. 5we. 150pm\n",
      "Predicted: spam, True: spam, Message: U’ve Bin Awarded £50 to Play 4 Instant Cash. Call 08715203028 To Claim. EVERY 9th Player Wins Min £50-£500. OptOut 08718727870\n",
      "Predicted: ham, True: ham, Message: Hmm ok, i'll stay for like an hour cos my eye is really sore!\n",
      "Predicted: ham, True: ham, Message: I'm on da bus going home...\n",
      "Predicted: ham, True: ham, Message: Ok cool. See ya then.\n",
      "Predicted: ham, True: ham, Message: Oops sorry. Just to check that you don't mind picking me up tomo at half eight from station. Would that be ok?\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(X_test)\n",
    "for i, prediction in enumerate(predictions[:10]):\n",
    "    print('Predicted: %s, True: %s, Message: %s' % (prediction, y_test[i], X_test_raw[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(classifier.predict(X_train) != y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(classifier.predict(X_test) != y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二. 对数几率回归（logit regression）\n",
    "考虑一个二分类任务，其生产标记$y\\in \\{0,1\\}$，而线性回归模型产生的预测值$z=\\mathbf{\\omega^Tx+b}$是实数，于是需将$z$转换为0/1值。直观地，可以考虑\"单位阶跃函数\"\n",
    "$$\n",
    "\\begin{equation}\n",
    "y=\\begin{cases}\n",
    "0,z<0;\\\\\n",
    "0.5,z=0;\\\\\n",
    "1,z>0.\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "即若预测值$z$大于0则判为正例。显然，单位阶越函数是不连续函数，因此退而使用有更好性质的对数几率函数（logistic function）:\n",
    "$$\n",
    "y=\\frac{1}{1+e^{-z}}.\n",
    "$$\n",
    "显然对数几率函数可以将z值转换为一个接近0或1的值，且在$z=0$附近变化很陡。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(z):\n",
    "    if z < 0:\n",
    "        return 0\n",
    "    elif z == 0:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def g(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数的向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_f = np.vectorize(f)\n",
    "v_g = np.vectorize(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(-5, 5, num=200)\n",
    "y1 = v_f(z)\n",
    "y2 = v_g(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.set_matplotlib_formats('svg')\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(z, y1, 'r-', label='Heaviside')\n",
    "ax.plot(z, y2, 'g-', label='logit')\n",
    "ax.scatter([0], [0.5], s=50, alpha=0.5)\n",
    "ax.set_xlim([-5, 5])\n",
    "ax.set_xlabel(\"z\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将$z=\\mathbf{\\omega^Tx+b}$代入对数几率函数，可得\n",
    "$$\n",
    "y=\\frac{1}{1+e^{-\\mathbf{\\omega^Tx+b}}}.\n",
    "$$\n",
    "进而转换为\n",
    "$$\n",
    "\\mathrm{ln}\\frac{y}{1-y}=\\mathbf{\\omega^Tx+b}.\n",
    "$$\n",
    "若将$y$视为样本$\\mathbf{x}$作为正例的可能性，则$1-y$是其反例可能性，两者比值为\n",
    "$$\n",
    "\\frac{y}{1-y}\n",
    "$$\n",
    "称为几率（odd），反映了x作为正例的相对可能性。对几率取自然对数则可得对数几率（log odds, 也称为logit）\n",
    "$$\n",
    "\\mathrm{ln}\\frac{y}{1-y}\n",
    "$$\n",
    "通过“极大似然法”来估计$\\omega$和$b$，给定数据集$\\{(x_i,y_i)\\}^m_{i=1}$，最大化对数似然率\n",
    "$$\n",
    "\\text{max  } \\mathbb{l(w,b)}=\\sum_{i=1}^m \\mathrm{ln}p(y_i|\\mathbf{x_i;w,b})\n",
    "$$\n",
    "即令每个样本属于其真实标记的概率越大越好。上式又等价于最小化负对数似然率\n",
    "$$\n",
    "(\\omega, b)^* = \\text{argmin  } \\mathbb{l(w,b)}=\\sum_{i=1}^m\\left(-y_i(\\omega^Tx_i+b)+\\mathbf{ln}(1+e^{\\omega^Tx_i+b})\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对应的矩阵形式为\n",
    "$$\n",
    "\\mathbf{-(X^T\\omega+b)^TY + \\left(\\ln(1+e^{X^T\\omega+b})\\right)^T1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmod函数\n",
    "def logit(w, x, b):\n",
    "    return torch.sigmoid(x@w + b)  # 1 / (1 + torch.exp(-x@w - b))\n",
    "\n",
    "# 负对数似然率函数\n",
    "def neglikelihood(x, y, w, b):\n",
    "    z = x@w + b\n",
    "    llike = -y.reshape(1, -1)@z + torch.ones_like(z).reshape(1, -1)@torch.log(1 + torch.exp(z))\n",
    "    return llike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(w, b, feature, true_label):\n",
    "    z = logit(w, feature, b)\n",
    "    y = (z >= 0.5).float()\n",
    "    return torch.sum(y == true_label).numpy() / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w = torch.FloatTensor([1, -2]).reshape(-1, 1)\n",
    "true_b = torch.FloatTensor([1])\n",
    "x = torch.randn(size=(1000, 2)).float()\n",
    "\n",
    "# 生成数据集\n",
    "z = logit(true_w, x, true_b)  # 为正例的概率\n",
    "y = z >= 0.5  # 生成True或False\n",
    "y = y.float()  # 注意要转换为浮点数，否则后面迭代时报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, neglikelihood: 413.5895\n",
      "    train accuracy: 0.9575, test accuracy: 0.965\n",
      "epoch 2, neglikelihood: 306.1644\n",
      "    train accuracy: 0.99375, test accuracy: 0.995\n",
      "epoch 3, neglikelihood: 257.0077\n",
      "    train accuracy: 1.0, test accuracy: 1.0\n",
      "epoch 4, neglikelihood: 227.7614\n",
      "    train accuracy: 1.0, test accuracy: 1.0\n",
      "epoch 5, neglikelihood: 207.6567\n",
      "    train accuracy: 1.0, test accuracy: 1.0\n",
      "epoch 6, neglikelihood: 192.8063\n",
      "    train accuracy: 1.0, test accuracy: 1.0\n",
      "epoch 7, neglikelihood: 181.2397\n",
      "    train accuracy: 1.0, test accuracy: 1.0\n",
      "epoch 8, neglikelihood: 171.8677\n",
      "    train accuracy: 1.0, test accuracy: 1.0\n",
      "epoch 9, neglikelihood: 164.0813\n",
      "    train accuracy: 1.0, test accuracy: 1.0\n",
      "epoch 10, neglikelihood: 157.4612\n",
      "    train accuracy: 1.0, test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "x_train = x[:int(len(x)*0.8)]\n",
    "x_test = x[int(len(x)*0.8):]\n",
    "\n",
    "y_train = y[:int(len(y)*0.8)]\n",
    "y_test = y[int(len(y)*0.8):]\n",
    "\n",
    "# 设置参数初始值\n",
    "w = torch.rand(size=true_w.shape)\n",
    "b = torch.FloatTensor([0.0])\n",
    "w.requires_grad_(True)\n",
    "b.requires_grad_(True)\n",
    "\n",
    "lr = 0.05\n",
    "num_epochs = 10\n",
    "batch_size = 10  # 构建10个批次的训练集\n",
    "dataset = TensorDataset(x_train, y_train)\n",
    "data_iter = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for t_x, t_y in data_iter:\n",
    "        l = neglikelihood(t_x, t_y, w, b)        \n",
    "        l.backward()  # 计算损失函数在 [w,b] 上的梯度\n",
    "        w.data.sub_(lr*w.grad/batch_size)\n",
    "        w.grad.data.zero_()\n",
    "        b.data.sub_(lr*b.grad/batch_size)\n",
    "        b.grad.data.zero_()\n",
    "        \n",
    "    with torch.no_grad():  # 不计算梯度，加速损失函数的运算\n",
    "        train_l = neglikelihood(x, y, w, b)  # 最近一次的负对数似然率\n",
    "        est_w = [u[0] for u in w.detach().numpy()]  # detach得到一个有着和原tensor相同数据的tensor\n",
    "        est_b = [v for v in b.detach().numpy()]\n",
    "        train_accu_ratio = precision(w, b, x_train, y_train)\n",
    "        test_accu_ratio = precision(w, b, x_test, y_test)\n",
    "        \n",
    "        print(f'epoch {epoch + 1}, neglikelihood: {train_l.numpy()[0][0]:.4f}')\n",
    "        print(f'    train accuracy: {train_accu_ratio}, test accuracy: {test_accu_ratio}')\n",
    "#         print(f'    w0: {est_w[0]:.4f}, w1: {est_w[1]:.4f},  b: {est_b[0]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于`LogisticRegression`验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression(solver='lbfgs')\n",
    "classifier.fit(x.data.numpy(), y.data.squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_y = classifier.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(p_y == y.data.squeeze().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
