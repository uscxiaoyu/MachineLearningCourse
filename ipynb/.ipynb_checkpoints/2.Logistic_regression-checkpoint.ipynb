{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\uscxi\\\\GitProjects\\\\MachineLearningCourse'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import os\n",
    "os.chdir(\"c://Users/uscxi/GitProjects/MachineLearningCourse/\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/smsspamcollection/SMSSpamCollection', delimiter='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0                                                  1\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(747, 4825)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[0]=='spam'][0].count(), df[df[0]=='ham'][0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[1].values\n",
    "y = df[0].values\n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train_raw)\n",
    "X_test = vectorizer.transform(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x55 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 58 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit_transform(X_train_raw[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression(solver='lbfgs')\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: ham, message: What time you coming down later? \n",
      "Predicted: ham, message: Eh u remember how 2 spell his name... Yes i did. He v naughty make until i v wet.\n",
      "Predicted: ham, message: Where to get those?\n",
      "Predicted: ham, message: Ok try to do week end course in coimbatore.\n",
      "Predicted: ham, message: Jus finished avatar nigro\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(X_test)\n",
    "for i, prediction in enumerate(predictions[:5]):\n",
    "    print('Predicted: %s, message: %s' % (prediction, X_test_raw[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二. 对数几率回归（logit regression）\n",
    "考虑一个二分类任务，其生产标记$y\\in \\{0,1\\}$，而线性回归模型产生的预测值$z=\\mathbf{\\omega^Tx+b}$是实数，于是需将$z$转换为0/1值。直观地，可以考虑\"单位阶跃函数\"\n",
    "$$\n",
    "\\begin{equation}\n",
    "y=\\begin{cases}\n",
    "0,z<0;\\\\\n",
    "0.5,z=0;\\\\\n",
    "1,z>0.\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "即若预测值$z$大于0则判为正例。显然，单位阶越函数是不连续函数，因此退而使用有更好兴致的对数几率函数（logistic function）:\n",
    "$$\n",
    "y=\\frac{1}{1+e^{-z}}.\n",
    "$$\n",
    "显然对数几率函数可以将z值转换为一个接近0或1的值，且在$z=0$附近变化很陡。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(z):\n",
    "    if z < 0:\n",
    "        return 0\n",
    "    elif z == 0:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def g(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_f = np.vectorize(f)\n",
    "v_g = np.vectorize(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(-5, 5, num=200)\n",
    "y1 = v_f(z)\n",
    "y2 = v_g(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.set_matplotlib_formats('svg')\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(z, y1, 'r-', label='Heaviside')\n",
    "ax.plot(z, y2, 'g-', label='logit')\n",
    "ax.scatter([0], [0.5], s=50, alpha=0.5)\n",
    "ax.set_xlim([-5, 5])\n",
    "ax.set_xlabel(\"z\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将$z=\\mathbf{\\omega^Tx+b}$代入对数几率函数，可得\n",
    "$$\n",
    "y=\\frac{1}{1+e^{-\\mathbf{\\omega^Tx+b}}}.\n",
    "$$\n",
    "进而转换为\n",
    "$$\n",
    "\\mathrm{ln}\\frac{y}{1-y}=\\mathbf{\\omega^Tx+b}.\n",
    "$$\n",
    "若将$y$视为样本$\\mathbf{x}$作为正例的可能性，则$1-y$是其反例可能性，两者比值为\n",
    "$$\n",
    "\\frac{y}{1-y}\n",
    "$$\n",
    "称为几率（odd），反映了x作为正例的相对可能性。对几率取自然对数则可得对数几率（log odds, 也称为logit）\n",
    "$$\n",
    "\\mathrm{ln}\\frac{y}{1-y}\n",
    "$$\n",
    "通过“极大似然法”来估计$\\omega$和$b$，给定数据集$\\{(x_i,y_i)\\}^m_{i=1}$，最大化对数似然率\n",
    "$$\n",
    "\\text{max  } \\mathbb{l(w,b)}=\\sum_{i=1}^m \\mathrm{ln}p(y_i|\\mathbf{x_i;w,b})\n",
    "$$\n",
    "即令每个样本属于其真实标记的概率越大越好。上式又等价于最小化负对数似然率\n",
    "$$\n",
    "(\\omega, b)^* = \\text{argmin  } \\mathbb{l(w,b)}=\\sum_{i=1}^m\\left(-y_i(\\omega^Tx_i+b)+\\mathbf{ln}(1+e^{\\omega^Tx_i+b})\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmod函数\n",
    "def logit(w, x, b):\n",
    "    return 1 / (1 + torch.exp(x@w + b))\n",
    "\n",
    "# 负对数似然率函数\n",
    "def neglikelihood(x, y, w, b):\n",
    "    z = x@w + b\n",
    "    llike = -y.reshape(1, -1)@z + torch.sum(torch.log(1 + torch.exp(z)))\n",
    "    return llike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w = torch.FloatTensor([0.2, 0.3]).reshape(-1, 1)\n",
    "true_b = torch.FloatTensor([0.2])\n",
    "x = torch.randn(size=(1000, 2)).float()\n",
    "\n",
    "# 生成数据集\n",
    "z = logit(true_w, x, true_b)  # 为正例的概率\n",
    "y = z >= 0.5  # 生成0或1\n",
    "y = y.float()  # 注意要转换为浮点数，否则后面迭代时报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置参数初始值\n",
    "w = torch.rand(size=true_w.shape)\n",
    "b = torch.FloatTensor([0.0])\n",
    "w.requires_grad_(True)\n",
    "b.requires_grad_(True)\n",
    "\n",
    "lr = 0.01\n",
    "num_epochs = 10\n",
    "batch_size = 10  # 构建10个批次的训练集\n",
    "dataset = TensorDataset(x, y)\n",
    "data_iter = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for x, y in data_iter:\n",
    "        l = neglikelihood(x, y, w, b)        \n",
    "        l.backward()  # 计算损失函数在 [w,b] 上的梯度\n",
    "        w.data.sub_(lr*w.grad/batch_size)\n",
    "        w.grad.data.zero_()\n",
    "        b.data.sub_(lr*b.grad/batch_size)\n",
    "        b.grad.data.zero_()\n",
    "        \n",
    "    with torch.no_grad():  # 不计算梯度，加速损失函数的运算\n",
    "        train_l = neglikelihood(x, y, w, b)  # 最近一次的负对数似然率\n",
    "        est_w = [x[0] for x in w.detach().numpy()]  # detach得到一个有着和原tensor相同数据的tensor\n",
    "        est_b = [x for x in b.detach().numpy()]\n",
    "        print(f'epoch {epoch + 1}, neglikelihood: {train_l.numpy()[0][0]:.4f}')\n",
    "        print(f'    w0: {est_w[0]:.4f}, w1: {est_w[1]:.4f},  b: {est_b[0]:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
