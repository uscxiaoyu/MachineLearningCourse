{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三. 对数几率回归（logit regression）\n",
    "考虑一个二分类任务，其生产标记$y\\in \\{0,1\\}$，而线性回归模型产生的预测值$z=\\mathbf{\\omega^Tx+b}$是实数，于是需将$z$转换为0/1值。直观地，可以考虑\"单位阶跃函数\"\n",
    "$$\n",
    "\\begin{equation}\n",
    "y=\\begin{cases}\n",
    "0,z<0;\\\\\n",
    "0.5,z=0;\\\\\n",
    "1,z>0.\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "即若预测值$z$大于0则判为正例。显然，单位阶越函数是不连续函数，因此退而使用有更好性质的对数几率函数（logistic function）:\n",
    "$$\n",
    "y=\\frac{1}{1+e^{-z}}.\n",
    "$$\n",
    "显然对数几率函数可以将z值转换为一个接近0或1的值，且在$z=0$附近变化很陡。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将$z=\\mathbf{\\omega^Tx+b}$代入对数几率函数，可得\n",
    "$$\n",
    "y=\\frac{1}{1+e^{-\\mathbf{\\omega^Tx+b}}}.\n",
    "$$\n",
    "进而转换为\n",
    "$$\n",
    "\\mathrm{ln}\\frac{y}{1-y}=\\mathbf{\\omega^Tx+b}.\n",
    "$$\n",
    "若将$y$视为样本$\\mathbf{x}$作为正例的可能性，则$1-y$是其反例可能性，两者比值为\n",
    "$$\n",
    "\\frac{y}{1-y}\n",
    "$$\n",
    "称为几率（odd），反映了x作为正例的相对可能性。对几率取自然对数则可得对数几率（log odds, 也称为logit）\n",
    "$$\n",
    "\\mathrm{ln}\\frac{y}{1-y}\n",
    "$$\n",
    "通过“极大似然法”来估计$\\omega$和$b$，给定数据集$\\{(x_i,y_i)\\}^m_{i=1}$，最大化对数似然率\n",
    "$$\n",
    "\\text{max  } \\mathbf{l(w,b)}=\\sum_{i=1}^m \\mathrm{ln}p(y_i|\\mathbf{x_i;w,b})\n",
    "$$\n",
    "即令每个样本属于其真实标记的概率越大越好。上式又等价于最小化负对数似然率\n",
    "$$\n",
    "(\\omega, b)^* = \\text{argmin  } \\mathbf{l(w,b)}=\\sum_{i=1}^m\\left(-y_i(\\omega^Tx_i+b)+\\mathbf{ln}(1+e^{\\omega^Tx_i+b})\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "令$\\beta = (\\omega, b), \\hat{x}_i = (x_i, 1)$，则有\n",
    "$$\n",
    "\\beta^* = \\text{argmin  } \\mathbf{l(\\beta)}=\\sum_{i=1}^m\\left(-y_i \\hat{x}_i \\beta^T+\\mathbf{ln}(1+e^{\\beta^T \\hat{x}_i})\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对$\\beta$求梯度可得\n",
    "$$\n",
    "\\mathbf{\\frac{\\partial l}{\\partial \\beta}}=\\sum_{i=1}^m(-y_i+\\frac{1}{1+e^{-\\beta^T\\hat{x}_i}})\\hat{x}_i\n",
    "$$\n",
    "令$\\hat{y}_i=\\frac{1}{1+e^{-\\beta^T\\hat{x}_i}}$，则有\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{\\frac{\\partial l}{\\partial \\beta}}&=\\sum_{i=1}^m(\\hat{y}_i-y_i)\\hat{x}_i \\\\\n",
    "&=(\\hat{y}-y)^T \\hat{X} \n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython import display\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 垃圾邮件数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dataset/smsspamcollection/SMSSpamCollection', delimiter='\\t', header=None, names=['category', 'message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('垃圾邮件数量: %d ' % np.sum(df.category == 'spam'))\n",
    "print('正常邮件数量: %d ' % np.sum(df.category == 'ham'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 使用scikit-learn获取tfidf分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.message.values\n",
    "y = df.category.values\n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "vectorizer = TfidfVectorizer()  # tfidf值\n",
    "X_train = vectorizer.fit_transform(X_train_raw)\n",
    "X_test = vectorizer.transform(X_test_raw)\n",
    "# vectorizer.vocabulary_  # 词典 \n",
    "# vectorizer.get_feature_names()  # 特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(solver='lbfgs')  # logit分类器\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(X_test)\n",
    "for i, prediction in enumerate(predictions[:10]):\n",
    "    print('Predicted: %s, True: %s, Message: %s' % (prediction, y_test[i], X_test_raw[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(classifier.predict(X_train) != y_train)  # 训练集预测出错数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(classifier.predict(X_test) != y_test)  # 测试集预测出错数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(z):\n",
    "    if z < 0:\n",
    "        return 0\n",
    "    elif z == 0:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def g(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数的向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_f = np.vectorize(f)\n",
    "v_g = np.vectorize(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(-5, 5, num=200)\n",
    "y1 = v_f(z)\n",
    "y2 = v_g(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.set_matplotlib_formats('svg')\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(z, y1, 'r-', label='Heaviside')\n",
    "ax.plot(z, y2, 'g-', label='logit')\n",
    "ax.scatter([0], [0.5], s=50, alpha=0.5)\n",
    "ax.set_xlim([-5, 5])\n",
    "ax.set_xlabel(\"z\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 基于`torch`实现Logit回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "示例数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_beta = torch.FloatTensor([4, -2, 1]).reshape(-1, 1)\n",
    "x = torch.randn(size=(1000, 2)).float()\n",
    "\n",
    "# 生成数据集\n",
    "z = sigmoid(x, true_beta)  # 为正例的概率\n",
    "y = z >= 0.5  # 生成True或False\n",
    "y = y.float()  # 注意要转换为浮点数，否则后面迭代时报错\n",
    "\n",
    "x_train = x[:int(len(x)*0.8)]\n",
    "x_test = x[int(len(x)*0.8):]\n",
    "\n",
    "y_train = y[:int(len(y)*0.8)]\n",
    "y_test = y[int(len(y)*0.8):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmod函数\n",
    "def sigmoid(X, beta):\n",
    "    '''\n",
    "    X: m * d m个样本，每个样本d维特征\n",
    "    beta: d+1 维, d维特征权重+1维截距\n",
    "    '''\n",
    "    hat_X = torch.cat([X, torch.ones(X.shape[0], 1)], axis=1)\n",
    "    return torch.sigmoid(hat_X@beta)  # 1 / (1 + torch.exp(-hat_X@beta))\n",
    "\n",
    "# 负对数似然函数\n",
    "def neglikelihood(X, y, beta):\n",
    "    '''\n",
    "    X: m * d m个样本，每个样本d维特征\n",
    "    y: m * 1 m个标签\n",
    "    beta: d+1 维, d维特征权重+1维截距\n",
    "    '''\n",
    "    hat_y = sigmoid(X, beta)\n",
    "    llike = -y.reshape(1, -1)@hat_y + torch.ones_like(hat_y).reshape(1, -1)@torch.log(1 + torch.exp(hat_y))\n",
    "    return llike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(beta, feature, true_label):\n",
    "    z = sigmoid(feature, beta)\n",
    "    y = (z >= 0.5).float()\n",
    "    return torch.sum(y == true_label).numpy() / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_logit(X, y, lr=0.05):\n",
    "    hat_X = torch.cat([X, torch.ones(X.shape[0], 1)], axis=1)\n",
    "    beta = torch.randn(X.shape[1] + 1, 1)  # 增广权重\n",
    "    loss = neglikelihood(X, y, beta)\n",
    "    trace_loss = loss.numpy()\n",
    "    while True:\n",
    "        err = sigmoid(X, beta) - y\n",
    "        beta = beta - lr * hat_X.t() @ err\n",
    "        loss = neglikelihood(X, y, beta)\n",
    "        loss = loss.numpy()\n",
    "        trace_loss = np.concatenate([trace_loss, loss])\n",
    "        if np.abs((trace_loss[-1] - trace_loss[-2]) / trace_loss[-1]) < 1e-5:\n",
    "            break\n",
    "            \n",
    "    return beta.squeeze(), trace_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_beta, _ = sgd_logit(x_train, y_train, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_precision = precision(est_beta.reshape(-1, 1), x_train, y_train)\n",
    "train_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_precision = precision(est_beta.reshape(-1, 1), x_test, y_test)\n",
    "test_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "vectorizer = TfidfVectorizer()  # tfidf值\n",
    "X_train = vectorizer.fit_transform(X_train_raw)\n",
    "X_test = vectorizer.transform(X_test_raw)\n",
    "\n",
    "torch_X_train = torch.from_numpy(X_train.todense())\n",
    "torch_X_test = torch.from_numpy(X_test.todense())\n",
    " \n",
    "c_y_train = y_train[:]\n",
    "c_y_test = y_test[:]\n",
    "c_y_train[c_y_train == 'ham'] = 1\n",
    "c_y_train[c_y_train == 'spam'] = 0\n",
    "\n",
    "c_y_test[c_y_test == 'ham'] = 1\n",
    "c_y_test[c_y_test == 'spam'] = 0\n",
    "\n",
    "torch_y_train = torch.from_numpy(c_y_train.astype(np.int8))\n",
    "torch_y_test = torch.from_numpy(c_y_test.astype(np.int8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_beta, _ = sgd_logit(torch_X_train, torch_y_train, lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 利用`torch`自动求导机制实现小批量随机梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "num_epochs = 20\n",
    "batch_size = 20  # 构建10个批次的训练集\n",
    "dataset = TensorDataset(x_train, y_train)\n",
    "data_iter = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "beta = torch.randn(x_train.shape[1] + 1, 1)\n",
    "beta.requires_grad_()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for t_x, t_y in data_iter:\n",
    "        l = neglikelihood(t_x, t_y, beta)        \n",
    "        l.backward()  # 计算损失函数在 [w,b] 上的梯度\n",
    "        beta.data.sub_(lr*beta.grad/batch_size)\n",
    "        beta.grad.data.zero_()\n",
    "        \n",
    "    with torch.no_grad():  # 不计算梯度，加速损失函数的运算\n",
    "        train_l = neglikelihood(x, y, beta)  # 最近一次的负对数似然率\n",
    "        est_beta = [u[0] for u in beta.detach().numpy()]  # detach得到一个有着和原tensor相同数据的tensor\n",
    "        train_accu_ratio = precision(beta, x_train, y_train)\n",
    "        test_accu_ratio = precision(beta, x_test, y_test)\n",
    "        \n",
    "        print(f'epoch {epoch + 1}, neglikelihood: {train_l.numpy()[0][0]:.4f}')\n",
    "        print(f'    train accuracy: {train_accu_ratio}, test accuracy: {test_accu_ratio}')\n",
    "#         print(f'    w0: {est_w[0]:.4f}, w1: {est_w[1]:.4f},  b: {est_b[0]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于`LogisticRegression`验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(solver='lbfgs')\n",
    "classifier.fit(x_train.numpy(), y_train.squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_y = classifier.predict(x_train.data.numpy())\n",
    "pred_test_y = classifier.predict(x_test.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(pred_train_y == y_train.data.squeeze().numpy())/len(y_train))\n",
    "print(np.sum(pred_test_y == y_test.data.squeeze().numpy())/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.predict_proba(x_train.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
