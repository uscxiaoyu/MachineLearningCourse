{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 五. `softmax`回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 概述\n",
    "\n",
    "Softmax回归（Softmax Regression），也称为多项（Multinomial）或多类（Multi-Class）的Logistic回归，是Logistic回归在多分类问题上的推广。\n",
    "\n",
    "对于多类问题，类别标签$y \\in {1, 2,..., C}$ 可以有C个取值．给定一个样本x，Softmax 回归预测的属于类别c的条件概率为\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(y=c|\\mathbf{x})&=\\mathrm{softmax}(\\mathbf{w^T_cx})\\\\\n",
    "&=\\frac{\\exp(\\mathbf{w^T_cx})}{\\sum_{i=1}^C \\exp(\\mathbf{w^T_ix})}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中$\\mathbf{w_i}$是第i类的权重向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython import display\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, W):\n",
    "    \"\"\"\n",
    "    X: torch.FloatTensor, N*a, N样本数量, a为特征的维度\n",
    "    W: torch.FloatTensor, a*C, C为类别数量\n",
    "    \"\"\"\n",
    "    C = torch.exp(X@W)  # hat_y, N*C\n",
    "    return C / torch.sum(C, axis=1).reshape(X.shape[0], -1)  # 各样本对应类别的标准化概率分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8991e-01, 1.9777e-01, 6.1233e-01],\n",
       "        [9.2654e-02, 8.4814e-01, 5.9211e-02],\n",
       "        [8.9445e-02, 2.4870e-02, 8.8568e-01],\n",
       "        [4.5190e-01, 4.3690e-01, 1.1120e-01],\n",
       "        [1.8212e-02, 1.6665e-04, 9.8162e-01],\n",
       "        [1.5361e-01, 8.3875e-01, 7.6353e-03],\n",
       "        [3.4259e-01, 7.8311e-02, 5.7910e-01],\n",
       "        [1.8198e-01, 8.0394e-01, 1.4078e-02],\n",
       "        [1.5031e-01, 6.8826e-03, 8.4280e-01],\n",
       "        [2.7072e-01, 1.2412e-01, 6.0516e-01]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(10, 5)\n",
    "W = torch.randn(5, 3)\n",
    "softmax(X, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Softmax回归的决策函数\n",
    "\n",
    "Softmax回归的决策函数可以表示为\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{y}&=\\text{arg}\\max_{i=1}^{C}p(y=c|\\mathbf{x})\\\\\n",
    "&=\\text{arg}\\max_{i=1}^{C}\\mathbf{w_i^Tx}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hat_y(X, W):\n",
    "    S = softmax(X, W)  # 各样本在各类别上的概率\n",
    "    max_indices = torch.max(S, dim=1)[1]\n",
    "    pred_y = torch.zeros_like(S)\n",
    "    pred_y[torch.arange(S.shape[0]), max_indices] = 1\n",
    "    return max_indices, pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = hat_y(X, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 2, 0, 2, 1, 2, 1, 2, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 与`Logistic`回归的关系。当类别数$C=2$时，softmax回归的决策函数为\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{y}&=\\text{arg}\\max_{i\\in\\{1,2\\}}p(y=c|\\mathbf{x})\\\\\n",
    "&=\\text{arg}\\max_{i\\in\\{1,2\\}}\\mathbf{w_i^Tx}\\\\\n",
    "&=I(\\mathbf{(w_1-w_0)^Tx}>0)\n",
    "\\end{aligned}\n",
    "$$\n",
    "其中$I(\\cdot)$是指示函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 准则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定N个训练样本，Softmax回归使用交叉熵损失函数学习最有的参数矩阵$W$。为了方便起见，使用C维的`one-hot`向量表示类别标签，对于类别i，其向量表示为\n",
    "$$\n",
    "y = [I(i=1), I(i=2), ..., I(i=C)]\n",
    "$$\n",
    "\n",
    "采用交叉熵损失函数，Softmax回归模型的风险函数是\n",
    "$$\n",
    "\\begin{aligned}\n",
    "R(\\mathbf{W})&=-\\frac{1}{N}\\sum_{n=1}^N\\sum_{i=1}^{C}y_c^{(n)}\\log \\hat{y}_c^{(n)}\\\\\n",
    "&=-\\frac{1}{N}\\sum_{n=1}^N(\\mathbf{y^{(n)}})^T\\log \\hat{y}_c^{(n)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "其中，$\\hat{y}_c^{(n)}=\\text{softmax}(\\mathbf(W^Tx^{(n)}))$为样本$x^{(n)}$在每个类别的后验概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(X, y, W):\n",
    "    \"\"\"\n",
    "    X: N*(a+1), N个样本, 特征数量为为a, 外加1维偏置\n",
    "    y: N*C, y为N个C维的one-hot向量\n",
    "    W: (a+1)*C\n",
    "    \"\"\"\n",
    "    p_y = softmax(X, W) # N*C, N个样本分别在C个类别的后验概率\n",
    "    crossEnt = -torch.dot(y.reshape(-1), torch.log2(p_y).reshape(-1)) / y.shape[0]  # 展开成1维，点积\n",
    "    return crossEnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(10, 5)\n",
    "y = torch.zeros(10, 3)\n",
    "y[torch.arange(10), torch.randint(low=0, high=y.shape[1] - 1, size=(10,))] = 1\n",
    "W = torch.randn(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0838)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(X, y, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_y = softmax(X, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0838)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.dot(y.reshape(-1), torch.log2(prob_y).reshape(-1)) / y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 风险函数$\\mathbf{R(W)}$关于$W$的梯度为\n",
    "$$\n",
    "\\frac{\\partial R(W)}{\\partial W}=-\\frac{1}{N}\\sum_{n=1}^N\\mathbf{x^{(n)}(y^{(n)}-\\hat{y}^{(n)})}^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_crosEnt_W(X, y, W):\n",
    "    '''\n",
    "    X: N*(a+1), N个样本, 特征数量为为a, 外加1维偏置\n",
    "    y: N*C, y为N个C维的one-hot向量\n",
    "    W: (a+1)*C\n",
    "    '''\n",
    "    hat_y = softmax(X, W)\n",
    "    a = (X.t() @ (y - hat_y)) / y.shape[0]  # (a+1)*N | N*C\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2208e-01,  1.9918e-01, -7.7093e-02],\n",
       "        [-1.8273e-01,  1.5876e-02,  1.6686e-01],\n",
       "        [-3.1105e-01,  1.7246e-01,  1.3859e-01],\n",
       "        [ 1.4861e-02,  2.1199e-01, -2.2685e-01],\n",
       "        [-5.2817e-05,  2.6907e-02, -2.6854e-02]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_crosEnt_W(X, y, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 学习方法\n",
    "\n",
    "采用梯度下降法，softmax回归的训练过程为：\n",
    "\n",
    "初始化$W_0:=0$，然后通过下式进行参数的迭代更新\n",
    "$$\n",
    "W_{t+1}:=W_t+\\eta\\left(\\frac{1}{N}\\sum_{n=1}^N\\mathbf{x^{(n)}(y^{(n)}-\\hat{y}^{(n)})}^T\\right) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 预测的正确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_rate(X, y, W, X_with_bias=False):\n",
    "    if X_with_bias:\n",
    "        hat_X = X\n",
    "    else:\n",
    "        hat_X = torch.cat([X, torch.ones(X.shape[0], 1)], axis=1)  # 增广X\n",
    "        \n",
    "    pred_y = hat_y(hat_X, W)\n",
    "    precision = torch.sum(pred_y[0] == torch.max(y, axis=1)[1]).numpy() / pred_y[0].numel()\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 生成模拟数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(1000, 8)\n",
    "hat_X = torch.cat([X, torch.ones(X.shape[0], 1)], axis=1)  # 增广\n",
    "true_W = torch.randn(hat_X.shape[1], 5)  # 增广\n",
    "indices_y, y = hat_y(hat_X, true_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = X[:800], y[:800]\n",
    "train_indices_y = indices_y[:800]\n",
    "hat_train_X = hat_X[:800]\n",
    "test_X, test_y = X[800:], y[800:]\n",
    "test_indices_y = indices_y[800:]\n",
    "hat_test_X = hat_X[800:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 方法1: 梯度下降-人工求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_sgd(X, y, num_steps=100, lr=0.1):\n",
    "    '''\n",
    "    X: N*(a+1), N个样本, 特征数量为为a, 外加1维偏置\n",
    "    y: N*C, y为N个C维的one-hot向量\n",
    "    W: (a+1)*C\n",
    "    '''\n",
    "    hat_X = torch.cat([X, torch.ones(X.shape[0], 1)], axis=1)  # 增广X\n",
    "    W = torch.randn(hat_X.shape[1], y.shape[1])  # 增广参数矩阵\n",
    "    for i in range(num_steps):\n",
    "        W += lr*grad_crosEnt_W(hat_X, y, W)\n",
    "        loss = cross_entropy(hat_X, y, W)\n",
    "        if (i+1) % 50 == 0:\n",
    "            print(f'训练{i+1}轮, 交叉熵为{loss:.2f}')\n",
    "            \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练50轮, 交叉熵为2.00\n",
      "训练100轮, 交叉熵为1.09\n",
      "训练150轮, 交叉熵为0.77\n",
      "训练200轮, 交叉熵为0.64\n",
      "训练250轮, 交叉熵为0.56\n",
      "训练300轮, 交叉熵为0.52\n",
      "训练350轮, 交叉熵为0.48\n",
      "训练400轮, 交叉熵为0.46\n",
      "训练450轮, 交叉熵为0.44\n",
      "训练500轮, 交叉熵为0.42\n",
      "训练550轮, 交叉熵为0.41\n",
      "训练600轮, 交叉熵为0.39\n",
      "训练650轮, 交叉熵为0.38\n",
      "训练700轮, 交叉熵为0.37\n",
      "训练750轮, 交叉熵为0.36\n",
      "训练800轮, 交叉熵为0.36\n",
      "训练850轮, 交叉熵为0.35\n",
      "训练900轮, 交叉熵为0.34\n",
      "训练950轮, 交叉熵为0.33\n",
      "训练1000轮, 交叉熵为0.33\n"
     ]
    }
   ],
   "source": [
    "# 模拟数据\n",
    "est_W = softmax_sgd(train_X, train_y, num_steps=1000)\n",
    "precision_rate(train_X, train_y, est_W, X_with_bias=False), precision_rate(test_X, test_y, est_W, X_with_bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 方法2: 随机梯度下降-自动求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_miniBatch_sgd(X, y, num_epoch=50, batch_size=40, lr=0.05):\n",
    "    '''\n",
    "    X: N*a, N个样本, 特征数量为为a\n",
    "    y: N*C, y为N个C维的one-hot向量\n",
    "    W: a*C\n",
    "    '''\n",
    "    hat_X = torch.cat([X, torch.ones(X.shape[0], 1)], axis=1)  # 增广X\n",
    "    W = torch.randn(hat_X.shape[1], y.shape[1])  # 增广参数矩阵\n",
    "    W.requires_grad_()\n",
    "    dataset = TensorDataset(hat_X, y)\n",
    "    data_iter = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "    for epoch in range(num_epoch):\n",
    "        for t_x, t_y in data_iter:\n",
    "            l = cross_entropy(t_x, t_y, W)        \n",
    "            l.backward()  # 计算损失函数在 W 上的梯度\n",
    "            W.data.sub_(lr*W.grad/batch_size)\n",
    "            W.grad.data.zero_()\n",
    "            \n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            with torch.no_grad():  # 不计算梯度，加速损失函数的运算\n",
    "                train_l = cross_entropy(hat_X, y, W)  # 最近一次的负对数似然率\n",
    "                est_W = W.detach().numpy()  # detach得到一个有着和原tensor相同数据的tensor\n",
    "                print(f'epoch {epoch + 1}, loss: {train_l:.4f}')\n",
    "            \n",
    "    return est_W, train_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, loss: 2.3773\n",
      "epoch 100, loss: 1.2608\n",
      "epoch 150, loss: 0.8993\n",
      "epoch 200, loss: 0.7505\n",
      "epoch 250, loss: 0.6676\n",
      "epoch 300, loss: 0.6123\n",
      "epoch 350, loss: 0.5716\n",
      "epoch 400, loss: 0.5399\n",
      "epoch 450, loss: 0.5142\n",
      "epoch 500, loss: 0.4928\n",
      "epoch 550, loss: 0.4746\n",
      "epoch 600, loss: 0.4589\n",
      "epoch 650, loss: 0.4450\n",
      "epoch 700, loss: 0.4327\n",
      "epoch 750, loss: 0.4217\n",
      "epoch 800, loss: 0.4117\n",
      "epoch 850, loss: 0.4026\n",
      "epoch 900, loss: 0.3942\n",
      "epoch 950, loss: 0.3864\n",
      "epoch 1000, loss: 0.3792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.945, 0.96)"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模拟数据\n",
    "est_W, train_l = softmax_miniBatch_sgd(train_X, train_y, num_epoch=1000, batch_size=40, lr=0.1)\n",
    "precision_rate(train_X, train_y, est_W), precision_rate(test_X, test_y, est_W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 方法3: torch.nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SofmaxRegresModel(torch.nn.Module): \n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        # 首先找到LinearModel的父类torch.nn.Module，然后把类LinearModel的对象转换为类torch.nn.Module的对象, \n",
    "        # 即执行父类torch.nn.Module的初始化__init__()\n",
    "        super(SofmaxRegresModel, self).__init__() \n",
    "        self.layer1 = torch.nn.Linear(dim_in, dim_out, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = self.layer1(x)\n",
    "        return torch.nn.functional.softmax(y_pred, dim=1)  # softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义训练算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_in = X.shape[1]\n",
    "dim_out = y.shape[1]\n",
    "# 实例化1个网络\n",
    "net = SofmaxRegresModel(dim_in, dim_out)\n",
    "# 初始化网络参数和偏置\n",
    "net.layer1.weight.data = torch.randn(dim_out, dim_in)\n",
    "net.layer1.bias.data = torch.Tensor(dim_out)\n",
    "# 损失函数\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "# 随机梯度下降算法\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20, loss 1.368786334991455\n",
      "epoch 40, loss 1.173109531402588\n",
      "epoch 60, loss 1.1000187397003174\n",
      "epoch 80, loss 1.0768675804138184\n",
      "epoch 100, loss 1.0675488710403442\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "batch_size = 20\n",
    "num_epochs = 100\n",
    "dataset = TensorDataset(train_X, train_indices_y)\n",
    "data_iter = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "# 开始训练\n",
    "for epoch in range(num_epochs):\n",
    "    for t_x, t_y in data_iter:\n",
    "        l = loss(net(t_x), t_y)  # 计算当前批量的交叉熵损失\n",
    "        trainer.zero_grad()  # 参数梯度清零\n",
    "        l.backward()  # 反向传播，计算梯度\n",
    "        trainer.step()  # 更新参数\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        with torch.no_grad():  # 不计算梯度，加速损失函数的运算\n",
    "            l_epoch = loss(net(train_X), train_indices_y) \n",
    "            print('epoch {}, loss {}'.format(epoch+1, l_epoch)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0.90125\n",
      "test 0.9\n"
     ]
    }
   ],
   "source": [
    "w, b = net.parameters()\n",
    "W = torch.cat([w.data, b.data.reshape(-1, 1)], axis=1)\n",
    "\n",
    "pred_train_y = torch.max(net(train_X), axis=1)[1]\n",
    "pred_test_y = torch.max(net(test_X), axis=1)[1]\n",
    "\n",
    "print('train', torch.sum(pred_train_y == train_indices_y).numpy() / train_indices_y.numel())\n",
    "print('test', torch.sum(pred_test_y == test_indices_y).numpy() / test_indices_y.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 案例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "鸢尾花数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "d = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_labels, y_labels = d['feature_names'], d['target_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " array(['setosa', 'versicolor', 'virginica'], dtype='<U10'))"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_labels, y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_idx = np.arange(len(d['target']))\n",
    "np.random.shuffle(rand_idx)\n",
    "t_idx = rand_idx[:100]\n",
    "v_idx = rand_idx[100:]\n",
    "x_train, y_train = torch.from_numpy(d['data'][t_idx]).type(torch.FloatTensor), torch.from_numpy(d['target'][t_idx])\n",
    "x_valid, y_valid = torch.from_numpy(d['data'][v_idx]).type(torch.FloatTensor), torch.from_numpy(d['target'][v_idx])\n",
    "onehot_y_train = torch.zeros(x_train.shape[0], 3)\n",
    "onehot_y_train[torch.arange(x_train.shape[0]), y_train] = 1\n",
    "onehot_y_valid = torch.zeros(x_valid.shape[0], 3)\n",
    "onehot_y_valid[torch.arange(x_valid.shape[0]), y_valid] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 方法1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练50轮, 交叉熵为0.93\n",
      "训练100轮, 交叉熵为0.69\n",
      "训练150轮, 交叉熵为0.49\n",
      "训练200轮, 交叉熵为0.37\n",
      "训练250轮, 交叉熵为0.34\n",
      "训练300轮, 交叉熵为0.31\n",
      "训练350轮, 交叉熵为0.29\n",
      "训练400轮, 交叉熵为0.27\n",
      "训练450轮, 交叉熵为0.26\n",
      "训练500轮, 交叉熵为0.24\n",
      "训练550轮, 交叉熵为0.23\n",
      "训练600轮, 交叉熵为0.22\n",
      "训练650轮, 交叉熵为0.22\n",
      "训练700轮, 交叉熵为0.21\n",
      "训练750轮, 交叉熵为0.20\n",
      "训练800轮, 交叉熵为0.19\n",
      "训练850轮, 交叉熵为0.19\n",
      "训练900轮, 交叉熵为0.18\n",
      "训练950轮, 交叉熵为0.18\n",
      "训练1000轮, 交叉熵为0.18\n",
      "Train accuracy rate: 0.99\n",
      "Valid accuracy rate: 0.94\n"
     ]
    }
   ],
   "source": [
    "est_W = softmax_sgd(x_train, onehot_y_train, num_steps=1000)\n",
    "print(f\"Train accuracy rate: {precision_rate(x_train, onehot_y_train, est_W)}\")\n",
    "print(f\"Valid accuracy rate: {precision_rate(x_valid, onehot_y_valid, est_W)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 方法2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, loss: 1.7345\n",
      "epoch 100, loss: 1.2518\n",
      "epoch 150, loss: 1.0072\n",
      "epoch 200, loss: 0.8750\n",
      "epoch 250, loss: 0.7937\n",
      "epoch 300, loss: 0.7384\n",
      "epoch 350, loss: 0.6971\n",
      "epoch 400, loss: 0.6650\n",
      "epoch 450, loss: 0.6388\n",
      "epoch 500, loss: 0.6158\n",
      "epoch 550, loss: 0.5963\n",
      "epoch 600, loss: 0.5790\n",
      "epoch 650, loss: 0.5635\n",
      "epoch 700, loss: 0.5495\n",
      "epoch 750, loss: 0.5364\n",
      "epoch 800, loss: 0.5245\n",
      "epoch 850, loss: 0.5135\n",
      "epoch 900, loss: 0.5030\n",
      "epoch 950, loss: 0.4932\n",
      "epoch 1000, loss: 0.4839\n",
      "Train accuracy rate: 0.94\n",
      "Valid accuracy rate: 0.9\n"
     ]
    }
   ],
   "source": [
    "# 鸢尾花\n",
    "est_W, _ = softmax_miniBatch_sgd(x_train, onehot_y_train, num_epoch=1000, batch_size=40, lr=0.1)\n",
    "print(f\"Train accuracy rate: {precision_rate(x_train, onehot_y_train, est_W)}\")\n",
    "print(f\"Valid accuracy rate: {precision_rate(x_valid, onehot_y_valid, est_W)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 方法3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, loss 0.8891471028327942\n",
      "epoch 100, loss 0.8810678720474243\n",
      "epoch 150, loss 0.8779969215393066\n",
      "epoch 200, loss 0.8762195110321045\n",
      "epoch 250, loss 0.8750027418136597\n",
      "epoch 300, loss 0.8741034865379333\n",
      "epoch 350, loss 0.8734056949615479\n",
      "epoch 400, loss 0.8728427886962891\n",
      "epoch 450, loss 0.8723844885826111\n",
      "epoch 500, loss 0.8719981908798218\n",
      "epoch 550, loss 0.8716733455657959\n",
      "epoch 600, loss 0.8713958263397217\n",
      "epoch 650, loss 0.871156632900238\n",
      "epoch 700, loss 0.8709509372711182\n",
      "epoch 750, loss 0.8707689046859741\n",
      "epoch 800, loss 0.8706075549125671\n",
      "epoch 850, loss 0.8704662919044495\n",
      "epoch 900, loss 0.8703412413597107\n",
      "epoch 950, loss 0.8702294230461121\n",
      "epoch 1000, loss 0.870128870010376\n",
      "epoch 1050, loss 0.8700387477874756\n",
      "epoch 1100, loss 0.869956910610199\n",
      "epoch 1150, loss 0.869883120059967\n",
      "epoch 1200, loss 0.8698161244392395\n",
      "epoch 1250, loss 0.8697571754455566\n",
      "epoch 1300, loss 0.8696995377540588\n",
      "epoch 1350, loss 0.8696485757827759\n",
      "epoch 1400, loss 0.8696022033691406\n",
      "epoch 1450, loss 0.8695597052574158\n",
      "epoch 1500, loss 0.8695212006568909\n",
      "epoch 1550, loss 0.8694828152656555\n",
      "epoch 1600, loss 0.8694495558738708\n",
      "epoch 1650, loss 0.8694174885749817\n",
      "epoch 1700, loss 0.8693885207176208\n",
      "epoch 1750, loss 0.8693612813949585\n",
      "epoch 1800, loss 0.8693355321884155\n",
      "epoch 1850, loss 0.8693121075630188\n",
      "epoch 1900, loss 0.8692895770072937\n",
      "epoch 1950, loss 0.8692684173583984\n",
      "epoch 2000, loss 0.8692493438720703\n"
     ]
    }
   ],
   "source": [
    "# 实例化\n",
    "dim_in = 4\n",
    "dim_out = 3\n",
    "net = SofmaxRegresModel(dim_in, dim_out)\n",
    "# 初始化网络参数和偏置\n",
    "net.layer1.weight.data = torch.randn(dim_out, dim_in)\n",
    "net.layer1.bias.data = torch.randn(dim_out)\n",
    "# 损失函数\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "# 随机梯度下降算法\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.05)\n",
    "# 加载数据\n",
    "batch_size = 20\n",
    "num_epochs = 2000\n",
    "dataset = TensorDataset(x_train, y_train)\n",
    "data_iter = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 训练\n",
    "for epoch in range(num_epochs):\n",
    "    for t_x, t_y in data_iter:\n",
    "        l = loss(net(t_x), t_y)  # 计算当前批量的交叉熵损失\n",
    "        trainer.zero_grad()  # 参数梯度清零\n",
    "        l.backward()  # 反向传播，计算梯度\n",
    "        trainer.step()  # 更新参数\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        with torch.no_grad():  # 不计算梯度，加速损失函数的运算\n",
    "            l_epoch = loss(net(x_train), y_train) \n",
    "            print('epoch {}, loss {}'.format(epoch+1, l_epoch)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = net.parameters()\n",
    "W = torch.cat([w.data, b.data.reshape(-1, 1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_train = torch.max(net(x_train), axis=1)[1]\n",
    "pred_y_valid = torch.max(net(x_valid), axis=1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 0, 2, 0, 0, 0, 2, 0, 0, 2, 0, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 2, 0,\n",
       "        2, 0, 2, 0, 2, 2, 0, 2, 0, 0, 0, 0, 2, 2, 0, 2, 2, 2, 2, 2, 0, 0, 0, 2,\n",
       "        2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 2, 0, 2, 0,\n",
       "        0, 2, 2, 2, 2, 0, 2, 0, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 0, 0, 0,\n",
       "        0, 0, 0, 2])"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(pred_y_train == y_train).numpy() / y_train.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(pred_y_valid == y_valid).numpy() / y_valid.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考资料\n",
    "1. 李航. 统计学习方法. 2017.\n",
    "2. 邱锡鹏. 神经网络与机器学习. 2020."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
