{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 五. `softmax`回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 概述\n",
    "\n",
    "`Softmax`回归（`Softmax Regression`），也称为多项（`Multinomial`）或多类（`Multi-Class`）的`Logistic`回归，是`Logistic`回归在多分类问题上的推广。\n",
    "\n",
    "对于多类问题，类别标签$y \\in {1, 2,..., C}$ 可以有$C$个取值．给定一个样本$x$，`Softmax` 回归预测的属于类别$c$的条件概率为\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(y=c|\\mathbf{x})&=\\mathrm{softmax}(\\mathbf{w^T_c x})\\\\\n",
    "&=\\frac{\\exp(\\mathbf{w^T_c x})}{\\sum_{i=1}^C \\exp(\\mathbf{w^T_i x})}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中$\\mathbf{w_i}$是第i类的权重向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = ['svg']\n",
    "%matplotlib inline\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, W):\n",
    "    \"\"\"\n",
    "    X: torch.FloatTensor, N*a, N样本数量, a为特征的维度\n",
    "    W: torch.FloatTensor, a*C, C为类别数量\n",
    "    \"\"\"\n",
    "    C = torch.exp(X@W)  # hat_y, N*C\n",
    "    return C / torch.sum(C, axis=1).reshape(X.shape[0], -1)  # 各样本对应类别的标准化概率分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6579, 0.0061, 0.3361],\n",
       "        [0.3320, 0.6084, 0.0597],\n",
       "        [0.0685, 0.0467, 0.8848],\n",
       "        [0.1772, 0.8016, 0.0212],\n",
       "        [0.0745, 0.8999, 0.0256],\n",
       "        [0.9946, 0.0021, 0.0033],\n",
       "        [0.0011, 0.9967, 0.0022],\n",
       "        [0.0070, 0.2830, 0.7100],\n",
       "        [0.5584, 0.3187, 0.1229],\n",
       "        [0.0062, 0.6337, 0.3600]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(10, 5)\n",
    "W = torch.randn(5, 3)\n",
    "softmax(X, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Softmax回归的决策函数\n",
    "\n",
    "`Softmax`回归的决策函数可以表示为\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{y}&=\\text{arg}\\max_{i=1}^{C}p(y=c|\\mathbf{x})\\\\\n",
    "&=\\text{arg}\\max_{i=1}^{C}\\mathbf{w_i^T x}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hat_y(X, W):\n",
    "    S = softmax(X, W)  # 各样本在各类别上的概率\n",
    "    max_indices = torch.max(S, dim=1)[1]\n",
    "    pred_y = torch.zeros_like(S)\n",
    "    pred_y[torch.arange(S.shape[0]), max_indices] = 1  # one-hot\n",
    "    return max_indices, pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = hat_y(X, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 1, 1, 0, 1, 2, 0, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 与`Logistic`回归的关系。当类别数$C=2$时，softmax回归的决策函数为\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{y}&=\\text{arg}\\max_{i\\in\\{1,2\\}}p(y=c|\\mathbf{x})\\\\\n",
    "&=\\text{arg}\\max_{i\\in\\{1,2\\}}\\mathbf{w_i^T x}\\\\\n",
    "&=I(\\mathbf{(w_2-w_1)^T x}>0))\n",
    "\\end{aligned}\n",
    "$$\n",
    "其中$I(\\cdot)$是指示函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 准则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定`N`个训练样本，`Softmax`回归使用**交叉熵损失函数**学习最有的参数矩阵$W$。为了方便起见，使用$C$维的`one-hot`向量表示类别标签，对于类别$i$，其向量表示为\n",
    "$$\n",
    "y = [I(i=1), I(i=2), ..., I(i=C)]\n",
    "$$\n",
    "\n",
    "采用交叉熵损失函数，`Softmax`回归模型的风险函数是\n",
    "$$\n",
    "\\begin{aligned}\n",
    "R(\\mathbf{W})&=-\\frac{1}{N}\\sum_{n=1}^N\\sum_{i=1}^{C}y_i^{(n)}\\log \\hat{y}_i^{(n)}\\\\\n",
    "&=-\\frac{1}{N}\\sum_{n=1}^N(\\mathbf{y^{(n)}})^T\\log \\hat{y}_c^{(n)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中，$\\hat{y}_c^{(n)}=\\text{softmax}(\\mathbf{W^T x^{(n)}})$ 为样本 $x^{(n)}$ 在每个类别的后验概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 交叉熵\n",
    "\n",
    "交叉熵损失函数是一种常用于机器学习分类问题的损失函数。我们可以从它解决的问题和它如何工作的基本概念理解它。\n",
    "\n",
    "首先，当我们在做分类任务时，我们的目标是让模型能够准确地预测输入数据的类别。例如，在一个简单的二分类问题中，我们可能想要预测一个电子邮件是“垃圾邮件”还是“非垃圾邮件”。\n",
    "\n",
    "在这种情况下，模型会为每个类别输出一个概率，这些概率的总和为1。交叉熵损失函数帮助我们衡量模型预测的概率分布与真实情况的概率分布之间的差异。理想情况下，如果模型的预测完全准确，它预测的概率分布将与实际情况完全一致。\n",
    "\n",
    "交叉熵损失函数的计算方式是这样的：对于给定的真实类别，我们只关注该类别对应的模型输出概率。如果真实类别是“垃圾邮件”（我们可以用1表示），我们就查看模型预测“垃圾邮件”这一类别的概率；如果真实类别是“非垃圾邮件”（用0表示），我们就查看模型预测“非垃圾邮件”这一类别的概率。\n",
    "\n",
    "交叉熵损失的公式是这样的：\n",
    "$$\n",
    "-\\sum_{c=1}^{M} y_{o,c} \\log(p_{o,c})\n",
    "$$\n",
    "其中，$M$ 是类别的数量，$y_{o,c}$ 是如果样本属于类别 $c$ 则为1，否则为0，$p_{o,c}$ 是我们模型预测样本属于类别 $c$ 的概率。\n",
    "\n",
    "在二分类问题中，这个公式可以简化为：\n",
    "$$\n",
    "- y \\log(p) + (1 - y) \\log(1 - p)\n",
    "$$\n",
    "其中，$y$ 是真实标签，$p$ 是模型预测为正类的概率。\n",
    "\n",
    "交叉熵损失函数的关键点在于，它惩罚了错误的预测。如果真实类别是1，模型预测的概率越低，损失就越大；相反，如果真实类别是0，模型预测的概率越高，损失也越大。所以，通过最小化交叉熵损失函数，我们可以训练我们的模型更准确地预测真实的类别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(X, y, W):\n",
    "    \"\"\"\n",
    "    X: N*(a+1), N个样本, 特征数量为为a, 外加1维偏置\n",
    "    y: N*C, y为N个C维的one-hot向量\n",
    "    W: (a+1)*C\n",
    "    \"\"\"\n",
    "    p_y = softmax(X, W) # N*C, N个样本分别在C个类别的后验概率\n",
    "    crossEnt = -torch.dot(y.reshape(-1), torch.log2(p_y).reshape(-1)) / y.shape[0]  # 展开成1维，点积\n",
    "    return crossEnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(10, 5)\n",
    "y = torch.zeros(10, 3)\n",
    "y[torch.arange(10), torch.randint(low=0, high=y.shape[1] - 1, size=(10,))] = 1  # 随机取1行中的某个元素为1，即确定1个样本对应的类别\n",
    "W = torch.randn(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(285), tensor(285))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(10)\n",
    "b = torch.arange(10)\n",
    "\n",
    "torch.sum(a * b), torch.dot(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y  # 真实标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.7931)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(X, y, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_y = softmax(X, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.3872, 0.3757, 2.4621],\n",
       "        [4.9456, 6.6336, 0.0627],\n",
       "        [2.2047, 2.8269, 0.6390],\n",
       "        [3.3883, 4.1743, 0.2360],\n",
       "        [0.3264, 2.5155, 5.1807],\n",
       "        [4.3749, 3.7387, 0.1895],\n",
       "        [3.7005, 5.5953, 0.1482],\n",
       "        [0.3811, 3.0973, 3.1166],\n",
       "        [2.2471, 0.3700, 6.0072],\n",
       "        [1.2805, 2.7241, 1.1943]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.log2(prob_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.7931)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.dot(y.reshape(-1), torch.log2(prob_y).reshape(-1)) / y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 风险函数$\\mathbf{R(W)}$关于$W$的梯度为\n",
    "$$\n",
    "\\frac{\\partial R(W)}{\\partial W}=-\\frac{1}{N}\\sum_{n=1}^N\\mathbf{x^{(n)}(y^{(n)}-\\hat{y}^{(n)})}^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_crosEnt_W(X, y, W):\n",
    "    '''\n",
    "    X: N*(a+1), N个样本, 特征数量为为a, 外加1维偏置\n",
    "    y: N*C, y为N个C维的one-hot向量\n",
    "    W: (a+1)*C\n",
    "    '''\n",
    "    hat_y = softmax(X, W)\n",
    "    a = (X.t() @ (y - hat_y)) / y.shape[0]  # (a+1)*N | N*C\n",
    "    return -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1863,  0.1920, -0.3783],\n",
       "        [ 0.0889, -0.3530,  0.2641],\n",
       "        [ 0.0010,  0.1523, -0.1532],\n",
       "        [ 0.1650, -0.1263, -0.0387],\n",
       "        [-0.1780,  0.0200,  0.1579]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_crosEnt_W(X, y, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 学习方法\n",
    "\n",
    "- 输入: 训练集X，`one-hot`形式的标签y\n",
    "- 输出：最优参数$w^*$\n",
    "- 算法过程\n",
    "    - 初始化$W_0:=0$，最大迭代次数$T$\n",
    "    - 然后通过下式进行参数的迭代更新\n",
    "    $$\n",
    "    W_{t+1}:=W_t+\\eta\\left(\\frac{1}{N}\\sum_{n=1}^N\\mathbf{x^{(n)}(y^{(n)}-\\hat{y}^{(n)})}^T\\right) \n",
    "    $$\n",
    "    - 直到满足指定迭代次数，令$w^*=w^T$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 预测的正确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_rate(X, y, W, X_with_bias=False):\n",
    "    if X_with_bias:\n",
    "        hat_X = X\n",
    "    else:\n",
    "        hat_X = torch.cat([X, torch.ones(X.shape[0], 1)], axis=1)  # 增广X\n",
    "        \n",
    "    pred_y = hat_y(hat_X, W)\n",
    "    precision = torch.sum(pred_y[0] == torch.max(y, axis=1)[1]).numpy() / pred_y[0].numel()\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 生成模拟数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(1000, 8)\n",
    "hat_X = torch.cat([X, torch.ones(X.shape[0], 1)], axis=1)  # 增广\n",
    "true_W = torch.randn(hat_X.shape[1], 5)  # 增广\n",
    "indices_y, y = hat_y(hat_X, true_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = X[:800], y[:800]\n",
    "train_indices_y = indices_y[:800]\n",
    "hat_train_X = hat_X[:800]\n",
    "test_X, test_y = X[800:], y[800:]\n",
    "test_indices_y = indices_y[800:]\n",
    "hat_test_X = hat_X[800:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 方法1: 梯度下降-自己求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_sgd(X, y, num_steps=100, lr=0.1):\n",
    "    '''\n",
    "    X: N*(a+1), N个样本, 特征数量为为a, 外加1维偏置\n",
    "    y: N*C, y为N个C维的one-hot向量\n",
    "    W: (a+1)*C\n",
    "    '''\n",
    "    hat_X = torch.cat([X, torch.ones(X.shape[0], 1)], axis=1)  # 增广X\n",
    "    W = torch.randn(hat_X.shape[1], y.shape[1])  # 增广参数矩阵\n",
    "    for i in range(num_steps):\n",
    "        W -= lr*grad_crosEnt_W(hat_X, y, W)\n",
    "        loss = cross_entropy(hat_X, y, W)\n",
    "        if (i+1) % 50 == 0:\n",
    "            print(f'训练{i+1}轮, 交叉熵为{loss:.2f}')\n",
    "            \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练50轮, 交叉熵为1.74\n",
      "训练100轮, 交叉熵为0.96\n",
      "训练150轮, 交叉熵为0.71\n",
      "训练200轮, 交叉熵为0.61\n",
      "训练250轮, 交叉熵为0.55\n",
      "训练300轮, 交叉熵为0.52\n",
      "训练350轮, 交叉熵为0.49\n",
      "训练400轮, 交叉熵为0.47\n",
      "训练450轮, 交叉熵为0.45\n",
      "训练500轮, 交叉熵为0.43\n",
      "训练550轮, 交叉熵为0.42\n",
      "训练600轮, 交叉熵为0.41\n",
      "训练650轮, 交叉熵为0.40\n",
      "训练700轮, 交叉熵为0.39\n",
      "训练750轮, 交叉熵为0.38\n",
      "训练800轮, 交叉熵为0.37\n",
      "训练850轮, 交叉熵为0.36\n",
      "训练900轮, 交叉熵为0.35\n",
      "训练950轮, 交叉熵为0.35\n",
      "训练1000轮, 交叉熵为0.34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.95625, 0.94)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模拟数据\n",
    "est_W = softmax_sgd(train_X, train_y, num_steps=1000)\n",
    "precision_rate(train_X, train_y, est_W, X_with_bias=False), precision_rate(test_X, test_y, est_W, X_with_bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 方法2: 随机梯度下降-自动求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_miniBatch_sgd(X, y, num_epoch=50, batch_size=40, lr=0.05):\n",
    "    '''\n",
    "    X: N*a, N个样本, 特征数量为为a\n",
    "    y: N*C, y为N个C维的one-hot向量\n",
    "    W: a*C\n",
    "    '''\n",
    "    hat_X = torch.cat([X, torch.ones(X.shape[0], 1)], axis=1)  # 增广X\n",
    "    W = torch.randn(hat_X.shape[1], y.shape[1])  # 增广参数矩阵\n",
    "    W.requires_grad_()\n",
    "    dataset = TensorDataset(hat_X, y)\n",
    "    data_iter = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "    for epoch in range(num_epoch):\n",
    "        for t_x, t_y in data_iter:\n",
    "            l = cross_entropy(t_x, t_y, W)        \n",
    "            l.backward()  # 计算损失函数在 W 上的梯度\n",
    "            W.data.sub_(lr*W.grad/batch_size)\n",
    "            W.grad.data.zero_()\n",
    "            \n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            with torch.no_grad():  # 不计算梯度，加速损失函数的运算\n",
    "                train_l = cross_entropy(hat_X, y, W)  # 最近一次的负对数似然率\n",
    "                est_W = W.detach().numpy()  # detach得到一个有着和原tensor相同数据的tensor\n",
    "                print(f'epoch {epoch + 1}, loss: {train_l:.4f}')\n",
    "            \n",
    "    return est_W, train_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, loss: 3.6484\n",
      "epoch 100, loss: 1.9902\n",
      "epoch 150, loss: 1.1937\n",
      "epoch 200, loss: 0.8582\n",
      "epoch 250, loss: 0.7116\n",
      "epoch 300, loss: 0.6342\n",
      "epoch 350, loss: 0.5853\n",
      "epoch 400, loss: 0.5504\n",
      "epoch 450, loss: 0.5234\n",
      "epoch 500, loss: 0.5014\n",
      "epoch 550, loss: 0.4828\n",
      "epoch 600, loss: 0.4668\n",
      "epoch 650, loss: 0.4527\n",
      "epoch 700, loss: 0.4402\n",
      "epoch 750, loss: 0.4289\n",
      "epoch 800, loss: 0.4188\n",
      "epoch 850, loss: 0.4095\n",
      "epoch 900, loss: 0.4009\n",
      "epoch 950, loss: 0.3930\n",
      "epoch 1000, loss: 0.3857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.95, 0.94)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模拟数据\n",
    "est_W, train_l = softmax_miniBatch_sgd(train_X, train_y, num_epoch=1000, batch_size=40, lr=0.1)\n",
    "precision_rate(train_X, train_y, est_W), precision_rate(test_X, test_y, est_W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 方法3: 利用`torch.nn`构建模型，完成参数估计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SofmaxRegresModel(torch.nn.Module): \n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        # 首先找到LinearModel的父类torch.nn.Module，然后把类LinearModel的对象转换为类torch.nn.Module的对象, \n",
    "        # 即执行父类torch.nn.Module的初始化__init__()\n",
    "        super(SofmaxRegresModel, self).__init__() \n",
    "        self.linear = torch.nn.Linear(dim_in, dim_out, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return torch.nn.functional.softmax(y_pred, dim=1)  # softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义训练算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 8]), torch.Size([1000, 5]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_in = X.shape[1]\n",
    "dim_out = y.shape[1]\n",
    "# 实例化1个神经网络模型\n",
    "net = SofmaxRegresModel(dim_in, dim_out)\n",
    "# 初始化网络参数和偏置\n",
    "net.linear.weight.data = torch.randn(dim_out, dim_in)\n",
    "net.linear.bias.data = torch.zeros(dim_out)\n",
    "# 损失函数\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "# 随机梯度下降算法\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20, loss 1.530058741569519\n",
      "epoch 40, loss 1.3637938499450684\n",
      "epoch 60, loss 1.2479828596115112\n",
      "epoch 80, loss 1.140608787536621\n",
      "epoch 100, loss 1.0810521841049194\n",
      "epoch 120, loss 1.0579485893249512\n",
      "epoch 140, loss 1.0455591678619385\n",
      "epoch 160, loss 1.0366816520690918\n",
      "epoch 180, loss 1.0297178030014038\n",
      "epoch 200, loss 1.0240322351455688\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "batch_size = 20\n",
    "num_epochs = 200\n",
    "dataset = TensorDataset(train_X, train_indices_y)\n",
    "data_iter = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "# 开始训练\n",
    "for epoch in range(num_epochs):\n",
    "    for t_x, t_y in data_iter:\n",
    "        l = loss(net(t_x), t_y)  # 计算当前批量的交叉熵损失\n",
    "        trainer.zero_grad()  # 参数梯度清零\n",
    "        l.backward()  # 反向传播，计算梯度\n",
    "        trainer.step()  # 更新参数\n",
    "        \n",
    "    if (epoch+1) % 20 == 0:\n",
    "        with torch.no_grad():  # 不计算梯度，加速损失函数的运算\n",
    "            l_epoch = loss(net(train_X), train_indices_y) \n",
    "            print('epoch {}, loss {}'.format(epoch+1, l_epoch)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0.955\n",
      "test 0.935\n"
     ]
    }
   ],
   "source": [
    "w, b = net.parameters()\n",
    "W = torch.cat([w.data, b.data.reshape(-1, 1)], axis=1)\n",
    "\n",
    "pred_train_y = torch.max(net(train_X), axis=1)[1]\n",
    "pred_test_y = torch.max(net(test_X), axis=1)[1]\n",
    "\n",
    "print('train', torch.sum(pred_train_y == train_indices_y).numpy() / train_indices_y.numel())\n",
    "print('test', torch.sum(pred_test_y == test_indices_y).numpy() / test_indices_y.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 案例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "鸢尾花数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "d = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "print(d.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_labels, y_labels = d['feature_names'], d['target_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " array(['setosa', 'versicolor', 'virginica'], dtype='<U10'))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_labels, y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_idx = np.arange(len(d['target']))  # 样本下标\n",
    "np.random.shuffle(rand_idx)  # 打乱样本下标\n",
    "t_idx = rand_idx[:100]  # 训练集下标\n",
    "v_idx = rand_idx[100:]  # 测试集下标\n",
    "x_train, y_train = torch.from_numpy(d['data'][t_idx]).type(torch.FloatTensor), torch.from_numpy(d['target'][t_idx])  # 训练集\n",
    "x_valid, y_valid = torch.from_numpy(d['data'][v_idx]).type(torch.FloatTensor), torch.from_numpy(d['target'][v_idx])  # 测试集\n",
    "\n",
    "onehot_y_train = torch.zeros(x_train.shape[0], 3)  # 训练集标签转为one-hot向量\n",
    "onehot_y_train[torch.arange(x_train.shape[0]), y_train] = 1\n",
    "onehot_y_valid = torch.zeros(x_valid.shape[0], 3)  # 预测集标签转为one-hot向量\n",
    "onehot_y_valid[torch.arange(x_valid.shape[0]), y_valid] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 2, 2, 0, 0, 0, 2, 0, 2, 2, 0, 2, 1, 0, 2, 0, 1, 2, 2, 1, 0, 0,\n",
       "        1, 1, 2, 2, 1, 2, 1, 1, 2, 2, 2, 0, 1, 1, 0, 2, 2, 1, 1, 0, 0, 1, 2, 0,\n",
       "        2, 0, 1, 0, 0, 0, 2, 2, 0, 0, 0, 2, 1, 0, 0, 2, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "        2, 2, 2, 2, 2, 1, 0, 2, 0, 1, 1, 0, 2, 2, 0, 0, 1, 0, 2, 1, 2, 0, 0, 1,\n",
       "        2, 2, 1, 2])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 方法1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练50轮, 交叉熵为0.80\n",
      "训练100轮, 交叉熵为0.59\n",
      "训练150轮, 交叉熵为0.46\n",
      "训练200轮, 交叉熵为0.40\n",
      "训练250轮, 交叉熵为0.36\n",
      "训练300轮, 交叉熵为0.33\n",
      "训练350轮, 交叉熵为0.31\n",
      "训练400轮, 交叉熵为0.29\n",
      "训练450轮, 交叉熵为0.27\n",
      "训练500轮, 交叉熵为0.25\n",
      "训练550轮, 交叉熵为0.24\n",
      "训练600轮, 交叉熵为0.23\n",
      "训练650轮, 交叉熵为0.22\n",
      "训练700轮, 交叉熵为0.21\n",
      "训练750轮, 交叉熵为0.20\n",
      "训练800轮, 交叉熵为0.20\n",
      "训练850轮, 交叉熵为0.19\n",
      "训练900轮, 交叉熵为0.18\n",
      "训练950轮, 交叉熵为0.18\n",
      "训练1000轮, 交叉熵为0.17\n",
      "Train accuracy rate: 0.97\n",
      "Valid accuracy rate: 0.96\n"
     ]
    }
   ],
   "source": [
    "est_W = softmax_sgd(x_train, onehot_y_train, num_steps=1000)\n",
    "print(f\"Train accuracy rate: {precision_rate(x_train, onehot_y_train, est_W)}\")\n",
    "print(f\"Valid accuracy rate: {precision_rate(x_valid, onehot_y_valid, est_W)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 方法2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, loss: 3.1562\n",
      "epoch 100, loss: 1.3976\n",
      "epoch 150, loss: 1.0792\n",
      "epoch 200, loss: 0.9179\n",
      "epoch 250, loss: 0.8240\n",
      "epoch 300, loss: 0.7614\n",
      "epoch 350, loss: 0.7160\n",
      "epoch 400, loss: 0.6807\n",
      "epoch 450, loss: 0.6522\n",
      "epoch 500, loss: 0.6278\n",
      "epoch 550, loss: 0.6071\n",
      "epoch 600, loss: 0.5886\n",
      "epoch 650, loss: 0.5721\n",
      "epoch 700, loss: 0.5572\n",
      "epoch 750, loss: 0.5433\n",
      "epoch 800, loss: 0.5307\n",
      "epoch 850, loss: 0.5189\n",
      "epoch 900, loss: 0.5078\n",
      "epoch 950, loss: 0.4974\n",
      "epoch 1000, loss: 0.4875\n",
      "Train accuracy rate: 0.93\n",
      "Valid accuracy rate: 0.88\n"
     ]
    }
   ],
   "source": [
    "# 鸢尾花\n",
    "est_W, _ = softmax_miniBatch_sgd(x_train, onehot_y_train, num_epoch=1000, batch_size=40, lr=0.1)\n",
    "print(f\"Train accuracy rate: {precision_rate(x_train, onehot_y_train, est_W)}\")\n",
    "print(f\"Valid accuracy rate: {precision_rate(x_valid, onehot_y_valid, est_W)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 方法3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化\n",
    "dim_in = 4  # 特征数量\n",
    "dim_out = 3  # 类别数量\n",
    "net = SofmaxRegresModel(dim_in, dim_out)\n",
    "# 初始化网络参数和偏置\n",
    "net.linear.weight.data = torch.randn(dim_out, dim_in)\n",
    "net.linear.bias.data = torch.randn(dim_out)\n",
    "# 损失函数\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "# 随机梯度下降算法\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.03)\n",
    "# 加载数据\n",
    "batch_size = 100\n",
    "num_epochs = 2000\n",
    "dataset = TensorDataset(x_train, y_train)\n",
    "data_iter = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, loss 1.1497149467468262\n",
      "epoch 100, loss 1.1343947649002075\n",
      "epoch 150, loss 1.1326546669006348\n",
      "epoch 200, loss 1.1309666633605957\n",
      "epoch 250, loss 1.1291085481643677\n",
      "epoch 300, loss 1.126813292503357\n",
      "epoch 350, loss 1.1233444213867188\n",
      "epoch 400, loss 1.1148219108581543\n",
      "epoch 450, loss 0.9585862755775452\n",
      "epoch 500, loss 0.849892795085907\n",
      "epoch 550, loss 0.8413602709770203\n",
      "epoch 600, loss 0.834632933139801\n",
      "epoch 650, loss 0.8284794688224792\n",
      "epoch 700, loss 0.822733461856842\n",
      "epoch 750, loss 0.8172634243965149\n",
      "epoch 800, loss 0.8119553327560425\n",
      "epoch 850, loss 0.806735098361969\n",
      "epoch 900, loss 0.8015611171722412\n",
      "epoch 950, loss 0.7964146137237549\n",
      "epoch 1000, loss 0.7912924289703369\n",
      "epoch 1050, loss 0.7862006425857544\n",
      "epoch 1100, loss 0.7811508178710938\n",
      "epoch 1150, loss 0.7761569023132324\n",
      "epoch 1200, loss 0.7712336182594299\n",
      "epoch 1250, loss 0.7663949728012085\n",
      "epoch 1300, loss 0.7616536021232605\n",
      "epoch 1350, loss 0.7570202350616455\n",
      "epoch 1400, loss 0.7525041103363037\n",
      "epoch 1450, loss 0.7481120228767395\n",
      "epoch 1500, loss 0.7438492774963379\n",
      "epoch 1550, loss 0.7397191524505615\n",
      "epoch 1600, loss 0.7357236742973328\n",
      "epoch 1650, loss 0.7318633198738098\n",
      "epoch 1700, loss 0.7281374931335449\n",
      "epoch 1750, loss 0.7245450019836426\n",
      "epoch 1800, loss 0.7210835218429565\n",
      "epoch 1850, loss 0.7177501916885376\n",
      "epoch 1900, loss 0.7145415544509888\n",
      "epoch 1950, loss 0.7114540338516235\n",
      "epoch 2000, loss 0.7084836363792419\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "for epoch in range(num_epochs):\n",
    "    for t_x, t_y in data_iter:\n",
    "        l = loss(net(t_x), t_y)  # 计算当前批量的交叉熵损失\n",
    "        trainer.zero_grad()  # 参数梯度清零\n",
    "        l.backward()  # 反向传播，计算梯度\n",
    "        trainer.step()  # 更新参数\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        with torch.no_grad():  # 不计算梯度，加速损失函数的运算\n",
    "            l_epoch = loss(net(x_train), y_train) \n",
    "            print('epoch {}, loss {}'.format(epoch+1, l_epoch)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = net.parameters()\n",
    "W = torch.cat([w.data, b.data.reshape(-1, 1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_train = torch.max(net(x_train), axis=1)[1]\n",
    "pred_y_valid = torch.max(net(x_valid), axis=1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 2, 2, 0, 0, 0, 2, 0, 2, 2, 0, 2, 2, 0, 2, 0, 1, 2, 2, 1, 0, 0,\n",
       "        1, 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 0, 1, 2, 0, 2, 2, 1, 1, 0, 0, 1, 2, 0,\n",
       "        2, 0, 1, 0, 0, 0, 2, 2, 0, 0, 0, 2, 1, 0, 0, 2, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "        2, 2, 2, 2, 2, 1, 0, 2, 0, 1, 1, 0, 2, 2, 0, 0, 1, 0, 2, 1, 2, 0, 0, 1,\n",
       "        2, 2, 1, 2])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(pred_y_train == y_train).numpy() / y_train.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(pred_y_valid == y_valid).numpy() / y_valid.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 附. 熵相关概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一条信息的信息量大小和它的不确定性有很大的关系。一句话如果能带来很多让我们不知道的内容，我们就称这句话的信息量比较大。比如你听到“云南西双版纳下雪了”，如果你要去验证这句话的真假，则需要去看天气预报、问当地人等等查证（因为云南西双版纳从没下过雪）。相反，如果和你说“人一天要吃三顿饭”，那这条信息的信息量就很小，因为这条信息的确定性很高，我们不需要用很多信息取证明它。因此，可将事件$x_0$的信息量表示为：\n",
    "$$\n",
    "I(x_0)=-\\log p(x_0)\n",
    "$$\n",
    "> 一条信息信息量的大小与其发生概率（不确定性越小，发生概率越大）呈反比，发生概率越大，信息量越小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A1. 熵\n",
    "信息量是对于单个事件来说的，但是实际情况一件事有很多种发生的可能，比如掷骰子有可能出现6种情况，明天的天气可能晴、多云或者下雨等等。因此，我们需要评估事件对应的所有可能性。\n",
    "\n",
    "熵（entropy）是表示随机变量不确定的度量，是对表征所有可能发生的事件所需信息量的期望。\n",
    "\n",
    "设$X$是一个取有限个值的随机变量，其概率分布为\n",
    "$$\n",
    "P(X=x_i)=p_i,i=1,2,...,n\n",
    "$$\n",
    "熵定义为\n",
    "$$\n",
    "H(x)=\\sum_{i=0} p(x_i) I(x_i)=-\\sum_{i=1}^n p(x_i) \\log p(x_i)\n",
    "$$\n",
    "上式中，若$p_i=0$，则定义$0\\log 0=0$；对数以2或者e为底，这时熵的单位分别称为比特(bit)或者纳特(nat)。熵只依赖于$X$的分布，与其取值无关，因此也可将$X$的熵记作$H(p)$, 即\n",
    "$$\n",
    "H(p)=-\\sum_{i=1}^n p_i \\log p_i\n",
    "$$\n",
    "熵越大，不确定越大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(P):\n",
    "    '''\n",
    "    P为概率分布\n",
    "    '''\n",
    "    return -np.sum([p*np.log2(p) if p > 0 else 0 for p in P])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.321928094887362, -0.0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P1 = np.ones(10) / 10\n",
    "P2 = np.zeros(10)\n",
    "P2[3] = 1\n",
    "entropy(P1), entropy(P2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "P3 = np.array([0, 0.1, 0.1, 0.1, 0.2, 0.2, 0.1, 0.1, 0.1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9219280948873623"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(P3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2. 条件熵\n",
    "条件熵(conditional entropy): 表示在已知随机变量$X$的条件下随机变量$Y$的不确定性。\n",
    "\n",
    "$$\n",
    "H(Y|X)=\\sum_{i=1}^n P(X=x_i)H(Y|X=x_i)\n",
    "$$\n",
    "\n",
    "其中，$H(Y|X=x_i)=-\\sum_j P(Y=y_j|X=x_i)\\log P(Y=y_j|X=x_i)$，表示在$X=x_i$时Y的不确定程度；$p(Y=y_j|X=x_i) = \\frac{p(X=x_i, Y=y_j)}{p(X=x_i)}$。\n",
    "> 如果X与Y无关，则有$H(Y|X)=H(Y)$；如果Y由X唯一决定，则有$H(Y|X)=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_entropy(P_XY):\n",
    "    '''\n",
    "    P_XY为X和Y的联合概率分布shape(x_size, y_z)\n",
    "    '''\n",
    "    return np.sum([np.sum(P_XY[i]) * entropy(P_XY[i, :]/np.sum(P_XY[i])) \n",
    "                   for i in P_XY.shape[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A3. KL散度（相对熵）\n",
    "\n",
    "相对熵(`relative entropy`)或KL散度(`Kullback-Leibler divergence`)：度量一个概率分布$p(x)$相对另一个概率分布$q(X)$的差异\n",
    "\n",
    "$$\n",
    "\\text{KL(p||q)}=-\\sum_x p(x)\\log\\frac{q(x)}{p(x)}\n",
    "$$\n",
    "\n",
    "由`Jesen`不等式可证明，$\\text{KL(p||q)}\\geq 0$，当且仅当对于所有$x$有$p(x)=q(x)$时，取等号。\n",
    "\n",
    "此外，需注意，$\\text{KL(p||q)}\\neq \\text{KL(q||p)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 注意，要讨论p_x[i] == 0时候p_x[i]*np.log(q_x[i]/p_x[i]) 的计算问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL(p_x, q_x):\n",
    "    return -np.sum([p_x[i]*np.log(q_x[i]/p_x[i]) if p_x[i] > 0 and q_x[i] > 0 else 0 \n",
    "                    for i in range(len(p_x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.0, 2.3025850929940455)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KL(P1, P1), KL(P2, P1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A4. 交叉熵\n",
    "\n",
    "交叉熵定义如下:\n",
    "\n",
    "$$\n",
    "\\text{crossEntropy(p(x), q(x))} = -\\sum_x p(x)\\log q(x)\n",
    "$$\n",
    "\n",
    "- 与KL散度的关系\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{KL(p||q)} &= -\\sum_x p(x)\\log\\frac{q(x)}{p(x)}\\\\\n",
    "&= -\\sum_x p(x)\\log q(x) + \\sum_x p(x)\\log p(x) \\\\\n",
    "&= \\text{crossEntropy(p(x), q(x))} - H\\left(p(x)\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "即有$\\text{crossEntropy(p(x), q(x))} = \\text{KL(p||q)} + H\\left(p(x)\\right)$\n",
    "\n",
    "由于$H\\left(p(x)\\right)$为定值，针对q最小化交叉熵等价于最小化`KL(p||q)`，即使理论分布与抽样分布之间的差异最小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(p_x, q_x):\n",
    "    return -np.sum([p_x[i]*np.log(q_x[i]) if q_x[i] > 0 else 0 for i in range(len(p_x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.0, 2.3025850929940455, -0.0)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(P1, P2), cross_entropy(P1, P1), cross_entropy(P2, P2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考资料\n",
    "1. 李航. 统计学习方法. 2017.\n",
    "2. 邱锡鹏. 神经网络与机器学习. 2020."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
