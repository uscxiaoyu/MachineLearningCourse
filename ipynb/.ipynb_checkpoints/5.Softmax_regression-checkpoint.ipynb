{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 五. `softmax`回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 概述\n",
    "\n",
    "Softmax回归（Softmax Regression），也称为多项（Multinomial）或多类（Multi-Class）的Logistic回归，是Logistic回归在多分类问题上的推广。\n",
    "\n",
    "对于多类问题，类别标签$y \\in {1, 2,..., C}$ 可以有C个取值．给定一个样本x，Softmax 回归预测的属于类别c的条件概率为\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(y=c|\\mathbf{x})&=\\mathrm{softmax}(\\mathbf{w^T_cx})\\\\\n",
    "&=\\frac{\\exp(\\mathbf{w^T_cx})}{\\sum_{i=1}^C \\exp(\\mathbf{w^T_ix})}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中$\\mathbf{w_i}$是第i类的权重向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = ['svg']\n",
    "%matplotlib inline\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, W):\n",
    "    \"\"\"\n",
    "    X: torch.FloatTensor, N*a, N样本数量, a为特征的维度\n",
    "    W: torch.FloatTensor, a*C, C为类别数量\n",
    "    \"\"\"\n",
    "    C = torch.exp(X@W)  # hat_y, N*C\n",
    "    return C / torch.sum(C, axis=1).reshape(X.shape[0], -1)  # 各样本对应类别的标准化概率分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.4598e-01, 3.9624e-01, 2.5778e-01],\n",
       "        [7.1913e-02, 9.2747e-01, 6.1734e-04],\n",
       "        [6.3532e-02, 9.3405e-01, 2.4192e-03],\n",
       "        [4.4572e-01, 4.4060e-01, 1.1368e-01],\n",
       "        [8.8572e-02, 8.9762e-01, 1.3809e-02],\n",
       "        [1.7300e-01, 5.1218e-03, 8.2188e-01],\n",
       "        [2.2291e-02, 2.9059e-03, 9.7480e-01],\n",
       "        [3.6031e-02, 9.0166e-04, 9.6307e-01],\n",
       "        [3.3506e-01, 7.0396e-02, 5.9455e-01],\n",
       "        [2.1375e-01, 3.3893e-01, 4.4732e-01]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(10, 5)\n",
    "W = torch.randn(5, 3)\n",
    "softmax(X, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Softmax回归的决策函数\n",
    "\n",
    "Softmax回归的决策函数可以表示为\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{y}&=\\text{arg}\\max_{i=1}^{C}p(y=c|\\mathbf{x})\\\\\n",
    "&=\\text{arg}\\max_{i=1}^{C}\\mathbf{w_i^Tx}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hat_y(X, W):\n",
    "    S = softmax(X, W)  # 各样本在各类别上的概率\n",
    "    max_indices = torch.max(S, dim=1)[1]\n",
    "    pred_y = torch.zeros_like(S)\n",
    "    pred_y[torch.arange(S.shape[0]), max_indices] = 1\n",
    "    return max_indices, pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = hat_y(X, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 0, 1, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 与`Logistic`回归的关系。当类别数$C=2$时，softmax回归的决策函数为\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{y}&=\\text{arg}\\max_{i\\in\\{1,2\\}}p(y=c|\\mathbf{x})\\\\\n",
    "&=\\text{arg}\\max_{i\\in\\{1,2\\}}\\mathbf{w_i^Tx}\\\\\n",
    "&=I(\\mathbf{(w_2-w_1)^Tx}>0))\n",
    "\\end{aligned}\n",
    "$$\n",
    "其中$I(\\cdot)$是指示函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 准则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定N个训练样本，Softmax回归使用交叉熵损失函数学习最有的参数矩阵$W$。为了方便起见，使用C维的`one-hot`向量表示类别标签，对于类别i，其向量表示为\n",
    "$$\n",
    "y = [I(i=1), I(i=2), ..., I(i=C)]\n",
    "$$\n",
    "\n",
    "采用交叉熵损失函数，Softmax回归模型的风险函数是\n",
    "$$\n",
    "\\begin{aligned}\n",
    "R(\\mathbf{W})&=-\\frac{1}{N}\\sum_{n=1}^N\\sum_{i=1}^{C}y_i^{(n)}\\log \\hat{y}_i^{(n)}\\\\\n",
    "&=-\\frac{1}{N}\\sum_{n=1}^N(\\mathbf{y^{(n)}})^T\\log \\hat{y}_c^{(n)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "其中，$\\hat{y}_c^{(n)}=\\text{softmax}(\\mathbf{W^Tx^{(n)}})$为样本$x^{(n)}$在每个类别的后验概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(X, y, W):\n",
    "    \"\"\"\n",
    "    X: N*(a+1), N个样本, 特征数量为为a, 外加1维偏置\n",
    "    y: N*C, y为N个C维的one-hot向量\n",
    "    W: (a+1)*C\n",
    "    \"\"\"\n",
    "    p_y = softmax(X, W) # N*C, N个样本分别在C个类别的后验概率\n",
    "    crossEnt = -torch.dot(y.reshape(-1), torch.log2(p_y).reshape(-1)) / y.shape[0]  # 展开成1维，点积\n",
    "    return crossEnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(10, 5)\n",
    "y = torch.zeros(10, 3)\n",
    "y[torch.arange(10), torch.randint(low=0, high=y.shape[1] - 1, size=(10,))] = 1  # 随机取1行中的某个元素为1，即确定1个样本对应的类别\n",
    "W = torch.randn(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(285), tensor(285))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(10)\n",
    "b = torch.arange(10)\n",
    "\n",
    "torch.sum(a * b), torch.dot(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y  # 真实标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8494)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(X, y, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_y = softmax(X, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.6241e-04, 1.1254e+01, 1.2376e+01],\n",
       "        [1.0368e+01, 7.9855e-01, 1.2368e+00],\n",
       "        [2.1208e+00, 1.2259e+00, 1.5457e+00],\n",
       "        [2.8011e-02, 5.8849e+00, 8.7603e+00],\n",
       "        [6.1275e-01, 1.8015e+00, 4.0790e+00],\n",
       "        [3.0443e+00, 2.1828e+00, 6.0268e-01],\n",
       "        [4.9242e-02, 4.9325e+00, 1.0272e+01],\n",
       "        [5.8039e+00, 3.4604e+00, 1.6609e-01],\n",
       "        [3.0465e-01, 2.9941e+00, 3.9468e+00],\n",
       "        [2.5823e+00, 7.3544e-01, 2.1054e+00]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.log2(prob_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8494)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.dot(y.reshape(-1), torch.log2(prob_y).reshape(-1)) / y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 风险函数$\\mathbf{R(W)}$关于$W$的梯度为\n",
    "$$\n",
    "\\frac{\\partial R(W)}{\\partial W}=-\\frac{1}{N}\\sum_{n=1}^N\\mathbf{x^{(n)}(y^{(n)}-\\hat{y}^{(n)})}^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_crosEnt_W(X, y, W):\n",
    "    '''\n",
    "    X: N*(a+1), N个样本, 特征数量为为a, 外加1维偏置\n",
    "    y: N*C, y为N个C维的one-hot向量\n",
    "    W: (a+1)*C\n",
    "    '''\n",
    "    hat_y = softmax(X, W)\n",
    "    a = (X.t() @ (y - hat_y)) / y.shape[0]  # (a+1)*N | N*C\n",
    "    return -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0322,  0.0375, -0.0053],\n",
       "        [ 0.0408, -0.0337, -0.0071],\n",
       "        [-0.2919,  0.1514,  0.1406],\n",
       "        [ 0.1522,  0.0413, -0.1935],\n",
       "        [-0.0409,  0.0083,  0.0326]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_crosEnt_W(X, y, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 学习方法\n",
    "\n",
    "- 输入: 训练集X，`one-hot`形式的标签y\n",
    "- 输出：最优参数$w^*$\n",
    "- 算法过程\n",
    "    - 初始化$W_0:=0$，最大迭代次数$T$\n",
    "    - 然后通过下式进行参数的迭代更新\n",
    "    $$\n",
    "    W_{t+1}:=W_t+\\eta\\left(\\frac{1}{N}\\sum_{n=1}^N\\mathbf{x^{(n)}(y^{(n)}-\\hat{y}^{(n)})}^T\\right) \n",
    "    $$\n",
    "    - 直到满足指定迭代次数，令$w^*=w^T$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 预测的正确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_rate(X, y, W, X_with_bias=False):\n",
    "    if X_with_bias:\n",
    "        hat_X = X\n",
    "    else:\n",
    "        hat_X = torch.cat([X, torch.ones(X.shape[0], 1)], axis=1)  # 增广X\n",
    "        \n",
    "    pred_y = hat_y(hat_X, W)\n",
    "    precision = torch.sum(pred_y[0] == torch.max(y, axis=1)[1]).numpy() / pred_y[0].numel()\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 生成模拟数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(1000, 8)\n",
    "hat_X = torch.cat([X, torch.ones(X.shape[0], 1)], axis=1)  # 增广\n",
    "true_W = torch.randn(hat_X.shape[1], 5)  # 增广\n",
    "indices_y, y = hat_y(hat_X, true_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = X[:800], y[:800]\n",
    "train_indices_y = indices_y[:800]\n",
    "hat_train_X = hat_X[:800]\n",
    "test_X, test_y = X[800:], y[800:]\n",
    "test_indices_y = indices_y[800:]\n",
    "hat_test_X = hat_X[800:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 方法1: 梯度下降-人工求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_sgd(X, y, num_steps=100, lr=0.1):\n",
    "    '''\n",
    "    X: N*(a+1), N个样本, 特征数量为为a, 外加1维偏置\n",
    "    y: N*C, y为N个C维的one-hot向量\n",
    "    W: (a+1)*C\n",
    "    '''\n",
    "    hat_X = torch.cat([X, torch.ones(X.shape[0], 1)], axis=1)  # 增广X\n",
    "    W = torch.randn(hat_X.shape[1], y.shape[1])  # 增广参数矩阵\n",
    "    for i in range(num_steps):\n",
    "        W -= lr*grad_crosEnt_W(hat_X, y, W)\n",
    "        loss = cross_entropy(hat_X, y, W)\n",
    "        if (i+1) % 50 == 0:\n",
    "            print(f'训练{i+1}轮, 交叉熵为{loss:.2f}')\n",
    "            \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练50轮, 交叉熵为1.90\n",
      "训练100轮, 交叉熵为1.05\n",
      "训练150轮, 交叉熵为0.76\n",
      "训练200轮, 交叉熵为0.64\n",
      "训练250轮, 交叉熵为0.57\n",
      "训练300轮, 交叉熵为0.52\n",
      "训练350轮, 交叉熵为0.49\n",
      "训练400轮, 交叉熵为0.46\n",
      "训练450轮, 交叉熵为0.44\n",
      "训练500轮, 交叉熵为0.42\n",
      "训练550轮, 交叉熵为0.41\n",
      "训练600轮, 交叉熵为0.40\n",
      "训练650轮, 交叉熵为0.38\n",
      "训练700轮, 交叉熵为0.37\n",
      "训练750轮, 交叉熵为0.36\n",
      "训练800轮, 交叉熵为0.36\n",
      "训练850轮, 交叉熵为0.35\n",
      "训练900轮, 交叉熵为0.34\n",
      "训练950轮, 交叉熵为0.33\n",
      "训练1000轮, 交叉熵为0.33\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.96, 0.935)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模拟数据\n",
    "est_W = softmax_sgd(train_X, train_y, num_steps=1000)\n",
    "precision_rate(train_X, train_y, est_W, X_with_bias=False), precision_rate(test_X, test_y, est_W, X_with_bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 方法2: 随机梯度下降-自动求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_miniBatch_sgd(X, y, num_epoch=50, batch_size=40, lr=0.05):\n",
    "    '''\n",
    "    X: N*a, N个样本, 特征数量为为a\n",
    "    y: N*C, y为N个C维的one-hot向量\n",
    "    W: a*C\n",
    "    '''\n",
    "    hat_X = torch.cat([X, torch.ones(X.shape[0], 1)], axis=1)  # 增广X\n",
    "    W = torch.randn(hat_X.shape[1], y.shape[1])  # 增广参数矩阵\n",
    "    W.requires_grad_()\n",
    "    dataset = TensorDataset(hat_X, y)\n",
    "    data_iter = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "    for epoch in range(num_epoch):\n",
    "        for t_x, t_y in data_iter:\n",
    "            l = cross_entropy(t_x, t_y, W)        \n",
    "            l.backward()  # 计算损失函数在 W 上的梯度\n",
    "            W.data.sub_(lr*W.grad/batch_size)\n",
    "            W.grad.data.zero_()\n",
    "            \n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            with torch.no_grad():  # 不计算梯度，加速损失函数的运算\n",
    "                train_l = cross_entropy(hat_X, y, W)  # 最近一次的负对数似然率\n",
    "                est_W = W.detach().numpy()  # detach得到一个有着和原tensor相同数据的tensor\n",
    "                print(f'epoch {epoch + 1}, loss: {train_l:.4f}')\n",
    "            \n",
    "    return est_W, train_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, loss: 2.6921\n",
      "epoch 100, loss: 1.4816\n",
      "epoch 150, loss: 0.9564\n",
      "epoch 200, loss: 0.7273\n",
      "epoch 250, loss: 0.6169\n",
      "epoch 300, loss: 0.5552\n",
      "epoch 350, loss: 0.5157\n",
      "epoch 400, loss: 0.4876\n",
      "epoch 450, loss: 0.4660\n",
      "epoch 500, loss: 0.4485\n",
      "epoch 550, loss: 0.4337\n",
      "epoch 600, loss: 0.4210\n",
      "epoch 650, loss: 0.4097\n",
      "epoch 700, loss: 0.3996\n",
      "epoch 750, loss: 0.3905\n",
      "epoch 800, loss: 0.3821\n",
      "epoch 850, loss: 0.3745\n",
      "epoch 900, loss: 0.3674\n",
      "epoch 950, loss: 0.3608\n",
      "epoch 1000, loss: 0.3547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.955, 0.93)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模拟数据\n",
    "est_W, train_l = softmax_miniBatch_sgd(train_X, train_y, num_epoch=1000, batch_size=40, lr=0.1)\n",
    "precision_rate(train_X, train_y, est_W), precision_rate(test_X, test_y, est_W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 方法3: torch.nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SofmaxRegresModel(torch.nn.Module): \n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        # 首先找到LinearModel的父类torch.nn.Module，然后把类LinearModel的对象转换为类torch.nn.Module的对象, \n",
    "        # 即执行父类torch.nn.Module的初始化__init__()\n",
    "        super(SofmaxRegresModel, self).__init__() \n",
    "        self.layer1 = torch.nn.Linear(dim_in, dim_out, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = self.layer1(x)\n",
    "        return torch.nn.functional.softmax(y_pred, dim=1)  # softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义训练算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 8]), torch.Size([1000, 5]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_in = X.shape[1]\n",
    "dim_out = y.shape[1]\n",
    "# 实例化1个网络\n",
    "net = SofmaxRegresModel(dim_in, dim_out)\n",
    "# 初始化网络参数和偏置\n",
    "net.layer1.weight.data = torch.randn(dim_out, dim_in)\n",
    "net.layer1.bias.data = torch.Tensor(dim_out)\n",
    "# 损失函数\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "# 随机梯度下降算法\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20, loss 1.370640754699707\n",
      "epoch 40, loss 1.1573972702026367\n",
      "epoch 60, loss 1.0841116905212402\n",
      "epoch 80, loss 1.0599939823150635\n",
      "epoch 100, loss 1.0462019443511963\n",
      "epoch 120, loss 1.0364203453063965\n",
      "epoch 140, loss 1.0288468599319458\n",
      "epoch 160, loss 1.0227004289627075\n",
      "epoch 180, loss 1.0175586938858032\n",
      "epoch 200, loss 1.0131640434265137\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "batch_size = 20\n",
    "num_epochs = 200\n",
    "dataset = TensorDataset(train_X, train_indices_y)\n",
    "data_iter = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "# 开始训练\n",
    "for epoch in range(num_epochs):\n",
    "    for t_x, t_y in data_iter:\n",
    "        l = loss(net(t_x), t_y)  # 计算当前批量的交叉熵损失\n",
    "        trainer.zero_grad()  # 参数梯度清零\n",
    "        l.backward()  # 反向传播，计算梯度\n",
    "        trainer.step()  # 更新参数\n",
    "        \n",
    "    if (epoch+1) % 20 == 0:\n",
    "        with torch.no_grad():  # 不计算梯度，加速损失函数的运算\n",
    "            l_epoch = loss(net(train_X), train_indices_y) \n",
    "            print('epoch {}, loss {}'.format(epoch+1, l_epoch)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0.97\n",
      "test 0.945\n"
     ]
    }
   ],
   "source": [
    "w, b = net.parameters()\n",
    "W = torch.cat([w.data, b.data.reshape(-1, 1)], axis=1)\n",
    "\n",
    "pred_train_y = torch.max(net(train_X), axis=1)[1]\n",
    "pred_test_y = torch.max(net(test_X), axis=1)[1]\n",
    "\n",
    "print('train', torch.sum(pred_train_y == train_indices_y).numpy() / train_indices_y.numel())\n",
    "print('test', torch.sum(pred_test_y == test_indices_y).numpy() / test_indices_y.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 案例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "鸢尾花数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "d = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "print(d.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_labels, y_labels = d['feature_names'], d['target_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " array(['setosa', 'versicolor', 'virginica'], dtype='<U10'))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_labels, y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_idx = np.arange(len(d['target']))  # 样本下标\n",
    "np.random.shuffle(rand_idx)  # 打乱样本下标\n",
    "t_idx = rand_idx[:100]  # 训练集下标\n",
    "v_idx = rand_idx[100:]  # 测试集下标\n",
    "x_train, y_train = torch.from_numpy(d['data'][t_idx]).type(torch.FloatTensor), torch.from_numpy(d['target'][t_idx])  # 训练集\n",
    "x_valid, y_valid = torch.from_numpy(d['data'][v_idx]).type(torch.FloatTensor), torch.from_numpy(d['target'][v_idx])  # 测试集\n",
    "onehot_y_train = torch.zeros(x_train.shape[0], 3)  # 训练集标签转为one-hot向量\n",
    "onehot_y_train[torch.arange(x_train.shape[0]), y_train] = 1\n",
    "onehot_y_valid = torch.zeros(x_valid.shape[0], 3)  # 预测集标签转为one-hot向量\n",
    "onehot_y_valid[torch.arange(x_valid.shape[0]), y_valid] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 2, 0, 1, 2, 0, 1, 0, 1, 0, 2,\n",
       "        0, 2, 2, 0, 1, 2, 0, 1, 0, 0, 2, 2, 1, 2, 1, 2, 0, 2, 2, 0, 1, 0, 0, 2,\n",
       "        2, 2, 0, 1, 2, 0, 2, 0, 2, 2, 1, 0, 2, 2, 1, 0, 0, 1, 2, 2, 1, 0, 1, 1,\n",
       "        0, 1, 2, 0, 2, 2, 2, 1, 0, 1, 0, 1, 2, 2, 1, 2, 2, 1, 0, 1, 2, 2, 1, 2,\n",
       "        1, 2, 0, 2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 方法1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练50轮, 交叉熵为1.04\n",
      "训练100轮, 交叉熵为0.81\n",
      "训练150轮, 交叉熵为0.62\n",
      "训练200轮, 交叉熵为0.46\n",
      "训练250轮, 交叉熵为0.39\n",
      "训练300轮, 交叉熵为0.35\n",
      "训练350轮, 交叉熵为0.33\n",
      "训练400轮, 交叉熵为0.31\n",
      "训练450轮, 交叉熵为0.29\n",
      "训练500轮, 交叉熵为0.27\n",
      "训练550轮, 交叉熵为0.26\n",
      "训练600轮, 交叉熵为0.25\n",
      "训练650轮, 交叉熵为0.24\n",
      "训练700轮, 交叉熵为0.23\n",
      "训练750轮, 交叉熵为0.23\n",
      "训练800轮, 交叉熵为0.22\n",
      "训练850轮, 交叉熵为0.21\n",
      "训练900轮, 交叉熵为0.21\n",
      "训练950轮, 交叉熵为0.20\n",
      "训练1000轮, 交叉熵为0.20\n",
      "Train accuracy rate: 0.97\n",
      "Valid accuracy rate: 0.98\n"
     ]
    }
   ],
   "source": [
    "est_W = softmax_sgd(x_train, onehot_y_train, num_steps=1000)\n",
    "print(f\"Train accuracy rate: {precision_rate(x_train, onehot_y_train, est_W)}\")\n",
    "print(f\"Valid accuracy rate: {precision_rate(x_valid, onehot_y_valid, est_W)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 方法2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, loss: 0.8281\n",
      "epoch 100, loss: 0.7560\n",
      "epoch 150, loss: 0.7091\n",
      "epoch 200, loss: 0.6748\n",
      "epoch 250, loss: 0.6482\n",
      "epoch 300, loss: 0.6265\n",
      "epoch 350, loss: 0.6082\n",
      "epoch 400, loss: 0.5922\n",
      "epoch 450, loss: 0.5780\n",
      "epoch 500, loss: 0.5651\n",
      "epoch 550, loss: 0.5533\n",
      "epoch 600, loss: 0.5424\n",
      "epoch 650, loss: 0.5326\n",
      "epoch 700, loss: 0.5228\n",
      "epoch 750, loss: 0.5141\n",
      "epoch 800, loss: 0.5052\n",
      "epoch 850, loss: 0.4972\n",
      "epoch 900, loss: 0.4897\n",
      "epoch 950, loss: 0.4823\n",
      "epoch 1000, loss: 0.4755\n",
      "Train accuracy rate: 0.9\n",
      "Valid accuracy rate: 0.96\n"
     ]
    }
   ],
   "source": [
    "# 鸢尾花\n",
    "est_W, _ = softmax_miniBatch_sgd(x_train, onehot_y_train, num_epoch=1000, batch_size=40, lr=0.1)\n",
    "print(f\"Train accuracy rate: {precision_rate(x_train, onehot_y_train, est_W)}\")\n",
    "print(f\"Valid accuracy rate: {precision_rate(x_valid, onehot_y_valid, est_W)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 方法3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化\n",
    "dim_in = 4  # 特征数量\n",
    "dim_out = 3  # 类别数量\n",
    "net = SofmaxRegresModel(dim_in, dim_out)\n",
    "# 初始化网络参数和偏置\n",
    "net.layer1.weight.data = torch.randn(dim_out, dim_in)\n",
    "net.layer1.bias.data = torch.randn(dim_out)\n",
    "# 损失函数\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "# 随机梯度下降算法\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "# 加载数据\n",
    "batch_size = 100\n",
    "num_epochs = 2000\n",
    "dataset = TensorDataset(x_train, y_train)\n",
    "data_iter = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, loss 1.2578121423721313\n",
      "epoch 100, loss 1.2572717666625977\n",
      "epoch 150, loss 1.25655996799469\n",
      "epoch 200, loss 1.255589485168457\n",
      "epoch 250, loss 1.254209041595459\n",
      "epoch 300, loss 1.25214421749115\n",
      "epoch 350, loss 1.2488677501678467\n",
      "epoch 400, loss 1.2433182001113892\n",
      "epoch 450, loss 1.2329506874084473\n",
      "epoch 500, loss 1.204219937324524\n",
      "epoch 550, loss 1.1108458042144775\n",
      "epoch 600, loss 1.0970641374588013\n",
      "epoch 650, loss 1.0919201374053955\n",
      "epoch 700, loss 1.086928367614746\n",
      "epoch 750, loss 1.082106590270996\n",
      "epoch 800, loss 1.0775548219680786\n",
      "epoch 850, loss 1.073352575302124\n",
      "epoch 900, loss 1.0695500373840332\n",
      "epoch 950, loss 1.0661671161651611\n",
      "epoch 1000, loss 1.0631977319717407\n",
      "epoch 1050, loss 1.0606169700622559\n",
      "epoch 1100, loss 1.0583884716033936\n",
      "epoch 1150, loss 1.0564707517623901\n",
      "epoch 1200, loss 1.054821491241455\n",
      "epoch 1250, loss 1.0534014701843262\n",
      "epoch 1300, loss 1.0521749258041382\n",
      "epoch 1350, loss 1.051110863685608\n",
      "epoch 1400, loss 1.0501829385757446\n",
      "epoch 1450, loss 1.0493690967559814\n",
      "epoch 1500, loss 1.0486501455307007\n",
      "epoch 1550, loss 1.048011064529419\n",
      "epoch 1600, loss 1.0474390983581543\n",
      "epoch 1650, loss 1.0469235181808472\n",
      "epoch 1700, loss 1.04645574092865\n",
      "epoch 1750, loss 1.0460283756256104\n",
      "epoch 1800, loss 1.0456355810165405\n",
      "epoch 1850, loss 1.0452719926834106\n",
      "epoch 1900, loss 1.0449339151382446\n",
      "epoch 1950, loss 1.0446172952651978\n",
      "epoch 2000, loss 1.0443196296691895\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "for epoch in range(num_epochs):\n",
    "    for t_x, t_y in data_iter:\n",
    "        l = loss(net(t_x), t_y)  # 计算当前批量的交叉熵损失\n",
    "        trainer.zero_grad()  # 参数梯度清零\n",
    "        l.backward()  # 反向传播，计算梯度\n",
    "        trainer.step()  # 更新参数\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        with torch.no_grad():  # 不计算梯度，加速损失函数的运算\n",
    "            l_epoch = loss(net(x_train), y_train) \n",
    "            print('epoch {}, loss {}'.format(epoch+1, l_epoch)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = net.parameters()\n",
    "W = torch.cat([w.data, b.data.reshape(-1, 1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_train = torch.max(net(x_train), axis=1)[1]\n",
    "pred_y_valid = torch.max(net(x_valid), axis=1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 2,\n",
       "        1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2,\n",
       "        2, 2, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1,\n",
       "        1, 1, 2, 1, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2,\n",
       "        1, 2, 1, 2])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(pred_y_train == y_train).numpy() / y_train.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(pred_y_valid == y_valid).numpy() / y_valid.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 附. 熵相关概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一条信息的信息量大小和它的不确定性有很大的关系。一句话如果需要很多外部信息才能确定，我们就称这句话的信息量比较大。比如你听到“云南西双版纳下雪了”，那你需要去看天气预报、问当地人等等查证（因为云南西双版纳从没下过雪）。相反，如果和你说“人一天要吃三顿饭”，那这条信息的信息量就很小，因为这条信息的确定性很高，我们不需要用很多信息取证明它。因此，可将事件$x_0$的信息量表示为：\n",
    "$$\n",
    "I(x_0)=-\\log p(x_0)\n",
    "$$\n",
    "> 一条信息信息量的大小与其发生概率（不确定性越小，发生概率越大）呈反比，概率越大，信息量越小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A1. 熵\n",
    "信息量是对于单个事件来说的，但是实际情况一件事有很多种发生的可能，比如掷骰子有可能出现6种情况，明天的天气可能晴、多云或者下雨等等。因此，我们需要评估事件对应的所有可能性。\n",
    "\n",
    "熵（entropy）是表示随机变量不确定的度量，是对表征所有可能发生的事件所需信息量的期望。\n",
    "\n",
    "设$X$是一个取有限个值的随机变量，其概率分布为\n",
    "$$\n",
    "P(X=x_i)=p_i,i=1,2,...,n\n",
    "$$\n",
    "熵定义为\n",
    "$$\n",
    "H(x)=\\sum_{i=0} p(x_i) I(x_i)=-\\sum_{i=1}^n p(x_i) \\log p(x_i)\n",
    "$$\n",
    "上式中，若$p_i=0$，则定义$0\\log 0=0$；对数以2或者e为底，这时熵的单位分别称为比特(bit)或者纳特(nat)。熵只依赖于$X$的分布，与其取值无关，因此也可将$X$的熵记作$H(p)$, 即\n",
    "$$\n",
    "H(p)=-\\sum_{i=1}^n p_i \\log p_i\n",
    "$$\n",
    "熵越大，不确定越大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(P):\n",
    "    '''\n",
    "    P为概率分布\n",
    "    '''\n",
    "    return -np.sum([p*np.log2(p) if p > 0 else 0 for p in P])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.321928094887362, -0.0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P1 = np.ones(10) / 10\n",
    "P2 = np.zeros(10)\n",
    "P2[3] = 1\n",
    "entropy(P1), entropy(P2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "P3 = np.array([0, 0.1, 0.1, 0.1, 0.2, 0.2, 0.1, 0.1, 0.1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9219280948873623"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(P3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2. 条件熵\n",
    "条件熵(conditional entropy): 表示在已知随机变量$X$的条件下随机变量$Y$的不确定性。\n",
    "\n",
    "$$\n",
    "H(Y|X)=\\sum_{i=1}^n P(X=x_i)H(Y|X=x_i)\n",
    "$$\n",
    "\n",
    "其中，$H(Y|X=x_i)=-\\sum_j P(Y=y_j|X=x_i)\\log P(Y=y_j|X=x_i)$，表示在$X=x_i$时Y的不确定程度；$p(Y=y_j|X=x_i) = \\frac{p(X=x_i, Y=y_j)}{p(X=x_i)}$。\n",
    "> 如果X与Y无关，则有$H(Y|X)=H(Y)$；如果Y由X唯一决定，则有$H(Y|X)=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_entropy(P_XY):\n",
    "    '''\n",
    "    P_XY为X和Y的联合概率分布shape(x_size, y_z)\n",
    "    '''\n",
    "    return np.sum([np.sum(P_XY[i]) * entropy(P_XY[i, :]/np.sum(P_XY[i])) \n",
    "                   for i in P_XY.shape[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A3. KL散度（相对熵）\n",
    "\n",
    "相对熵(`relative entropy`)或KL散度(`Kullback-Leibler divergence`)：度量一个概率分布$p(x)$相对另一个概率分布$q(X)$的差异\n",
    "\n",
    "$$\n",
    "\\text{KL(p||q)}=-\\sum_x p(x)\\log\\frac{q(x)}{p(x)}\n",
    "$$\n",
    "\n",
    "由`Jesen`不等式可证明，$\\text{KL(p||q)}\\geq 0$，当且仅当对于所有$x$有$p(x)=q(x)$时，取等号。\n",
    "\n",
    "此外，需注意，$\\text{KL(p||q)}\\neq \\text{KL(q||p)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL(p_x, q_x):\n",
    "    return -np.sum([p_x[i]*np.log(q_x[i]/p_x[i]) if p_x[i] > 0 and q_x[i] > 0 else 0 \n",
    "                    for i in range(len(p_x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.2302585092994046, -0.0, 2.3025850929940455)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KL(P1, P2), KL(P1, P1), KL(P2, P1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A4. 交叉熵\n",
    "\n",
    "交叉熵定义如下:\n",
    "\n",
    "$$\n",
    "\\text{crossEntropy(p(x), q(x))} = -\\sum_x p(x)\\log q(x)\n",
    "$$\n",
    "\n",
    "- 与KL散度的关系\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{KL(p||q)} &= -\\sum_x p(x)\\log\\frac{q(x)}{p(x)}\\\\\n",
    "&= -\\sum_x p(x)\\log q(x) + \\sum_x p(x)\\log p(x) \\\\\n",
    "&= \\text{crossEntropy(p(x), q(x))} - H\\left(p(x)\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "即有$\\text{crossEntropy(p(x), q(x))} = \\text{KL(p||q)} + H\\left(p(x)\\right)$\n",
    "\n",
    "由于$H\\left(p(x)\\right)$为定值，针对q最小化交叉熵等价于最小化`KL(p||q)`，即使理论分布与抽样分布之间的差异最小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(p_x, q_x):\n",
    "    return -np.sum([p_x[i]*np.log(q_x[i]) if q_x[i] > 0 else 0 for i in range(len(p_x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.0, 2.3025850929940455, -0.0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(P1, P2), cross_entropy(P1, P1), cross_entropy(P2, P2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考资料\n",
    "1. 李航. 统计学习方法. 2017.\n",
    "2. 邱锡鹏. 神经网络与机器学习. 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(pnovel, arc=3, rho=0.5, w=3, k=6):\n",
    "    a = w * (1- pnovel)\n",
    "    b1 = (1 - pnovel)**2 * arc * (k*rho - w)\n",
    "    b2 = pnovel + (1 - pnovel) * arc\n",
    "    return (arc/k) * (a + b1 / b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0883575490>]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"384.828125pt\" height=\"248.518125pt\" viewBox=\"0 0 384.828125 248.518125\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2023-10-18T22:17:12.675940</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.5.1, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 248.518125 \n",
       "L 384.828125 248.518125 \n",
       "L 384.828125 0 \n",
       "L 0 0 \n",
       "L 0 248.518125 \n",
       "z\n",
       "\" style=\"fill: none\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 42.828125 224.64 \n",
       "L 377.628125 224.64 \n",
       "L 377.628125 7.2 \n",
       "L 42.828125 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m9223457371\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m9223457371\" x=\"58.046307\" y=\"224.64\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(50.094744 239.238438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m9223457371\" x=\"119.53391\" y=\"224.64\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 0.2 -->\n",
       "      <g transform=\"translate(111.582348 239.238438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m9223457371\" x=\"181.021513\" y=\"224.64\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 0.4 -->\n",
       "      <g transform=\"translate(173.069951 239.238438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m9223457371\" x=\"242.509117\" y=\"224.64\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 0.6 -->\n",
       "      <g transform=\"translate(234.557554 239.238438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m9223457371\" x=\"303.99672\" y=\"224.64\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 0.8 -->\n",
       "      <g transform=\"translate(296.045158 239.238438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m9223457371\" x=\"365.484323\" y=\"224.64\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 1.0 -->\n",
       "      <g transform=\"translate(357.532761 239.238438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <defs>\n",
       "       <path id=\"m53dbcf0a8f\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m53dbcf0a8f\" x=\"42.828125\" y=\"221.652652\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0.000 -->\n",
       "      <g transform=\"translate(7.2 225.451871)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m53dbcf0a8f\" x=\"42.828125\" y=\"195.569868\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.025 -->\n",
       "      <g transform=\"translate(7.2 199.369086)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"222.65625\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m53dbcf0a8f\" x=\"42.828125\" y=\"169.487083\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.050 -->\n",
       "      <g transform=\"translate(7.2 173.286302)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m53dbcf0a8f\" x=\"42.828125\" y=\"143.404299\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.075 -->\n",
       "      <g transform=\"translate(7.2 147.203518)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \n",
       "L 3525 4666 \n",
       "L 3525 4397 \n",
       "L 1831 0 \n",
       "L 1172 0 \n",
       "L 2766 4134 \n",
       "L 525 4134 \n",
       "L 525 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-37\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"222.65625\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m53dbcf0a8f\" x=\"42.828125\" y=\"117.321515\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.100 -->\n",
       "      <g transform=\"translate(7.2 121.120734)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m53dbcf0a8f\" x=\"42.828125\" y=\"91.238731\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.125 -->\n",
       "      <g transform=\"translate(7.2 95.037949)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"222.65625\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m53dbcf0a8f\" x=\"42.828125\" y=\"65.155947\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.150 -->\n",
       "      <g transform=\"translate(7.2 68.955165)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_8\">\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m53dbcf0a8f\" x=\"42.828125\" y=\"39.073162\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 0.175 -->\n",
       "      <g transform=\"translate(7.2 42.872381)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-37\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"222.65625\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_9\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m53dbcf0a8f\" x=\"42.828125\" y=\"12.990378\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 0.200 -->\n",
       "      <g transform=\"translate(7.2 16.789597)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_16\">\n",
       "    <path d=\"M 58.046307 117.321515 \n",
       "L 61.120687 112.51185 \n",
       "L 64.195067 107.820426 \n",
       "L 67.269447 103.247245 \n",
       "L 70.343827 98.792305 \n",
       "L 73.418208 94.455607 \n",
       "L 76.492588 90.237152 \n",
       "L 79.566968 86.136938 \n",
       "L 82.641348 82.154966 \n",
       "L 85.715728 78.291237 \n",
       "L 88.790108 74.545749 \n",
       "L 91.864489 70.918503 \n",
       "L 94.938869 67.409499 \n",
       "L 98.013249 64.018737 \n",
       "L 101.087629 60.746217 \n",
       "L 104.162009 57.591939 \n",
       "L 107.236389 54.555903 \n",
       "L 110.31077 51.638109 \n",
       "L 113.38515 48.838557 \n",
       "L 116.45953 46.157246 \n",
       "L 119.53391 43.594178 \n",
       "L 122.60829 41.149352 \n",
       "L 125.68267 38.822768 \n",
       "L 128.757051 36.614425 \n",
       "L 131.831431 34.524325 \n",
       "L 134.905811 32.552466 \n",
       "L 137.980191 30.69885 \n",
       "L 141.054571 28.963475 \n",
       "L 144.128951 27.346343 \n",
       "L 147.203332 25.847452 \n",
       "L 150.277712 24.466803 \n",
       "L 153.352092 23.204396 \n",
       "L 156.426472 22.060232 \n",
       "L 159.500852 21.034309 \n",
       "L 162.575232 20.126628 \n",
       "L 165.649613 19.337189 \n",
       "L 168.723993 18.665992 \n",
       "L 171.798373 18.113037 \n",
       "L 174.872753 17.678324 \n",
       "L 177.947133 17.361853 \n",
       "L 181.021513 17.163624 \n",
       "L 184.095894 17.083636 \n",
       "L 187.170274 17.121891 \n",
       "L 190.244654 17.278388 \n",
       "L 193.319034 17.553126 \n",
       "L 196.393414 17.946107 \n",
       "L 199.467794 18.45733 \n",
       "L 202.542175 19.086794 \n",
       "L 205.616555 19.834501 \n",
       "L 208.690935 20.700449 \n",
       "L 211.765315 21.684639 \n",
       "L 214.839695 22.787072 \n",
       "L 217.914075 24.007746 \n",
       "L 220.988456 25.346662 \n",
       "L 224.062836 26.803821 \n",
       "L 227.137216 28.379221 \n",
       "L 230.211596 30.072863 \n",
       "L 233.285976 31.884747 \n",
       "L 236.360356 33.814873 \n",
       "L 239.434737 35.863241 \n",
       "L 242.509117 38.029851 \n",
       "L 245.583497 40.314703 \n",
       "L 248.657877 42.717797 \n",
       "L 251.732257 45.239132 \n",
       "L 254.806637 47.87871 \n",
       "L 257.881018 50.63653 \n",
       "L 260.955398 53.512592 \n",
       "L 264.029778 56.506895 \n",
       "L 267.104158 59.619441 \n",
       "L 270.178538 62.850228 \n",
       "L 273.252918 66.199258 \n",
       "L 276.327299 69.666529 \n",
       "L 279.401679 73.252043 \n",
       "L 282.476059 76.955798 \n",
       "L 285.550439 80.777795 \n",
       "L 288.624819 84.718035 \n",
       "L 291.699199 88.776516 \n",
       "L 294.77358 92.953239 \n",
       "L 297.84796 97.248204 \n",
       "L 300.92234 101.661411 \n",
       "L 303.99672 106.19286 \n",
       "L 307.0711 110.842551 \n",
       "L 310.14548 115.610484 \n",
       "L 313.219861 120.496659 \n",
       "L 316.294241 125.501076 \n",
       "L 319.368621 130.623735 \n",
       "L 322.443001 135.864636 \n",
       "L 325.517381 141.223778 \n",
       "L 328.591761 146.701163 \n",
       "L 331.666142 152.29679 \n",
       "L 334.740522 158.010658 \n",
       "L 337.814902 163.842769 \n",
       "L 340.889282 169.793121 \n",
       "L 343.963662 175.861716 \n",
       "L 347.038042 182.048552 \n",
       "L 350.112423 188.353631 \n",
       "L 353.186803 194.776951 \n",
       "L 356.261183 201.318513 \n",
       "L 359.335563 207.978317 \n",
       "L 362.409943 214.756364 \n",
       "\" clip-path=\"url(#p180f10cb44)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 42.828125 224.64 \n",
       "L 42.828125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 377.628125 224.64 \n",
       "L 377.628125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 42.828125 224.64 \n",
       "L 377.628125 224.64 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 42.828125 7.2 \n",
       "L 377.628125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p180f10cb44\">\n",
       "   <rect x=\"42.828125\" y=\"7.2\" width=\"334.8\" height=\"217.44\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pnovel = np.arange(0, 1, step=0.01)\n",
    "y = f(pnovel, arc=1, rho=0.1, w=4, k=6)\n",
    "plt.plot(pnovel, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
