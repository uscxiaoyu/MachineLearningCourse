{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = ['svg']\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import tensor, nn\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.Tensor是一种包含单一数据类型元素的多维矩阵。Torch定义了七种CPU tensor类型和八种GPU tensor类型：\n",
    "\n",
    "|Data type|\t CPU tensor | GPU tensor|\n",
    "|:---|:---|:---|\n",
    "|32-bit floating point|\ttorch.FloatTensor|\ttorch.cuda.FloatTensor|\n",
    "|64-bit floating point|\ttorch.DoubleTensor|\ttorch.cuda.DoubleTensor|\n",
    "|16-bit floating point|\tN/A|\ttorch.cuda.HalfTensor|\n",
    "|8-bit integer (unsigned)|\ttorch.ByteTensor|\ttorch.cuda.ByteTensor|\n",
    "|8-bit integer (signed)|\ttorch.CharTensor|\ttorch.cuda.CharTensor|\n",
    "|16-bit integer (signed)|\ttorch.ShortTensor|\ttorch.cuda.ShortTensor|\n",
    "|32-bit integer (signed)|\ttorch.IntTensor|\ttorch.cuda.IntTensor|\n",
    "|64-bit integer (signed)|\ttorch.LongTensor|\ttorch.cuda.LongTensor|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.tensor`和`torch.Tensor`的区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor([5, 4]).type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor([5, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([5, 4]).type()  # tensor接受已经存在的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([5, 4])  # tensor接受已经存在的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor(5, 4)  #  Tensor创建一个多维矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(torch.Tensor(5), torch.Tensor([5]), torch.tensor(5), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 不同类别的`Tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.IntTensor([[1, 2, 3], [4, 5, 6]])  # 一个张量tensor可以从Python的list或序列构建："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ShortTensor(2, 4).zero_()  # 一个空张量tensor可以通过规定其大小来构建："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ByteTensor(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ShortTensor(np.arange(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.random.randint(10, size=(2, 10))\n",
    "u = torch.from_numpy(u)  # 默认转为LongTensor\n",
    "u.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = u.type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.requires_grad = True  # 附加梯度，反向传播时计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.常用函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(1, 50, size=(10, 5))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape  # 各维度上的数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.reshape(5, 10)  # 重塑形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.reshape(5, -1)  # 如果某一维度为-1，则根据总元素个数自动计算该轴长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.numel()  # 元素个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.reshape(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(1, 10, 2)  # torch.range()也可用，建议用前者"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linspace(1, 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones(size=x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(10, 2) * 0.5 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.randn(10, 1, 1, 1)  # 10*1*1*1\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.squeeze(y)  # 10\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.arange(10).reshape(2, 5)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.t()  # 输入一个矩阵（2维张量），并转置0, 1维。 可以被视为函数transpose(input, 0, 1)的简写函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.reshape(z.shape[1], -1)  # 注意和转置之间的区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sign(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sigmoid(y)  # 返回一个新张量，包含输入input张量每个元素的sigmoid值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.norm(y, p=2)  # 返回输入张量input 的p 范数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.median(y)  # 返回输入张量给定维度每行的中位数，同时返回一个包含中位数的索引的LongTensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.prod(y)  # 返回输入张量input 所有元素的积。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sort(y, dim=0, descending=True)  # 对输入张量input沿着指定维按升序排序。如果不给定dim，则默认为输入的最后一维。如果指定参数descending为True，则按降序排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.relu(torch.randn(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 二元运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(10)\n",
    "b = torch.arange(10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a * b  # 按元素乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mul(a, b)  # 按元素乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.matmul(a, b)  # 矩阵乘法，前者 矩阵乘 后者的转置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mm(a.reshape(1, -1), b.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.reshape(1, -1), b.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.dot(a, b)  # 內积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a @ b  # 矩阵乘， 前者 矩阵乘 后者的转置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(4, 3)\n",
    "y = torch.rand(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mm(x, y)  # 矩阵乘法: 对矩阵mat1和mat2进行相乘。 如果mat1 是一个n×m张量，mat2 是一个 m×p张量，将会输出一个 n×p张量out。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.matmul(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x @ y  # 矩阵乘法运算符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.rand(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mv(x, z)  # 矩阵向量乘： 对矩阵mat和向量vec进行相乘。 如果mat 是一个n×m张量，vec 是一个m元 1维张量，将会输出一个n元 1维张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x @ z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 张量拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tensor(range(10), dtype=torch.float32).reshape(2, 5)\n",
    "b = tensor(range(10, 20), dtype=torch.float32).reshape(2, 5)\n",
    "c = tensor(range(20, 30), dtype=torch.float32).reshape(2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_ = torch.cat([a, b, c], 0)  # 在给定维度上对输入的张量序列进行连接操作，和extend类似\n",
    "cat_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.sum(a) > 10:\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([a, b, c], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_ = torch.stack([a, b, c], 0)  # 沿着一个新维度对输入张量序列进行连接。 序列中所有的张量都应该为相同形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_.split([2, 3], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_.chunk(2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 自动求导`autograd`\n",
    "\n",
    "torch.autograd提供了类和函数用来对任意标量函数进行求导。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例:\n",
    "$$\n",
    "f(\\mathbf{x})=2\\mathbf{x}+1, g(y)=\\mathbf{y^2}+5, z=mean(\\mathbf{g(y)})\n",
    "$$\n",
    "求$\\frac{dz}{dx}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x:tensor):\n",
    "    return 2*x + 1\n",
    "\n",
    "def g(x:tensor):\n",
    "    return x**2 + 5\n",
    "\n",
    "def mean(x:tensor):\n",
    "    return torch.mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dz_dx(x:tensor):  # 实际上的导数\n",
    "    return (8*x + 4) / x.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(1, 10, size=(2, 5), dtype=torch.float32, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad\n",
    "# x.requires_grad_(True)  # 如果为False, 可以追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch自动求导结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = mean(g(f(x)))\n",
    "z.backward()  # 反向传播，自动求微分\n",
    "x.grad  # dz/dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解析求导结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dz_dx(x)  # dz/dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([a, a[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 案例: Bass模型拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bass扩散模型针对创新产品、服务和扩散进行建模，常被用作市场分析工具，对新产品、新技术需求进行预测。\n",
    "\n",
    "新产品创新扩散是指新产品从创造研制到进入市场推广、最终使用的过程，表现为广大消费者从知晓、兴趣、评估、试用到最终采用新产品的行为。\n",
    "\n",
    "Bass扩散模型的许多变形也已被开发出来，用以满足某些特殊情形的精确需求。\n",
    "\n",
    "$$\n",
    "N(t)=m\\frac{1-e^{-(p+1)t}}{1+\\frac{q}{p}e^{-(p+q)t}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_momentum(lossfunc, w, x_dict, beta1=0.5, beta2=0.9, learn_rate=0.999, max_iter=1000, epsilon=1e-8):\n",
    "    trace_w = w.clone().data.reshape(1, -1)\n",
    "    v_0, s_0 = 0, 0\n",
    "    i = 1\n",
    "    while i <= max_iter:\n",
    "        l = lossfunc(w, **x_dict)\n",
    "        l.backward()\n",
    "        v_1 = (beta1 * v_0 + (1 - beta1) * w.grad.data) / (1 - beta1 ** i)\n",
    "        s_1 = (beta2 * s_0 + (1 - beta2) * w.grad.data ** 2) / (1 - beta2 ** i)\n",
    "        w.data.sub_(learn_rate * v_1 / (torch.sqrt(s_1) + epsilon))\n",
    "        with torch.no_grad():\n",
    "            trace_w = torch.cat([trace_w, w.detach().data.reshape(1, -1)], 0)\n",
    "            if i % 50 == 0:\n",
    "                loss = lossfunc(w, **x_dict).data.numpy()\n",
    "                print(f\"迭代次数: {i}, 损失函数值: {loss:.4f}\")\n",
    "\n",
    "            if torch.sum(torch.abs(trace_w[-1] - trace_w[-2])) < 1e-3:  # 停止条件\n",
    "                break\n",
    "\n",
    "        w.grad.zero_()\n",
    "        v_0, s_0 = v_1, s_1\n",
    "        i += 1\n",
    "\n",
    "    print(f\"共迭代{i - 1}次, 损失函数值: {lossfunc(w, **x_dict).data.numpy():.4f}, 最优参数值: {w.tolist()}\")\n",
    "    return trace_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bass(params, T:int): # 如果要使用其它模型，可以重新定义\n",
    "    p, q, m = params\n",
    "    t_tensor = torch.arange(1, T + 1, dtype=torch.float32)\n",
    "    a = 1 - torch.exp(- (p + q) * t_tensor)\n",
    "    b = 1 + q / p * torch.exp(- (p + q) * t_tensor)\n",
    "    diffu_cont = m * a / b\n",
    "\n",
    "    adopt_cont = torch.zeros_like(diffu_cont)\n",
    "    adopt_cont[0] = diffu_cont[0]\n",
    "    for t in range(1, T):\n",
    "        adopt_cont[t] = diffu_cont[t] - diffu_cont[t - 1]\n",
    "        \n",
    "    return adopt_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanSquaredLoss(params, y):  # 平均平方误差\n",
    "    T = y.numel()\n",
    "    hat_y = bass(params, T)\n",
    "    return torch.mean((hat_y - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_2(params, y):  # R2\n",
    "    T = y.numel()\n",
    "    hat_y = bass(params, T)\n",
    "    tse = torch.sum((y - hat_y)**2)\n",
    "    ssl = torch.sum((y - torch.mean(y))**2)\n",
    "    R_2 = (ssl - tse)/ssl\n",
    "    return R_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tensor([96, 195, 238, 380, 1045, 1230, 1267, 1828, 1586, 1673, 1800, 1580, 1500], dtype=torch.float32) / 1000+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.FloatTensor([0.001, 0.3, 20])\n",
    "params.requires_grad_(True)\n",
    "# res = grad_desc(meanSquaredLoss, params, y, learn_rate=1e-9)\n",
    "res = adaptive_momentum(meanSquaredLoss, params, x_dict={\"y\": y}, beta1=0.6, beta2=0.5, learn_rate=0.003, max_iter=1000)\n",
    "r2 = r_2(res[-1], y).numpy()\n",
    "print(\"r2:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = y.numel()\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Sales volumn\")\n",
    "plt.plot(np.arange(T), bass(res[-1], T).numpy() * 1000)\n",
    "plt.scatter(np.arange(T), y.numpy() * 1000, marker='o', color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 构建神经网络的流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. 定义计算架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, dim_feature, dim_hidden, dim_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = nn.Linear(dim_feature, dim_hidden)\n",
    "        self.output = nn.Linear(dim_hidden, dim_output)\n",
    "    \n",
    "    def forward(self, X):  # 层之间的计算次序\n",
    "        f1 = torch.relu(self.hidden(X))  # 0-1\n",
    "        f2 = self.output(f1)  # 1->2\n",
    "        f3 = nn.functional.softmax(f2, dim=1)  # 2->3\n",
    "        return f3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(dim_feature=2, dim_hidden=10, dim_output=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.05)  # 指定需优化的参数\n",
    "loss_func = nn.CrossEntropyLoss()  # 确定训练准则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. 生成测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = torch.randn(100, 2) + 2  # 均值为 2\n",
    "y0 = torch.zeros(100)\n",
    "x1 = torch.randn(100, 2) - 2  # 均值为 -2\n",
    "y1 = torch.ones(100)\n",
    "\n",
    "x = torch.cat((x0, x1)).type(torch.FloatTensor)\n",
    "y = torch.cat((y0, y1)).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(len(x))\n",
    "np.random.shuffle(idx)\n",
    "train_x, train_y = x[idx[:50]], y[idx[:50]]  # 随机选取50个\n",
    "test_x, test_y = x[idx[50:]], y[idx[50:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(400):\n",
    "    out = net.forward(train_x)\n",
    "    loss = loss_func(out, train_y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 40 == 0:\n",
    "        with torch.no_grad():\n",
    "            loss_train = loss_func(out, train_y)\n",
    "            out_test = net.forward(test_x)\n",
    "            loss_test = loss_func(out_test, test_y)\n",
    "            print(f\"loss_train: {loss_train}, loss_test: {loss_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = net(train_x)\n",
    "predict_train_y = torch.max(train_result, 1)[1]\n",
    "\n",
    "test_result = net(test_x)\n",
    "predict_test_y = torch.max(test_result, 1)[1]\n",
    "\n",
    "x_list = [train_x, test_x]\n",
    "y_list = [predict_train_y, predict_test_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "for i in range(2):\n",
    "    px = x_list[i]\n",
    "    py = y_list[i]\n",
    "    ax = fig.add_subplot(1, 2, i+1)\n",
    "    ax.set_xlabel('$x_0$')\n",
    "    ax.set_ylabel('$x_1$')\n",
    "    ax.scatter(px.data.numpy()[:,0], px.data.numpy()[:,1], c=py.data.numpy(), s=60, lw=0, cmap='RdYlGn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 案例： 垃圾邮件分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用`nn.Module`实现`Logit`回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogitNet(nn.Module):\n",
    "    def __init__(self, dim_feature, dim_output):\n",
    "        super(LogitNet, self).__init__()\n",
    "        self.output = nn.Linear(dim_feature, dim_output)\n",
    "    \n",
    "    def forward(self, X):  # 层之间的计算次序\n",
    "        h = self.output(X)  # 1 -> 2\n",
    "        o = nn.functional.softmax(h, dim=1)  # 2 -> 3\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dataset/smsspamcollection/SMSSpamCollection', delimiter='\\t', header=None, names=['category', 'message'])\n",
    "df['label'] = (df.category == 'ham').astype('int')\n",
    "print('垃圾邮件数量: %d ' % np.sum(df.label == 0))\n",
    "print('正常邮件数量: %d ' % np.sum(df.label == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.message.values\n",
    "y = df.label.values\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=200)\n",
    "# y转换为tensor\n",
    "y_train = torch.tensor(y_train).type(torch.LongTensor)  # 注意label的形式为1维，即类别的标签，无需reshape(-1, 1)\n",
    "y_test = torch.tensor(y_test).type(torch.LongTensor)\n",
    "# 获取词的tf-idf\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train_raw)\n",
    "X_test = vectorizer.transform(X_test_raw)\n",
    "# X转换为tensor\n",
    "X_train = torch.tensor(X_train.toarray(), dtype=torch.float)\n",
    "X_test = torch.tensor(X_test.toarray(), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 300  # 构建每批次100个样本的训练集\n",
    "dataset = TensorDataset(X_train, y_train)\n",
    "data_iter = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snet = LogitNet(dim_feature=X_train.shape[1], dim_output=2)\n",
    "optimizer = torch.optim.SGD(snet.parameters(), lr=0.03)  # 指定需优化的参数\n",
    "# loss_func = nn.CrossEntropyLoss()  # 确定训练准则\n",
    "loss_func = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    for X, y in data_iter:        \n",
    "        loss = loss_func(snet.forward(X), y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if i % 20 == 0:\n",
    "        with torch.no_grad():\n",
    "            loss_train = loss_func(snet.forward(X_train), y_train)\n",
    "            out_test = snet.forward(X_test)\n",
    "            loss_test = loss_func(out_test, y_test)\n",
    "            print(f\"loss_train: {loss_train:.5f}, loss_test: {loss_test:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = snet(X_train)\n",
    "predict_y_train = torch.max(train_result, 1)[1]\n",
    "print(torch.sum(predict_y_train != y_train), torch.sum(predict_y_train == y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.std(a, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
