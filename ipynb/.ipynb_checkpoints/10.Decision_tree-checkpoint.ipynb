{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 六. 决策树\n",
    "\n",
    "决策树是一种基本的分类和回归方法。决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程，可认为是if-then规则的集合，也可认为是定义在特征空间与类空间上的条件概率分布。\n",
    "\n",
    "决策树的有点在于可读性强，且分类速度快。学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。预测时，对新的数据，利用决策树模型进行分类。\n",
    "\n",
    "决策树学习通常包含3个步骤：**特征选择、决策树的生成和决策树的修剪**。\n",
    "\n",
    "决策树学习的思想主要源于`Quinlan(1986)`的`ID3`算法和`Quinlan(1993)`的`C4.5`算法，以及`Breiman等(1984)`提出的`CART`算法."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 决策树模型与学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 决策树模型\n",
    "\n",
    ">定义5.1 （决策树）分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点(node)和有向边(directed edge)组成。结点又进一步分为内部结点(internal node)和叶结点(leaf node)。内部结点表示一个特征或属性，叶结点表示一个类。\n",
    "\n",
    "用决策树分类，从根结点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子节点；这时，每一个子节点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直至达到叶结点。最后将实例分到叶结点的类中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 对决策树的认识\n",
    "    - 可将决策树看作**if-then规则的集合**：由决策树的根结点到叶结点的每一条路径构建一条规则；路径上内部结点的特征对应着规则的条件，而叶结点则对应着规则的结论。决策树的路径或其对应着if-then规则集合具有互斥和完备性。即每个实例都被一条路径覆盖，且只有一条路径覆盖。\n",
    "    - 也可将决策树视为在**给定特征条件下类的条件概率分布**。将特征空间划分为互补相交的单元，并在每个单元定义一个类的概率分布构成一个条件概率分布。决策树的一条路径对应于划分中的一个单元。决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。假设$X$为表示特征的随机变量，$Y$为表示类的随机变量，那么这个条件概率分布可表示为$P(Y|X)$。$X$取值给定划分下单元的集合，Y取值于类的集合。各叶结点上的条件概率往往偏向于某一大类，即属于某一类的概率较大。决策树分类时将该节点的实例强行分到条件概率大的那一类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 决策树学习\n",
    "\n",
    "假设给定训练数据集\n",
    "$$\n",
    "D = \\{(x_1,y_1),(x_2,y_2),...,(x_N,y_n)\\}\n",
    "$$\n",
    "其中, $x_i=(x_i^{(1)},x_i^{(2)},...,x_i^{(n)})^T$为输入实例（特征向量），$n$为特征个数，$y_i\\in \\{1,2,...,K\\}$为类标记，$i=1,2,...,N$，N为样本容量。决策树学习的目标是根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类。\n",
    "\n",
    "决策树学习的损失函数通常是正则化的极大似然函数，学习的策略是以损失函数为目标函数的最小化。从所有可能的决策树中选取最优决策问题是`NP hard`问题，所以现实中决策树学习算法通常采用启发式方法，近似求解这一优化问题。这样得到的决策树是次最优的(sub-optimal)。\n",
    "\n",
    "决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。这个过程对应着特征空间的划分，也对应着决策树的构建。\n",
    "\n",
    "由于生成的决策树可能存在过拟合，因此需要自下而上进行剪枝，将数变得简单，使之更具泛化能力。具体地，去掉过于细分的叶结点，使其回退到父结点，甚至更高的结点，然后将父结点或更高的结点改为新的叶节点。\n",
    "\n",
    "决策树的生成对应于模型的局部选择，剪枝对应于模型的全局选择。换而言之，决策树生成只考虑局部最优，剪枝则考虑全局最优。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 特征选择\n",
    "\n",
    "特征选择在于选取对训练数据具有分类能力的特征，决定用哪个特征来划分特征空间，通常特征选择的准则是信息增益或信息增益比。\n",
    "\n",
    "- 熵(entropy): 表示随机变量不确定性的度量\n",
    "\n",
    "设$X$是一个取有限个值的随机变量，其概率分布为\n",
    "$$\n",
    "P(X=x_i)=p_i,i=1,2,...,n\n",
    "$$\n",
    "熵定义为\n",
    "$$\n",
    "H(x)=-\\sum_{i=1}^n p_i \\log p_i\n",
    "$$\n",
    "上式中，若$p_i=0$，则定义$0\\log 0=0$；对数以2或者e为底，这时熵的单位分别称为比特(bit)或者纳特(nat)。熵只依赖于X的分布，与其取值无关，因此也可将X的熵记作$H(p)$, 即\n",
    "$$\n",
    "H(p)=-\\sum_{i=1}^n p_i \\log p_i\n",
    "$$\n",
    "熵越大，不确定越大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython import display\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(P):\n",
    "    '''\n",
    "    P为概率分布\n",
    "    '''\n",
    "    return np.dot(P, np.log2(P))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 条件熵(conditional entropy): 表示在已知随机变量X的条件下随机变量Y的不确定性。\n",
    "\n",
    "$$\n",
    "H(Y|X)=\\sum_{i=1}^n P(X=x_i)H(Y|X=x_i)\n",
    "$$\n",
    "\n",
    "其中，$H(Y|X=x_i)=-\\sum_j P(Y=y_j|X=x_i)\\log P(Y=y_j|X=x_i)$，表示在$X=x_i$时Y的不确定程度；$p(Y=y_j|X=x_i) = \\frac{p(X=x_i, Y=y_j)}{p(X=x_i)}$。\n",
    "> 如果X与Y无关，则有$H(Y|X)=H(Y)$；如果Y由X唯一决定，则有$H(Y|X)=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_entropy(P_XY):\n",
    "    '''\n",
    "    P_XY为X和Y的联合概率分布shape(x_size, y_z)\n",
    "    '''\n",
    "    return np.sum([np.sum(P_XY[i]) * entropy(P_XY[i, :]/np.sum(P_XY[i])) \n",
    "                   for i in P_XY.shape[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">定义5.2 （信息增益）特征A对训练数据集D的信息增益$g(D,A)$，定义为集合D的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵H(D|A)之差，即$g(D,A)=H(D)-H(D|A)$。\n",
    ">可以理解为A对于减少D的不确定的贡献。\n",
    "\n",
    "一般地，熵$H(Y)$与条件熵$H(Y|X)$之差称为互信息。决策树学习中的信息增益等价于训练数据集中类于特征的互信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树学习应用信息增益准则选择特征。给定训练集D合特征A，经验熵$H(D)$表示对数据集D进行分类的不确定性。而经验条件熵$H(D|A)$表示特征A在给定的条件下对数据集D进行分类的不确定性。**信息增益，就表示由于特征A而使得对数据集D的分类的不确定性减少的程度。** 信息增益大的特征具有更强的分类能力。\n",
    "\n",
    "根据信息增益准则的特征选择方法是：对训练集D，计算其每个特征的信息增益，选择信息增益最大的特征。\n",
    "\n",
    "假设训练集为$D, |D|$表示其样本个数。设有$K$个类$C_k, k=1,2,...,K$，$C_k$为属于类$C_k$的样本数量，$\\sum_{k=1}^K|C_k|=|D|$。\n",
    "设特征A有n个不同的取值$\\{a_1,a_2,...,a_n\\}$，根据特征A的取值将D划分为n个子集$D_1,D_2,...,D_n, |D_i|$为样本为$D_i$的样本个数，$\\sum_{i=1}^n|D_i|=|D|$。\n",
    "记子集$D_i$中属于类$C_k$的样本的集合为$D_{ik}$，即$D_{ik}=D_i \\cap C_k$, $D_{ik}$为$D_{ik}$的样本个数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**算法5.1 （经验信息增益的算法）**\n",
    "- 输入: 训练集D和特征A\n",
    "- 输出：特征A对训练集D的信息增益$g(D, A)$\n",
    "- 算法过程\n",
    "    - 计算数据集D的经验熵$H(D)$\n",
    "    $$\n",
    "    H(D)=-\\sum_{k=1}^K\\frac{|C_k|}{|D|}\\log_2\\frac{|C_k|}{|D|}\n",
    "    $$\n",
    "    - 计算A对D的经验条件熵$H(D|A)$\n",
    "    $$\n",
    "    H(D|A)=\\sum_{i=1}^n\\frac{|D_i|}{|D|}H(D_i)=-\\sum_{i=1}^n\\frac{|D_i|}{|D|}\\sum_{k=1}^K \\frac{|D_{ik}|}{|D_i|}\\log_2\\frac{|D_{ik}|}{|D_i|}\n",
    "    $$\n",
    "    - 计算信息增益\n",
    "    $$\n",
    "    g(D,A)=H(D)-H(D|A)\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_gain(X, y, i):\n",
    "    \"\"\"\n",
    "    X: 特征 pd.DataFrame 或者 np.ndarrary, 多列\n",
    "    y: 类别 pd.DataFrame 或者 np.ndarrary, 一列\n",
    "    i: 特征名称 或 X的列\n",
    "    \"\"\"\n",
    "    if type(X) == pd.core.frame.DataFrame and type(y) == pd.core.frame.DataFrame:\n",
    "        Xi = X[attr].values\n",
    "        y = y.values[:, 1]\n",
    "    elif type(X) == np.ndarray and type(y) == np.ndarray:\n",
    "        Xi = X[:, i]\n",
    "    else:\n",
    "        raise(TypeError, \"请将X和y转换成pandas.DataFrame或者numpy.ndarray!\")\n",
    "        \n",
    "    prob_y = {c: np.sum(y == c) / y.size for c in np.unique(y)}  # 计算y的概率分布\n",
    "    H_y = -np.sum([prob_y[p]*np.log2(prob_y[p]) for p in prob_y])  # 计算y的经验熵\n",
    "    Xi_y_dict = {xi: y[Xi==xi] for xi in np.unique(Xi)}  # 找出对应xi的y\n",
    "    probs = {xi: {c: np.sum(Xi_y_dict[xi]==c)/Xi_y_dict[xi].size for c in np.unique(Xi_y_dict[xi])} \n",
    "                 for xi in Xi_y_dict}  # 各xi下y的概率分布\n",
    "    H_yi = {xi: -np.sum([probs[xi][p]*np.log2(probs[xi][p]) for p in probs[xi]]) \n",
    "                for xi in probs}  # 计算xi对应的y的经验熵\n",
    "    H_y_x = np.sum([H_yi[xi]*(Xi_y_dict[xi].size / y.size) for xi in H_yi])  # 计算Xi对y的经验条件熵\n",
    "    return H_y - H_y_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 信息增益比\n",
    "\n",
    "以信息增益作为训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比(information gain ratio)可以对这一问题进行校正。这是特征选择的另一准则。\n",
    "\n",
    ">定义5.3 （信息增益比） 特征A对D的训练增益比$\\mathrm{g_R(D,A)}$定义为其信息增益$\\mathrm{g_R(D,A)}$与训练数据集D关于特征A的值的熵$H_A(D)$之比，即\n",
    ">$$\n",
    "g_R(D,A)=\\frac{g(D,A)}{H_A(D)}\n",
    "$$\n",
    ">其中，$H_A(D)=-\\sum_{i=1}^n\\frac{|D_i|}{|D|}\\log_2 \\frac{|D_i|}{|D|}$，n是特征A取值的个数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_grain_ratio(X, y, i):\n",
    "    \"\"\"\n",
    "    X: 特征 pd.DataFrame 或者 np.ndarrary, 多列\n",
    "    y: 类别 pd.DataFrame 或者 np.ndarrary, 一列\n",
    "    i: 特征名称 或 X的列\n",
    "    \"\"\"\n",
    "    if type(X) == pd.core.frame.DataFrame and type(y) == pd.core.frame.DataFrame:\n",
    "        Xi = X[attr].values\n",
    "        y = y.values[:, 1]\n",
    "    elif type(X) == np.ndarray and type(y) == np.ndarray:\n",
    "        Xi = X[:, i]\n",
    "    else:\n",
    "        raise(TypeError, \"请将X和y转换成pandas.DataFrame或者numpy.ndarray!\")\n",
    "    ig = info_gain(X, y, i)\n",
    "    prob_xi_y = {xi: len(y[Xi==xi])/len(y) for xi in np.unique(Xi)}  # 找出对应xi的y\n",
    "    H_y = -np.sum([prob_xi_y[xi] * np.log2(prob_xi_y[xi]) for xi in prob_xi_y])\n",
    "    return ig / H_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 模型学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 `ID3`算法\n",
    "\n",
    "ID3算法的核心是在决策树各个节点上应用信息增益准则选择特征，递归地构建决策树。\n",
    "\n",
    "具体方法是：从根结点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子节点；再对子结点递归调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止\n",
    "\n",
    "**算法 5.2 （ID3算法）**\n",
    "- 输入：训练集D，特征A，阈值$\\epsilon$\n",
    "- 输出：决策树T\n",
    "- 算法过程\n",
    "    - 若D中所有实例都属于同一类$C_k$, 则T为单结点树，并将类$C_k$作为该结点的类标记，返回T；\n",
    "    - 若$A=\\emptyset$，则$T$为单结点树，并将D中实例数最大的类$C_k$作为该结点的类标记，返回T；\n",
    "    - 否则，按照算法5.1计算A中各特征对D的信息增益，选择信息增益最大的特征$A_g$;\n",
    "    - 如果$A_g$的信息增益小于阈值$\\epsilon$,则T为单结点树，并将D中实例数最大的类$C_k$作为该结点的类标记，返回T；\n",
    "    - 否则，对$A_g$的每一可能值$a_i$，依$A_g=a_i$将D分割为若干非空子集$D_i$，将$D_i$中最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T；\n",
    "    - 对第i个子结点，以$D_i$为训练集，以$A-\\{A_g\\}$为特征集，递归地调用以上步骤，得到子树$T_i$，返回$T_i$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# 导入操作符\n",
    "from operator import le, ge, lt, gt, eq\n",
    "# 可以使用字典保存树结构及其属性\n",
    "nodes_dict = {0: {'X': X, 'y': y, 'name': 'age'}, \n",
    "             1: {'X': X_1, 'y': y_1, 'name': 'education'},\n",
    "             2: {'X': X_2, 'y': y_2, 'name': 'leaf_node', 'category': 'no'},\n",
    "             3: {'X': X_3, 'y': y_3, 'name': 'leaf_node', 'category': 'no'},\n",
    "             4: {'X': X_4, 'y': y_4, 'name': 'leaf_node', 'category': 'yes'}}  # 节点属性，划分至该节点的训练集\n",
    "\n",
    "edges_dict = {(0, 1): {'attr': 'age', 'oper': le, 'value': 35},\n",
    "             (0, 2): {'attr': 'age', 'oper': gt, 'value': 35},\n",
    "             (1, 3): {'attr': 'education', 'oper': eq, 'value': '本科'},\n",
    "             (1, 4): {'attr': 'education', 'oper': eq, 'value': '硕士'}}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree:\n",
    "    def __init__(self):\n",
    "        self.nodes_dict = {}\n",
    "        self.edges_dict = {}\n",
    "    \n",
    "    @property\n",
    "    def nodes(self):\n",
    "        '''\n",
    "        得到树的结点列表\n",
    "        '''\n",
    "        self._nodes = list(self.nodes_dict)\n",
    "        return self._nodes\n",
    "    \n",
    "    @property\n",
    "    def edges(self):\n",
    "        '''\n",
    "        得到树的边列表\n",
    "        '''\n",
    "        self._edges = list(self.edges_dict)\n",
    "        return self._edges\n",
    "    \n",
    "    def add_node(self, node_id):\n",
    "        '''\n",
    "        添加1个结点\n",
    "        '''\n",
    "        self.nodes_dict[node_id] = {}\n",
    "    \n",
    "    def add_edge(self, u, v, attr_dict=None):\n",
    "        '''\n",
    "        添加1条边，及其对应的划分属性，操作符，以及对应的值。\n",
    "        '''        \n",
    "        if (u, v) not in self.edges_dict:\n",
    "            if u not in self.nodes_dict:\n",
    "                self.add_node(u)\n",
    "            if v not in self.nodes_dict:\n",
    "                self.add_node(v)\n",
    "            if not attr_dict:\n",
    "                self.edges_dict[(u, v)] = attr_dict\n",
    "            else:\n",
    "                self.edges_dict[(u, v)] = {}\n",
    "        else:\n",
    "            print(f'{(u, v)} has already existed!')\n",
    "    \n",
    "    def remove_node(self, node_id):\n",
    "        '''\n",
    "        删除1个结点\n",
    "        '''\n",
    "        try:\n",
    "            self.nodes_dict.pop(node_id)\n",
    "            for edge in self.edges_dict:\n",
    "                if node_id in edge:\n",
    "                    self.edges_dict.pop(edge)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    def remove_edge(self, u, v):\n",
    "        '''\n",
    "        删除一条边\n",
    "        '''\n",
    "        try:\n",
    "            self.edges_dict.pop(edge)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    def get_successors(self, node_id):\n",
    "        '''\n",
    "        得到结点的后继结点列表\n",
    "        '''\n",
    "        return [e[1] for e in self.edges_dict if e[0] == node_id]\n",
    "    \n",
    "    def get_predecessors(self, node_id):\n",
    "        '''\n",
    "        得到结点的前继结点列表\n",
    "        '''\n",
    "        return [e[0] for e in self.edges_dict if e[1] == node_id]\n",
    "    \n",
    "    def get_subtree(self, node_id):\n",
    "        '''\n",
    "        得到某个结点对应的子树对应的结点列表\n",
    "        '''\n",
    "        node_list = []\n",
    "        new_nodes = [node_id]\n",
    "        while new_nodes:\n",
    "            node = new_nodes.pop(0)\n",
    "            successors = self.get_successors(node)\n",
    "            node_list.append(node)\n",
    "            new_nodes.extend(successors)\n",
    "        return node_list\n",
    "    \n",
    "    def has_node(self, node_id):\n",
    "        '''\n",
    "        检查是否有某个结点\n",
    "        '''\n",
    "        if node_id in self.nodes_dict:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def has_edge(self, u, v):\n",
    "        '''\n",
    "        检查是否有某条边\n",
    "        '''\n",
    "        if (u, v) in self.edges_dict:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = Tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.add_edge(1,2)\n",
    "tree.add_edge(2,3)\n",
    "tree.add_edge(2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (2, 3), (2, 4)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.get_subtree(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.nodes_dict[4]['name'] = 'leaf_node'\n",
    "tree.nodes_dict[4]['category'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'leaf_node', 'category': 1}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.nodes_dict[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import le, ge, lt, gt, eq\n",
    "\n",
    "# 用于生成唯一的node_id\n",
    "def id_generator(root=0, end=1e5, step=1):\n",
    "    num = root + 1\n",
    "    while num < end:\n",
    "        yield num\n",
    "        num += step\n",
    "        \n",
    "nodeId_gener = id_generator(root=0)  # 迭代器\n",
    "        \n",
    "def id3_decision_tree(node_id, nodes_dict, edges_dict):\n",
    "    '''\n",
    "    使用递归实现\n",
    "    node: 当前节点id\n",
    "    nodes_dict:\n",
    "    edges_dict:\n",
    "    '''\n",
    "    X = nodes_dict[node_id][\"X\"]\n",
    "    y = nodes_dict[node_id][\"y\"]\n",
    "    frequency_y = [(v, np.sum(y[y.columns[0]]==v)) for v in np.unique(y[y.columns[0]].values)]\n",
    "    max_y = max(frequency_y, key=lambda x: x[1])[0]  # 数量最多的标签\n",
    "    # case1: 只有1类，或者X中已没有特征，当前节点设为叶节点\n",
    "    if len(frequency_y) == 1 or X.size == 0:\n",
    "        nodes_dict[node_id][\"name\"] = \"leaf_node\"\n",
    "        nodes_dict[node_id][\"category\"] = max_y\n",
    "        return nodes_dict, edges_dict\n",
    "    else:\n",
    "        attr_set = set(X.columns.values)  # 节点对应的特征名称\n",
    "        info_gain_list = [(attr, info_gain(X, y, attr)) for attr in attr_set]  # 计算各属性的信息增益\n",
    "        target_attr, target_info_gain = max(info_gain_list, lambda x: x[1])  # 选择信息增益(比)最大的属性\n",
    "        # case 2: 如果信息增益小于阈值或者target_attr所有值相同，则不继续生成结点，当前节点重置为叶结点\n",
    "        if len(np.unique(X[target_attr].values)) == 1 or target_info_gain < self.epsilon:\n",
    "            nodes_dict[node_id][\"name\"] = 'leaf_node'\n",
    "            nodes_dict[node_id][\"category\"] = max_y\n",
    "            return nodes_dict, edges_dict\n",
    "        # case 3: 继续往下增加结点, 当前节点名称置为续分属性名\n",
    "        else:\n",
    "            nodes_dict[node_id][\"name\"] = target_attr  # 续分属性名\n",
    "            Xi = X[target_attr]\n",
    "            for xi in np.unique(Xi.values):  # 往下生成节点\n",
    "                next_node_id = next(nodeId_gener)\n",
    "                edges_dict[(node_id, next_node_id)] = {\"attr\": target_attr, 'operator': eq, 'value': xi}\n",
    "                nodes_dict[next_node_id][\"X\"] = X[Xi == xi].drop(target_attr)  # 获取目标值，且删除该属性\n",
    "                nodes_dict[next_node_id][\"y\"] = y.iloc[X[Xi == xi].index]  # 根据X的索引获取对应的y\n",
    "                nodes_dict, edges_dict = id3_decision_tree(next_node_id, nodes_dict, edges_dict) # 递归调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id3_decision_tree_2(root_node_id, nodes_dict, edges_dict):\n",
    "    '''\n",
    "    使用循环实现\n",
    "    node: 当前节点id\n",
    "    nodes_dict:\n",
    "    edges_dict:\n",
    "    '''\n",
    "    no_name_nodes = [root_node_id]\n",
    "    while no_name_nodes:\n",
    "        new_nodes = []\n",
    "        for node_id in no_name_nodes:\n",
    "            X = nodes_dict[node_id][\"X\"]\n",
    "            y = nodes_dict[node_id][\"y\"]\n",
    "            frequency_y = [(v, np.sum(y[y.columns[0]]==v)) for v in np.unique(y[y.columns[0]].values)]\n",
    "            max_y = max(frequency_y, key=lambda x: x[1])[0]  # 数量最多的标签\n",
    "            # case1: 只有1类，或者X中已没有特征，当前节点设为叶节点\n",
    "            if len(frequency_y) == 1 or X.size == 0:\n",
    "                nodes_dict[node_id][\"name\"] = \"leaf_node\"\n",
    "                nodes_dict[node_id][\"category\"] = max_y\n",
    "            else:\n",
    "                attr_set = set(X.columns.values)  # 节点对应的特征名称\n",
    "                info_gain_list = [(attr, info_gain(X, y, attr)) for attr in attr_set]  # 计算各属性的信息增益\n",
    "                target_attr, target_info_gain = max(info_gain_list, lambda x: x[1])  # 选择信息增益(比)最大的属性\n",
    "                # case 2: 如果信息增益小于阈值或者target_attr所有值相同，则不继续生成结点，当前节点重置为叶结点\n",
    "                if len(np.unique(X[target_attr].values)) == 1 or target_info_gain < self.epsilon:\n",
    "                    nodes_dict[node_id][\"name\"] = 'leaf_node'\n",
    "                    nodes_dict[node_id][\"category\"] = max_y\n",
    "                # case 3: 继续往下增加结点, 当前节点名称置为续分属性名\n",
    "                else:\n",
    "                    nodes_dict[node_id][\"name\"] = target_attr  # 续分属性名\n",
    "                    Xi = X[target_attr]\n",
    "                    for xi in np.unique(Xi.values):  # 往下生成节点\n",
    "                        next_node_id = next(nodeId_gener)\n",
    "                        edges_dict[(node_id, next_node_id)] = {\"attr\": target_attr, 'operator': eq, 'value': xi}\n",
    "                        nodes_dict[next_node_id][\"X\"] = X[Xi == xi].drop(target_attr)  # 获取目标值，且删除该属性\n",
    "                        nodes_dict[next_node_id][\"y\"] = y.iloc[X[Xi == xi].index]  # 根据X的索引获取对应的y\n",
    "                        new_nodes.append(next_node_id)  # 添加至待确定节点列表\n",
    "                        \n",
    "        no_name_nodes = new_nodes\n",
    "    return nodes_dict, edges_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 `C4.5`的生成算法\n",
    "\n",
    "c4.5算法与ID3算法相似，C4.5算法对ID3算法进行了改进，在其决策树的生成过程中，使用**信息增益比**来选择特征。\n",
    "\n",
    "**算法5.3 （C4.5的生成算法）**\n",
    "\n",
    "- 输入：训练集D，特征A，阈值$\\epsilon$\n",
    "- 输出：决策树T\n",
    "- 算法过程\n",
    "    - 若D中所有实例都属于同一类$C_k$, 则T为单结点树，并将类$C_k$作为该结点的类标记，返回T；\n",
    "    - 若$A=\\emptyset$，则$T$为单结点树，并将D中实例数最大的类$C_k$作为该结点的类标记，返回T；\n",
    "    - 否则，按照算法5.1计算A中各特征对D的信息增益比，选择信息增益比最大的特征$A_g$;\n",
    "    - 如果$A_g$的信息增益小于阈值$\\epsilon$,则T为单结点树，并将D中实例数最大的类$C_k$作为该结点的类标记，返回T；\n",
    "    - 否则，对$A_g$的每一可能值$a_i$，依$A_g=a_i$将D分割为若干非空子集$D_i$，将$D_i$中最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T；\n",
    "    - 对第i个子结点，以$D_i$为训练集，以$A-\\{A_g\\}$为特征集，递归地调用以上步骤，得到子树$T_i$，返回$T_i$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c45_decision_tree(root_node_id, nodes_dict, edges_dict):\n",
    "    '''\n",
    "    使用循环实现\n",
    "    node: 当前节点id\n",
    "    nodes_dict:\n",
    "    edges_dict:\n",
    "    '''\n",
    "    no_name_nodes = [root_node_id]\n",
    "    while no_name_nodes:\n",
    "        new_nodes = []\n",
    "        for node_id in no_name_nodes:\n",
    "            X = nodes_dict[node_id][\"X\"]\n",
    "            y = nodes_dict[node_id][\"y\"]\n",
    "            frequency_y = [(v, np.sum(y[y.columns[0]]==v)) for v in np.unique(y[y.columns[0]].values)]\n",
    "            max_y = max(frequency_y, key=lambda x: x[1])[0]  # 数量最多的标签\n",
    "            # case1: 只有1类，或者X中已没有特征，当前节点设为叶节点\n",
    "            if len(frequency_y) == 1 or X.size == 0:\n",
    "                nodes_dict[node_id][\"name\"] = \"leaf_node\"\n",
    "                nodes_dict[node_id][\"category\"] = max_y\n",
    "            else:\n",
    "                attr_set = set(X.columns.values)  # 节点对应的特征名称\n",
    "                info_gain_ratio_list = [(attr, info_gain_ratio(X, y, attr)) for attr in attr_set]  # 计算各属性的信息增益\n",
    "                target_attr, target_info_gain = max(info_gain_list, lambda x: x[1])  # 选择信息增益(比)最大的属性\n",
    "                # case 2: 如果信息增益小于阈值或者target_attr所有值相同，则不继续生成结点，当前节点重置为叶结点\n",
    "                if len(np.unique(X[target_attr].values)) == 1 or target_info_gain < self.epsilon:\n",
    "                    nodes_dict[node_id][\"name\"] = 'leaf_node'\n",
    "                    nodes_dict[node_id][\"category\"] = max_y\n",
    "                # case 3: 继续往下增加结点, 当前节点名称置为续分属性名\n",
    "                else:\n",
    "                    nodes_dict[node_id][\"name\"] = target_attr  # 续分属性名\n",
    "                    Xi = X[target_attr]\n",
    "                    for xi in np.unique(Xi.values):  # 往下生成节点\n",
    "                        next_node_id = next(nodeId_gener)\n",
    "                        edges_dict[(node_id, next_node_id)] = {\"attr\": target_attr, 'operator': eq, 'value': xi}\n",
    "                        nodes_dict[next_node_id][\"X\"] = X[Xi == xi].drop(target_attr)  # 获取目标值，且删除该属性\n",
    "                        nodes_dict[next_node_id][\"y\"] = y.iloc[X[Xi == xi].index]  # 根据X的索引获取对应的y\n",
    "                        new_nodes.append(next_node_id)  # 添加至待确定节点列表\n",
    "                        \n",
    "        no_name_nodes = new_nodes\n",
    "    return nodes_dict, edges_dict\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 决策树的剪枝\n",
    "\n",
    "决策树生成算法递归地生成决策树，直到不能继续下去为止，这样产生的数往往会出现过拟合现象。原因在于学习时过多地考虑如何提高训练数据的分类准确性，从而构建了过于复杂的决策树。解决的办法是简化已生成的决策树，即执行**剪枝(pruning)**操作。具体地，从已生成的树上裁减一些子树或叶结点，并将其根结点或父结点作为新的叶结点。\n",
    "\n",
    "决策树的剪枝往往通过极小化决策树整体的**损失函数(loss function)**或**代价函数(cost function)**来实现。设树T的叶节点个数为$|T|$，t是树T的叶结点，该叶结有$N_t$个样本点，其中k类的样本点有$N_{tk}$个，$k=1,2,...,K$个，$H_t(T)$为叶结点t上的经验熵，$\\alpha\\geq 0$为参数，则决策树学习的损失函数可以定义为\n",
    "$$\n",
    "C_{\\alpha}(T)=\\sum_{t=1}^{|T|}N_tH_t(T)+\\alpha |T|\n",
    "$$\n",
    "其中经验熵为\n",
    "$$\n",
    "H_t(T)=-\\sum_k\\frac{N_{tk}}{N_t}\\log\\frac{N_{tk}}{N_t}\n",
    "$$\n",
    "\n",
    "在损失函数中，将右端的第1项记作\n",
    "$$\n",
    "C(T)=\\sum_{t=1}^{|T|}N_tH_t(T)=-\\sum_{t=1}^{|T|}\\sum_{k=1}^K N_{tk}\\log\\frac{N_{tk}}{N_t}\n",
    "$$\n",
    "\n",
    "这时有\n",
    "$$\n",
    "C_{\\alpha}(T)=C(T)+\\alpha |T|\n",
    "$$\n",
    "\n",
    "其中$C(T)$表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，$|T|$表示模型复杂度，参数$\\alpha\\geq 0$控制两者之间的影响。$\\alpha=0$意味着只考虑模型与训练数据的拟合程度，不考虑模型的复杂度; $\\alpha$较大促使选择简单的模型。\n",
    "\n",
    "以上定义的损失函数的极小化等价于正则化的极大似然估计。所以，利用损失函数最小化原则进行剪枝就是用正则化的极大似然估计进行模型选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "剪枝，就是当$\\alpha$确定时，选择损失函数最小的模型，即损失函数最小的子树。决策树生成学习局部的模型，而决策树剪枝则学习整体的模型。\n",
    "\n",
    "**算法5.4 （树的剪枝算法）**\n",
    "\n",
    "- 输入：生成算法产生的整个树T，参数$\\alpha$\n",
    "- 输出：修剪后的子树$T_{\\alpha}$\n",
    "- 算法过程\n",
    "    - 计算每个节点的经验熵。\n",
    "    - 递归地从树的叶节点向上收缩。设一组**兄弟叶结点(即有同一父结点)**回缩到其父结点之前与之后的整体树分别为$T_B$与$T_A$，其对应的损失函数值分别为$C_{\\alpha}(T_B)$与$C_{\\alpha}(T_A)$，如果$C_{\\alpha}(T_B)\\leq C_{\\alpha}(T_A)$，则进行剪枝，即将父结点变为新的叶结点。\n",
    "    - 返回(2)，直至不能继续为止，得到损失函数最小的子树$T_{\\alpha}$。\n",
    "    \n",
    "由于$C_{\\alpha}(T_B)\\leq C_{\\alpha}(T_A)$只需考虑两个树的损失函数的差，其计算可以在局部进行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假定针对节点p及其子叶结点集合X，由于其它叶节点的损失在剪枝前后保持不变，因此可以通过局部计算决策树的损失变化\n",
    "$$\n",
    "\\begin{aligned}\n",
    "C_\\alpha(T_A) - C_\\alpha(T_B) &= (\\sum_{x=1}^{|X|}N_x H_x(t) + \\alpha |X|)  - (N_p H_p(T) + \\alpha)\\\\\n",
    "&=\\sum_k\\left(N_{pk}\\log\\frac{N_{pk}}{N_p}\\right) - \\sum_{x=1}^{|X|}\\sum_k \\left(N_{xk}\\log\\frac{N_{xk}}{N_x} + \\alpha (|X| - 1)\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">除以上基于正则化的剪枝方法外，也有采用留出法评估决策树的泛化能力，即从预留一部分样本数据用作“验证集”以进行泛化性能评估(`周志华, 2017`)。该策略下的剪枝才做也分为**“预剪枝”(prepruning)**和**“后剪枝”(postpruning)**(`Quinlan, 1993`)。\n",
    ">- 预剪枝是指在决策树生成过程中，对每个结点在划分前进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶节点。\n",
    ">- 后剪枝是指先从训练集生成一颗完整的决策树，然后自底向上地对非叶节点进行考察，若将该结点对应的子树替代为叶节点能带来决策树泛化性能提升，则将该子树替换为叶节点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    '''\n",
    "    算法实现思想：树的生成过程为结点的新增及其标记(name)过程，因此需判断何时新增结点？新增的节点是否满足叶结点的条件？\n",
    "        新增的节点应选择哪个属性作为其标记？\n",
    "    默认train_data的最后一列为标签，其它列为特征\n",
    "    '''\n",
    "    def __init__(self, train_data, epsilon, alpha, mode='id.3'):\n",
    "        if type(train_data) != pd.core.frame.DataFrame:\n",
    "            raise(TypeError, '请使用pandas.DataFrame组织数据!')\n",
    "\n",
    "        self.feature = train_data[train_data.columns[:-1]]\n",
    "        self.label = train_data[train_data.columns[-1]]\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.tree = nx.DiGraph()\n",
    "        self.node_id = 0\n",
    "        self.no_name_nodes = [0]  # 尚未分类的节点，动态变化\n",
    "        self.mode = mode\n",
    "        self.tree.add_node(0)\n",
    "        self.tree.nodes[0][\"X\"] = self.feature\n",
    "        self.tree.nodes[0][\"y\"] = self.label\n",
    "        \n",
    "    def info_gain(self, X, y, col_name):\n",
    "        '''\n",
    "        计算信息增益\n",
    "        '''\n",
    "        Xi = X[col_name].values\n",
    "        y = y[y.columns[0]].values\n",
    "        prob_y = {c: np.sum(y==c)/y.size for c in np.unique(y)}  # 计算y的概率分布\n",
    "        H_y = -np.sum([prob_y[p]*np.log2(prob_y[p]) for p in prob_y])  # 计算y的经验熵\n",
    "        Xi_y_dict = {xi: y[Xi==xi] for xi in np.unique(Xi)}  # 找出对应xi的y\n",
    "        probs = {xi: {c: np.sum(Xi_y_dict[xi]==c)/Xi_y_dict[xi].size for c in np.unique(Xi_y_dict[xi])} \n",
    "                     for xi in Xi_y_dict}  # 各xi下y的概率分布\n",
    "        H_yi = {xi: -np.sum([probs[xi][p]*np.log2(probs[xi][p]) for p in probs[xi]]) \n",
    "                    for xi in probs}  # 计算xi对应的y的经验熵\n",
    "        H_y_x = np.sum([H_yi[xi]*(Xi_y_dict[xi].size / y.size) for xi in H_yi])  # 计算Xi对y的经验条件熵\n",
    "        return H_y - H_y_x\n",
    "    \n",
    "    def info_grain_ratio(self, X, y, col_name):\n",
    "        '''\n",
    "        计算信息增益比\n",
    "        '''\n",
    "        Xi = X[col_name].values\n",
    "        y = y[y.columns[0]].values\n",
    "        ig = info_gain(X, y, col_name)\n",
    "        prob_xi_y = {xi: len(y[Xi==xi])/len(y) for xi in np.unique(Xi)}  # 找出对应xi的y\n",
    "        H_y = -np.sum([prob_xi_y[xi] * np.log2(prob_xi_y[xi]) for xi in prob_xi_y])\n",
    "        return ig / H_y\n",
    "        \n",
    "    def generate_successors(self, node):\n",
    "        '''\n",
    "        从属性列表中选出目标属性，并和当前备用节点node建立联系\n",
    "        node的属性: \n",
    "            name -- 续分属性attribute或者leaf_node\n",
    "            category -- 对应样本的类别\n",
    "            X -- 特征向量\n",
    "            y -- 类别\n",
    "        '''\n",
    "        X = self.tree.nodes[node][\"X\"]\n",
    "        y = self.tree.nodes[node][\"y\"]\n",
    "        new_add_nodes = []\n",
    "        frequency_y = [(v, np.sum(y[y.columns[0]]==v)) for v in np.unique(y[y.columns[0]].values)]\n",
    "        max_y = max(frequency_y, key=lambda x: x[1])[0]\n",
    "        # case 1: 如果无续分属性或样本同属一类, 当前节点置为叶结点\n",
    "        if X.size == 0 or len(frequency_y) == 1:\n",
    "            self.tree.nodes[node][\"name\"] = 'leaf_node'\n",
    "            self.tree.nodes[node][\"category\"] = max_y\n",
    "        else:  # 存在续分属性\n",
    "            attr_set = set(X.columns.values)  # 节点对应的属性\n",
    "            if self.mode == \"id.3\":\n",
    "                info_gain_list = [(attr, self.info_gain(X, y, attr)) \n",
    "                                  for attr in attr_set]  # 计算各属性的信息增益\n",
    "            elif self.mode = \"c4.5\":\n",
    "                info_gain_list = [(attr, self.info_gain_ratio(X, y, attr)) \n",
    "                                  for attr in attr_set]  # 计算各属性的信息增益比\n",
    "            else:\n",
    "                raise(ValueError, \"请选择正确的分类准则('id.3'或'c4.5')\")\n",
    "\n",
    "            target_attr, target_info_gain = max(info_gain_list, lambda x: x[1])  # 选择信息增益(比)最大的属性\n",
    "            # case 2: 如果信息增益小于阈值或者target_attr所有值相同，则不继续生成结点，当前节点重置为叶结点\n",
    "            if len(np.unique(X[target_attr].values)) == 1 or target_info_gain < self.epsilon:\n",
    "                self.tree.nodes[node][\"name\"] = 'leaf_node'\n",
    "                self.tree.nodes[node][\"category\"] = max_y\n",
    "            else:  # case 3: 继续往下增加结点, 当前节点名称置为续分属性名\n",
    "                self.tree.nodes[node][\"name\"] = target_attr  # 续分属性名\n",
    "                Xi = X[target_attr]\n",
    "                for xi in np.unique(Xi.values):\n",
    "                    self.node_id += 1\n",
    "                    self.tree.add_edge(node, self.node_id, value=xi)  # 添加连边\n",
    "                    self.tree.nodes[self.node_id][\"X\"] = X[Xi == xi].drop(target_attr)  # 获取目标值，且删除该属性\n",
    "                    self.tree.nodes[self.node_id][\"y\"] = y.iloc[X[Xi == xi].index]  # 根据X的索引获取对应的y\n",
    "                    new_add_nodes.append(self.node_id)\n",
    "\n",
    "        return new_add_nodes\n",
    "            \n",
    "    def generate_decision_tree(self):\n",
    "        '''\n",
    "        生成决策树\n",
    "        '''\n",
    "        while self.no_name_nodes:\n",
    "            new_nodes = []\n",
    "            for node in self.no_name_nodes:\n",
    "                new_add_nodes = self.generate_successors(node)\n",
    "                new_nodes.extend(new_add_nodes)\n",
    "\n",
    "            self.no_name_nodes = new_nodes\n",
    "                \n",
    "    def loss(self, tree):\n",
    "        '''\n",
    "        损失函数\n",
    "        '''\n",
    "        CT_list = []\n",
    "        for i in tree:\n",
    "            if tree.nodes[i]['name']=='leaf_node':  # 叶结点\n",
    "                y = tree.nodes[i]['y'].values[:, 0]\n",
    "                frequency = np.array([np.sum(y == c) for c in np.unique(y)])\n",
    "                CT_list.append(np.dot(frequency, np.log2(frequency/y.size)))\n",
    "                \n",
    "        return -np.sum(CT_list) + self.alpha*len(CT_list)\n",
    "    \n",
    "    def select_nodes(self, node, tree):\n",
    "        \"\"\"\n",
    "        判断目标叶结点及其兄弟结点是否都为叶结点, 如果是，则返回(父结点, 子节点们)\n",
    "        \"\"\"\n",
    "        prede_node = list(tree.predecessors(node))[0]\n",
    "        succe_nodes = list(tree.successors(node))\n",
    "        labels = np.array([tree.nodes[i]['name'] for i in succe_nodes])\n",
    "        if np.sum(labels != 'leaf_node') == 0:  # 兄弟结点需为叶结点\n",
    "            return [prede_node, succe_nodes]\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def evaluate_pruning(self, parent_node, leaf_nodes):\n",
    "        \"\"\"\n",
    "        判断是否进行一次剪枝操作\n",
    "        C(T_A) - C(T_B) <= 0, 则剪枝\n",
    "        \"\"\"\n",
    "        parent_y = self.tree.nodes[parent_node][\"y\"].values[:, 0]\n",
    "        p_freq = np.array([np.sum(parent_y == c) for c in np.unique(parent_y)])\n",
    "        p_entropy = np.dot(p_freq, np.log2(p_freq/parent_y.size))  # 父节点的熵\n",
    "        l_entropy = 0  # 叶节点的熵\n",
    "        for i in leaf_nodes:\n",
    "            i_y = self.tree.nodes[i][\"y\"].values[:, 0]\n",
    "            i_freq = np.array([np.sum(i_y == c) for c in np.unique(i_y)])\n",
    "            l_entropy += np.dot(i_freq, np.log2(i_freq/i_y.size))\n",
    "        \n",
    "        return p_entropy - l_entropy + self.alpha * (len(target) - 1)\n",
    "        \n",
    "    def prune_tree(self):\n",
    "        \"\"\"\n",
    "        利用正则化剪枝: 取所有兄弟节点都为叶节点的子树考虑\n",
    "        \"\"\"\n",
    "        tree = deepcopy(self.tree)\n",
    "        c_subtrees = []  # 待评估子树\n",
    "        for node in tree:\n",
    "            if node not in candidate_nodes and tree.nodes[node]['name'] == 'leaf_node':\n",
    "                parent_node, succe_nodes = self.select_nodes(node, tree)\n",
    "                c_subtrees.apppend([parent_node, succe_nodes])\n",
    "        \n",
    "        while c_subtrees:  # 无待评估子树时停止迭代\n",
    "            parent_node, leaf_nodes = c_subtrees.pop(0)  # 取出并删除第1个元素\n",
    "            if self.evaluate_pruning(parent_node, leaf_nodes) <= 0:  # 判断是否剪枝\n",
    "                tree.remove_nodes_from(leaf_nodes)  # 删除叶结点\n",
    "                tree.nodes[parent_node][\"name\"] = 'leaf_node' # 重置其父结点为叶结点      \n",
    "                parent_y = self.tree.nodes[parent_node][\"y\"].values[:, 0]\n",
    "                max_y = max([(c, np.sum(parent_y==c)) for c in np.unique(parent_y)], lambda x: x[1])[0]\n",
    "                tree.nodes[parent_node][\"category\"] = max_y  # 新叶结点对应的类别\n",
    "                c_subtrees.append(self.select_nodes(parent_node))  # 评估回缩的节点是否满足候选子树条件\n",
    "        \n",
    "        return tree\n",
    "            \n",
    "    def predict(self, x_dict, tree=self.tree):\n",
    "        \"\"\"\n",
    "        x_dict: 字典\n",
    "        \"\"\"\n",
    "        node = 0\n",
    "        while True:\n",
    "            xi = tree.nodes[node][\"name\"]\n",
    "            for i in tree.successors(node):\n",
    "                if tree.edges[node][i]['value'] == xi:\n",
    "                    break\n",
    "            else:  # 如果没有相等的值\n",
    "                print(f'目标属性值不存在{tree.nodes[node][\"name\"]} = {xi}')\n",
    "                break\n",
    "                \n",
    "            if tree.nodes[i]['name'] == 'leaf_node':\n",
    "                return tree.nodes[i]['category']\n",
    "            else:\n",
    "                node = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. `CART`算法\n",
    "\n",
    "分类与回归数(classification and regression tree, CART)模型由`Breiman`等(1984)提出，是应用广泛的决策树学习方法。`CART`同样由特征选择、树的生成及剪枝组成，既可用于分类也可以用于回归。\n",
    "\n",
    "CART是在给定输入随机变量X条件下输出随机变量Y的条件概率分布的学习方法。CART假定决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，有分支是取值为“否”的分支。这样的决策树等价于递归地二分每个特征，将输入控件即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是输入给定的条件下输出的条件概率分布。\n",
    "\n",
    "CART算法由以下两步组成：\n",
    "- 决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大；\n",
    "- 决策树剪枝：用验证集对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 `CART`生成\n",
    "\n",
    "对回归树用平方误差最小化准则，对分类树用基尼指数最小化准则，进行特征选择，生成二叉树。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1). 回归树的生成\n",
    "假设$X$与$Y$分别为输入和输出变量，并且$Y$是连续变量，给定训练数据集\n",
    "$$\n",
    "D=\\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\\}\n",
    "$$\n",
    "考虑如何生成回归树。\n",
    "\n",
    "一颗回归树对应着输入控件的一个划分以及在划分的单元上的输出值。假设已将输入控件划分为$M$个单元$R_1,R_2,...R_M$，并且在每个单元$R_m$上有一个固定的输出值$c_m$，于是回归树模型可表示为\n",
    "$$\n",
    "f(x)=\\sum_{m=1}^M c_m I(x\\in R_m)\n",
    "$$\n",
    "\n",
    "当输入空间的划分确定时，可以用平方误差$\\sum_{x_i\\in R_m}(y_i-f(x_i))^2$来表示回归树对于训练数据的预测误差，用平方误差最小的准则求解每个单元上的最优输出值。显然，$c_m$的最优值为\n",
    "$$\n",
    "\\hat{c}_m=\\mathrm{average}(y_i|x_i\\in R_m)\n",
    "$$\n",
    "\n",
    "问题时怎样对输入空间进行划分。可以使用启发式的方法，选择第j个变量$x^{(j)}$和它的取值s，作为切分变量和切分点，并定义两个区域:\n",
    "$$\n",
    "R_1(j,s)=\\{x|x^{(j)}\\leq s\\} \\text{ 和 } R_2(j,s)=\\{x|x^{(j)}> s\\}\n",
    "$$\n",
    "然后寻找最优切分变量j和最优切分点s。具体地，求解\n",
    "$$\n",
    "\\min_{j,s}\\left[\\min_{c_1}\\sum_{x_i\\in R_1(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum_{x_i\\in R_2(j,s)}(y_i-c_2)^2\\right]\n",
    "$$\n",
    "对于固定输入变量j可以找到最优切分点s。\n",
    "$$\n",
    "\\hat{c}_1=\\mathrm{average}(y_i|x_i\\in R_1(j,s)) \\text{ 和 }  \\hat{c}_2=\\mathrm{average}(y_i|x_i\\in R_2(j,s))\n",
    "$$\n",
    "遍历所有输入变量，找到最优的切分变量j构成一对$(j,s)$，将输入空间划分为两个区域，然后对每个区域重复以上划分过程，知道满足停止条件为止。\n",
    "\n",
    "这样，可以生成一颗回归树。这样的回归树通常称为**最小二乘回归树(`least square regression tree`)**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**算法5.5 （最小二乘回归树生成算法）**\n",
    "- 输入：训练集$D$\n",
    "- 输出：回归树$f(x)$\n",
    "- 算法过程:\n",
    "    - 选择最有切分变量$j$与切分点$s$，求解\n",
    "    $$\n",
    "    \\min_{j,s}\\left[\\min_{c_1}\\sum_{x_i\\in R_1(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum_{x_i\\in R_2(j,s)}(y_i-c_2)^2\\right]\n",
    "    $$\n",
    "    遍历变量$j$，对固定的切分变量$j$扫描切分点$s$，选择使上式达到最小值的$(j,s)$\n",
    "    - 用选定的对$(j,s)$划分区域并决定相应的输出值:\n",
    "    $$\n",
    "    R_1(j,s)=\\{x|x^{(j)}\\leq s\\},R_2(j,s)=\\{x|x^{(j)}> s\\}\\\\\n",
    "    \\hat{c}_m=\\frac{1}{N_m}\\sum_{x_i\\in R_m(j,s)}y_i,x\\in R_m,m=1,2\n",
    "    $$\n",
    "    - 继续对两个子区域调用以上两个步骤，直到满足停止条件\n",
    "    - 将输入空间划分为$M$个区域$R_1,R_2,...,R_M$，生成决策树\n",
    "    $$\n",
    "    f(x)=\\sum_{m=1}^M\\hat{c}_m I(x\\in R_m)\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CART:\n",
    "    '''\n",
    "    算法实现思想：树的生成过程为结点的新增及其标记(name)过程，因此需判断何时新增结点？新增的节点是否满足叶结点的条件？\n",
    "        新增的节点应选择哪个属性作为其标记？\n",
    "    默认train_data的最后一列为标签，其它列为特征\n",
    "    '''\n",
    "    def __init__(self, train_data, validation_data, epsilon):\n",
    "        if type(train_data) != pd.core.frame.DataFrame:\n",
    "            raise(TypeError, '请使用pandas.DataFrame组织数据!')\n",
    "\n",
    "        self.feature = train_data[train_data.columns[:-1]]\n",
    "        self.label = train_data[train_data.columns[-1]]\n",
    "        self.v_feature = validation_data[validation_data.columns[:-1]]\n",
    "        self.v_label = validation_data[validation_data.columns[-1]]\n",
    "        self.epsilon = epsilon  # 终止条件\n",
    "        self.tree = nx.DiGraph()  # 初始化二叉树\n",
    "        self.node_id = 0\n",
    "        self.no_name_nodes = [0]  # 尚未分类的节点，动态变化\n",
    "        self.tree.add_node(0)  # 根结点\n",
    "        self.tree.nodes[0][\"X\"] = self.feature\n",
    "        self.tree.nodes[0][\"y\"] = self.label\n",
    "    \n",
    "    def gini_D_A(self, X, y, col_name, s):\n",
    "        '''\n",
    "        计算gini指数\n",
    "        '''\n",
    "        Xi = X[col_name].values\n",
    "        y = y[y.columns[0]].values\n",
    "        y1 = y[Xi == s]  # 等于s的部分\n",
    "        y2 = y[Xi != s]\n",
    "        prob_1 = np.array([np.sum(y1==c)/y1.size for c in np.unique(y1)])\n",
    "        prob_2 = np.array([np.sum(y2==c)/y2.size for c in np.unique(y2)])\n",
    "        \n",
    "        return (y1.size/y.size)*(1 - np.sum(np.square(prob_1))) + \n",
    "               (y2.size/y.size)*(1 - np.sum(np.square(prob_2)))\n",
    "    \n",
    "    def find_optimal_variable_value(self, X, y):\n",
    "        '''\n",
    "        选择最优切分变量和切分点\n",
    "        '''\n",
    "        results = []\n",
    "        for col in X.columns:\n",
    "            c_x = X[col].values\n",
    "            c_y = y[y.columns[0]].values\n",
    "            result = []\n",
    "            for v in np.sort(np.unique(xi)):\n",
    "                idx1 = c_x <= v  # 比v值小的点的索引\n",
    "                idx2 = c_x > v  # 比v值大的点的索引\n",
    "                c_x_1 = c_x[idx1]\n",
    "                c_y_1 = c_y[idx1]\n",
    "                c_x_2 = c_x[idx2]\n",
    "                c_y_2 = c_y[idx2]\n",
    "                avg_y_1 = np.mean(c_y_1)\n",
    "                avg_y_2 = np.mean(c_y_2)\n",
    "                r1 = np.sqrt(np.square(c_y_1 - avg_y_1))\n",
    "                r2 = np.sqrt(np.square(c_y_2 - avg_y_2))\n",
    "                result.append([v, c_y_1, c_y_2, r1+r2])  # 切分点, 左侧y的取值，右侧y的取值，\n",
    "            \n",
    "            opt = min(result, lambda x: x[-1])  # 获取根据col划分的最低损失\n",
    "            results.append([col, *opt])  # [变量名, x切分点, 左侧y的取值，右侧y的取值，损失]\n",
    "        \n",
    "        opt_res = min(results, lambda x: x[-1])\n",
    "        \n",
    "        return min(results, lambda)\n",
    "        \n",
    "    def generate_successors(self, node):\n",
    "        '''\n",
    "        从属性列表中选出目标属性，并和当前备用节点node建立联系\n",
    "        node的属性: \n",
    "            name -- 续分属性attribute或者leaf_node\n",
    "            category -- 对应样本的类别\n",
    "            X -- 特征向量\n",
    "            y -- 类别\n",
    "        '''\n",
    "        X = self.tree.nodes[node][\"X\"]\n",
    "        y = self.tree.nodes[node][\"y\"]\n",
    "        new_add_nodes = []\n",
    "        if len(np.unique(X[col].values)) == 1:  # 如果只有1个X的值，则为叶节点\n",
    "            self.tree.nodes[node][\"name\"] = 'leaf_node'\n",
    "            self.tree.nodes[node][\"value\"] = np.mean(y[y.columns[0]].values)\n",
    "        else:\n",
    "            col, value, y1, y2, loss = self.find_optimal_variable_value(X, y)\n",
    "            self.tree.nodes[node][\"name\"] = col\n",
    "            idx1 = X[col] <= value\n",
    "            idx2 = X[col] > value\n",
    "            X1 = X[idx1]\n",
    "            y1 = y[idx1]\n",
    "            X2 = X[idx2]\n",
    "            y2 = y[idx2]\n",
    "            node_1 = self.node_id + 1\n",
    "            node_2 = self.node_id + 2\n",
    "            self.tree.add_edge(node, node_1, value=('leq', value))\n",
    "            self.tree.add_edge(node, node_2, value=('gt', value))\n",
    "            self.node += 2\n",
    "            self.tree.nodes[node_1][\"X\"] = X1\n",
    "            self.tree.nodes[node_2][\"X\"] = X2\n",
    "            self.tree.nodes[node_1][\"y\"] = y1\n",
    "            self.tree.nodes[node_2][\"y\"] = y2\n",
    "            new_add_nodes.extend([node_1, node_2])\n",
    "\n",
    "        return new_add_nodes\n",
    "            \n",
    "    def generate_decision_tree(self):\n",
    "        '''\n",
    "        生成决策树\n",
    "        '''\n",
    "        while self.no_name_nodes:\n",
    "            new_nodes = []\n",
    "            for node in self.no_name_nodes:\n",
    "                new_add_nodes = self.generate_successors(node)\n",
    "                new_nodes.extend(new_add_nodes)\n",
    "\n",
    "            self.no_name_nodes = new_nodes\n",
    "    \n",
    "    def get_responses(self, tree):\n",
    "        '''\n",
    "        获取叶节点的y和对应的预测值hat_y\n",
    "        '''\n",
    "        hat_y_list = []\n",
    "        y_list = []\n",
    "        for i in tree:\n",
    "            if tree.nodes[i]['name']=='leaf_node':  # 叶结点\n",
    "                y = tree.nodes[i]['y'].values[:, 0].tolist()\n",
    "                hat_y = [tree.nodes[i]['value']] * len(y)  # 单值\n",
    "                y_list.extend(y)\n",
    "                hat_y_list.extend(hat_y)\n",
    "                \n",
    "        return np.array(hat_y_list), np.array(y_list)\n",
    "    \n",
    "    def r2(self, hat_y_array, y_array):\n",
    "        '''\n",
    "        计算r2\n",
    "        '''\n",
    "        rse = np.sum((hat_y_array - y_array)**2)  # 回归平方和\n",
    "        sse = np.sum((y_array -np.mean(y_array))**2) # 总平方和\n",
    "        return rse / sse\n",
    "    \n",
    "    def select_nodes(self, node, tree):\n",
    "        \"\"\"\n",
    "        判断目标叶结点及其兄弟结点是否都为叶结点, 如果是，则返回(父结点, 子节点们)\n",
    "        \"\"\"\n",
    "        prede_node = list(tree.predecessors(node))[0]\n",
    "        succe_nodes = list(tree.successors(node))\n",
    "        labels = np.array([tree.nodes[i]['name'] for i in succe_nodes])\n",
    "        if np.sum(labels != 'leaf_node') == 0:  # 兄弟结点需为叶结点\n",
    "            return [prede_node, succe_nodes]\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def evaluate_pruning(self, parent_node, leaf_nodes):\n",
    "        \"\"\"\n",
    "        判断是否进行一次剪枝操作\n",
    "        C(T_A) - C(T_B) <= 0, 则剪枝\n",
    "        \"\"\"\n",
    "        parent_y = self.tree.nodes[parent_node][\"y\"].values[:, 0]\n",
    "        p_freq = np.array([np.sum(parent_y == c) for c in np.unique(parent_y)])\n",
    "        p_entropy = np.dot(p_freq, np.log2(p_freq/parent_y.size))  # 父节点的熵\n",
    "        l_entropy = 0  # 叶节点的熵\n",
    "        for i in leaf_nodes:\n",
    "            i_y = self.tree.nodes[i][\"y\"].values[:, 0]\n",
    "            i_freq = np.array([np.sum(i_y == c) for c in np.unique(i_y)])\n",
    "            l_entropy += np.dot(i_freq, np.log2(i_freq/i_y.size))\n",
    "        \n",
    "        return p_entropy - l_entropy + self.alpha * (len(target) - 1)\n",
    "        \n",
    "    def prune_tree(self):\n",
    "        \"\"\"\n",
    "        利用正则化剪枝: 取所有兄弟节点都为叶节点的子树考虑\n",
    "        \"\"\"\n",
    "        tree = deepcopy(self.tree)\n",
    "        c_subtrees = []  # 待评估子树\n",
    "        for node in tree:\n",
    "            if node not in candidate_nodes and tree.nodes[node]['name'] == 'leaf_node':\n",
    "                parent_node, succe_nodes = self.select_nodes(node, tree)\n",
    "                c_subtrees.apppend([parent_node, succe_nodes])\n",
    "        \n",
    "        while c_subtrees:  # 无待评估子树时停止迭代\n",
    "            parent_node, leaf_nodes = c_subtrees.pop(0)  # 取出并删除第1个元素\n",
    "            if self.evaluate_pruning(parent_node, leaf_nodes) <= 0:  # 判断是否剪枝\n",
    "                tree.remove_nodes_from(leaf_nodes)  # 删除叶结点\n",
    "                tree.nodes[parent_node][\"name\"] = 'leaf_node' # 重置其父结点为叶结点      \n",
    "                parent_y = self.tree.nodes[parent_node][\"y\"].values[:, 0]\n",
    "                max_y = max([(c, np.sum(parent_y==c)) for c in np.unique(parent_y)], lambda x: x[1])[0]\n",
    "                tree.nodes[parent_node][\"category\"] = max_y  # 新叶结点对应的类别\n",
    "                c_subtrees.append(self.select_nodes(parent_node))  # 评估回缩的节点是否满足候选子树条件\n",
    "        \n",
    "        return tree\n",
    "            \n",
    "    def predict(self, x_dict, tree=self.tree):\n",
    "        \"\"\"\n",
    "        x_dict: 字典\n",
    "        \"\"\"\n",
    "        node = 0\n",
    "        while True:\n",
    "            xi = tree.nodes[node][\"name\"]\n",
    "            for i in tree.successors(node):\n",
    "                if tree.edges[node][i]['value'] == xi:\n",
    "                    break\n",
    "            else:  # 如果没有相等的值\n",
    "                print(f'目标属性值不存在{tree.nodes[node][\"name\"]} = {xi}')\n",
    "                break\n",
    "                \n",
    "            if tree.nodes[i]['name'] == 'leaf_node':\n",
    "                return tree.nodes[i]['category']\n",
    "            else:\n",
    "                node = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2). 分类树的生成\n",
    "分类树用基尼指数选择最有特征，同时决定该特征的最优二值切分点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**定义5.4（基尼指数）** \n",
    ">分类问题中，假设有K个类，样本点属于第K类的概率是$p_k$，则概率分布的基尼指数为\n",
    ">$$\n",
    "Gini(p)=\\sum_{k=1}^K p_k(1-p_k)=1-\\sum_{k=1}^K p_k^2\n",
    "$$\n",
    ">对于二分类问题，若样本点属于第1个类的概率是p，则概率分布的基尼指数为\n",
    ">$$\n",
    "Gini(p)=2p(1-p)\n",
    "$$\n",
    ">对于给定的样本集合D，其基尼指数为\n",
    ">$$\n",
    "Gini(D)=1-\\sum_{k=1}^K*\\left(\\frac{|C_k|}{|D|} \\right)^2\n",
    "$$\n",
    ">其中, $C_k$是D属于第k类的样本子集，K是类的数量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果样本D根据特征A是否取某一可能值a被分割为$D_1$和$D_2$两部分，即\n",
    "$$\n",
    "D_1=\\{(x,y)\\in D|A(x)=a\\},D_2=D-D_1\n",
    "$$\n",
    "则在特征A的条件下，集合D的基尼指数定义为\n",
    "$$\n",
    "Gini(D,A)=\\frac{D_1}{D}Gini(D_1)+\\frac{D_2}{D}Gini(D_2)\n",
    "$$\n",
    "基尼指数$Gini(D)$表示集合D的不确定性，基尼指数$Gini(D,A)$表示经$A=a$分割后集合D的不确定性。基尼指数值越大，样本集合的不确定也就越大，与熵相似。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**算法5.6 （CART生成算法）**\n",
    "- 输入：训练数据集D，停止计算条件\n",
    "- 输出：CART决策树\n",
    "- 算法过程：根据训练数据集，从根结点开始，递归地对每个结点进行一下操作，构建二叉树决策树\n",
    "    - 设结点的训练集D，计算现有所有特征及其不同取值对该数据集的基尼指数。\n",
    "    - 选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现有结点生成两个子结点，将训练数据集依特征分配到两个子结点中去\n",
    "    - 对两个子结点递归地调用以上两个步骤，直到满足停止条件\n",
    "    - 生成CART决策树\n",
    "    \n",
    ">算法停止计算的条件是结点中的样本个数小于预定阈值，或样本集的基尼指数小于预定阈值（样本基本属于一类），或者没有更多特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 CART剪枝\n",
    "\n",
    "和`id.3`,`c4.5`一样，CART二叉决策树的剪枝也是为了防止“过拟合”现象。CART剪枝算法由两个步骤组成：\n",
    "- 从决策树$T_0$底端不断开始剪枝，直到$T_0$的根结点，形成一个子树序列$\\{T_0,T_1,...,T_n\\}$\n",
    "- 通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**算法5.7（CART剪枝算法）**\n",
    "- 输入：CART算法生成的决策树$T_0$\n",
    "- 输出：最优决策树$T_{\\alpha}$\n",
    "- 算法过程\n",
    "    - 设$k=0,T=T_0$\n",
    "    - 设$\\alpha=+\\infty$\n",
    "    - 自下而上地对各内部结点t计算$C(T_t),|T_t|$以及\n",
    "    $$\n",
    "    g(t)=\\frac{C(t)-C(T_t)}{|T_t|-1}\\\\\n",
    "    \\alpha=\\min (\\alpha,g(t))\n",
    "    $$\n",
    "    这里，$T_t$表示以t为根结点的子树，$C(T_t)$是对训练数据的预测误差，$|T_t|$是$T_t$的叶结点个数。\n",
    "    - 对$g(t)=\\alpha$的内部结点t进行剪枝，并对叶结点以多数表决法决定其类，得到树T\n",
    "    - 设$k:=k+1,\\alpha_{k}=\\alpha,T_k=T$\n",
    "    - 如果$T_k$不是由根结点及两个叶结点构成的树，则回到步骤(2)；否则令$T_k=T_n$\n",
    "    - 采用交叉验证法在子树序列$T_0,T_1,...,T_n$中选择最优子树$T_{\\alpha}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
