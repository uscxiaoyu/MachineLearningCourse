{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = ['svg']\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 生成模拟数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearModel(X: torch.tensor, w: torch.tensor)-> torch.tensor:\n",
    "    return X@w.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成数据\n",
    "true_w = torch.tensor([2, -3.4, 4.2])  # 权重+偏置\n",
    "num_inputs = true_w.numel()  # 权重数量(true_w中元素个数)\n",
    "num_examples = 1000  # 样本数量\n",
    "features = torch.cat([torch.randn(num_examples, num_inputs - 1), torch.ones(num_examples).reshape(-1, 1)], axis=1 ) # 随机生成1000个特征(1000*2) 加上偏置\n",
    "labels = linearModel(features, true_w) + torch.randn(num_examples).reshape(-1, 1) * 0.01  # 将随机生成的特征输入linearModel，然后加入随机项（表示偏差）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (8, 3)\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "# y随第1个特征x_0的散点图\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.set_xlabel(\"$x_0$\")\n",
    "ax1.set_ylabel(\"y\")\n",
    "ax1.scatter(features[:, 0].numpy(), labels.numpy(), s=1)\n",
    "# y随第2个特征x_1的散点图\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.set_xlabel(\"$x_1$\")\n",
    "ax2.set_ylabel(\"y\")\n",
    "ax2.scatter(features[:, 1].numpy(), labels.numpy(), s=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `mse`损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanSquaredLoss(X: torch.tensor, y: torch.tensor, w: torch.tensor) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    y: 标签\n",
    "    \"\"\"\n",
    "    hat_y = X @ w.reshape(-1, 1)\n",
    "    sLoss = torch.dot(hat_y.reshape(-1) - y.reshape(-1), hat_y.reshape(-1) - y.reshape(-1)) / y.numel()\n",
    "    return sLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_mse_w(X, y, w):\n",
    "    '''\n",
    "    X: N*a, N为样本数量，a为（增广）特征维度\n",
    "    y: N\n",
    "    w: a*1\n",
    "    '''\n",
    "    return 2*(X.t()@(X@w.reshape(-1, 1) - y.reshape(-1, 1))) / y.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 梯度下降(gradient descendent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 解释1\n",
    "\n",
    "假设目标函数$f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$的输入是一个$d$维向量$\\boldsymbol{x} = [x_1, x_2, \\ldots, x_d]^\\top$。目标函数$f(\\boldsymbol{x})$有关$\\boldsymbol{x}$的梯度是一个由$d$个偏导数组成的向量：\n",
    "\n",
    "$$\\nabla_{\\boldsymbol{x}} f(\\boldsymbol{x}) = \\bigg[\\frac{\\partial f(\\boldsymbol{x})}{\\partial x_1}, \\frac{\\partial f(\\boldsymbol{x})}{\\partial x_2}, \\ldots, \\frac{\\partial f(\\boldsymbol{x})}{\\partial x_d}\\bigg]^\\top.$$\n",
    "\n",
    "为表示简洁，我们用$\\nabla f(\\boldsymbol{x})$代替$\\nabla_{\\boldsymbol{x}} f(\\boldsymbol{x})$。梯度中每个偏导数元素$\\partial f(\\boldsymbol{x})/\\partial x_i$代表着$f$在$\\boldsymbol{x}$有关输入$x_i$的变化率。为了测量$f$沿着单位向量$\\boldsymbol{u}$（即$\\|\\boldsymbol{u}\\|=1$）方向上的变化率，在多元微积分中，我们定义$f$在$\\boldsymbol{x}$上沿着$\\boldsymbol{u}$方向的方向导数为\n",
    "\n",
    "$$\\text{D}_{\\boldsymbol{u}} f(\\boldsymbol{x}) = \\lim_{h \\rightarrow 0}  \\frac{f(\\boldsymbol{x} + h \\boldsymbol{u}) - f(\\boldsymbol{x})}{h}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上方向导数可以改写为\n",
    "\n",
    "$$\\text{D}_{\\boldsymbol{u}} f(\\boldsymbol{x}) = \\nabla f(\\boldsymbol{x}) \\cdot \\boldsymbol{u}.$$\n",
    "\n",
    "方向导数$\\text{D}_{\\boldsymbol{u}} f(\\boldsymbol{x})$给出了$f$在$\\boldsymbol{x}$上沿着所有可能方向的变化率。为了最小化$f$，我们希望找到$f$能被降低最快的方向。因此，我们可以通过单位向量$\\boldsymbol{u}$来最小化方向导数$\\text{D}_{\\boldsymbol{u}} f(\\boldsymbol{x})$。\n",
    "\n",
    "由于$\\text{D}_{\\boldsymbol{u}} f(\\boldsymbol{x}) = \\|\\nabla f(\\boldsymbol{x})\\| \\cdot \\|\\boldsymbol{u}\\|  \\cdot \\text{cos} (\\theta) = \\|\\nabla f(\\boldsymbol{x})\\|  \\cdot \\text{cos} (\\theta)$，\n",
    "其中$\\theta$为梯度$\\nabla f(\\boldsymbol{x})$和单位向量$\\boldsymbol{u}$之间的夹角，当$\\theta = \\pi$时，$\\text{cos}(\\theta)$取得最小值$-1$。因此，当$\\boldsymbol{u}$在梯度方向$\\nabla f(\\boldsymbol{x})$的相反方向时，方向导数$\\text{D}_{\\boldsymbol{u}} f(\\boldsymbol{x})$被最小化。因此，我们可能通过梯度下降算法来不断降低目标函数$f$的值：\n",
    "\n",
    "$$\\boldsymbol{x} \\leftarrow \\boldsymbol{x} - \\eta \\nabla f(\\boldsymbol{x}).$$\n",
    "\n",
    "同样，其中$\\eta$（取正数）称作学习率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 解释2\n",
    "\n",
    "考虑$\\mathbf{x} \\in \\mathbb{R}^d$, 其中$\\mathbf{x}$为向量, 目标函数$f: \\mathbb{R}^d \\to \\mathbb{R}$映射至标量。对应的$f$关于$\\mathbf{x}$每一维度$x_i$的偏导构成梯度\n",
    "\n",
    "$$\\nabla f(\\mathbf{x}) = \\bigg[\\frac{\\partial f(\\mathbf{x})}{\\partial x_1}, \\frac{\\partial f(\\mathbf{x})}{\\partial x_2}, \\ldots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_d}\\bigg]^\\top.$$\n",
    "\n",
    "利用泰勒展开式可得\n",
    "\n",
    "$$f(\\mathbf{x} + \\Delta x) = f(\\mathbf{x}) + \\Delta\\mathbf{x}^\\top \\nabla f(\\mathbf{x}) + O(|\\mathbf{\\Delta x}|^2).$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "换而言之，最速下降方向由$-\\nabla f(\\mathbf{x})$给出。令$\\Delta \\mathbf{x}=-\\eta \\nabla f(\\mathbf{x})$\n",
    "\n",
    "$$f(\\mathbf{x} + \\Delta x) = f(\\mathbf{x}) - \\eta \\nabla f(\\mathbf{x})^T \\nabla f(\\mathbf{x}) + O(|\\Delta x|^2).$$ \n",
    "\n",
    "选定合适的学习率$\\eta > 0$，则可得梯度下降更新公式\n",
    "\n",
    "$$\\mathbf{x} \\leftarrow \\mathbf{x} - \\eta \\nabla f(\\mathbf{x}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批量梯度下降法在每次迭代时需要计算每个样本上损失函数的梯度并求和。当训练集中的样本数量𝑁很大时，空间复杂度比较高，每次迭代的计算开销也很大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**随机梯度下降**\n",
    "\n",
    "> $f_i(\\boldsymbol{x})$是有关索引为$i$的训练数据样本的损失函数，$n$是训练数据样本数，$\\boldsymbol{x}$是模型的参数向量，那么目标函数定义为\n",
    "> \n",
    "> $$f(\\boldsymbol{x}) = \\frac{1}{n} \\sum_{i = 1}^n f_i(\\boldsymbol{x}).$$\n",
    "> \n",
    "> 目标函数在$\\boldsymbol{x}$处的梯度计算为\n",
    "> \n",
    ">$$\\nabla f(\\boldsymbol{x}) = \\frac{1}{n} \\sum_{i = 1}^n \\nabla f_i(\\boldsymbol{x}).$$\n",
    ">\n",
    "> 如果使用梯度下降，每次自变量迭代的计算开销为$\\mathcal{O}(n)$，它随着$n$线性增长。因此，当训练数据样本数很大时，梯度下降每次迭代的计算开销很高。\n",
    ">\n",
    "> 随机梯度下降（stochastic gradient descent，SGD）减少了每次迭代的计算开销。在随机梯度下降的每次迭代中，我们随机均匀采样的一个样本索引$i\\in\\{1,\\ldots,n\\}$，并计算梯度$\\nabla f_i(\\boldsymbol{x})$来迭代$\\boldsymbol{x}$：\n",
    ">\n",
    "> $$\\boldsymbol{x} \\leftarrow \\boldsymbol{x} - \\eta \\nabla f_i(\\boldsymbol{x}).$$\n",
    ">\n",
    "> 这里$\\eta$同样是学习率。可以看到每次迭代的计算开销从梯度下降的$\\mathcal{O}(n)$降到了常数$\\mathcal{O}(1)$。\n",
    ">\n",
    "> 值得强调的是，随机梯度$\\nabla f_i(\\boldsymbol{x})$是对梯度$\\nabla f(\\boldsymbol{x})$的无偏估计：\n",
    ">\n",
    "> $$E_i \\nabla f_i(\\boldsymbol{x}) = \\frac{1}{n} \\sum_{i = 1}^n \\nabla f_i(\\boldsymbol{x}) = \\nabla f(\\boldsymbol{x}).$$\n",
    ">\n",
    "> 这意味着，平均来说，随机梯度是对梯度的一个良好的估计。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `numpy`实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"\n",
    "    函数: f(x0, x1) = x0**2 + x1**2\n",
    "    \"\"\"\n",
    "    return x[0] ** 2 + 2 * x[1] ** 2  # objective\n",
    "\n",
    "def grad_f(x):\n",
    "    \"\"\"\n",
    "    f(x)的梯度\n",
    "    \"\"\"\n",
    "    return np.array([2 * x[0], 4 * x[1]])  # gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_numpy(f, grad_f, x0, learn_rate=0.05, max_iter=100):\n",
    "    \"\"\"\n",
    "    f: 待优化目标函数, grad_f: f的梯度, x0: 参数初值, learn_rate: 学习率\n",
    "    \"\"\"\n",
    "    trace_x = np.array([x0])  # x的历史记录\n",
    "    x = x0\n",
    "    i = 1\n",
    "    while i <= max_iter:\n",
    "        x = x - learn_rate * grad_f(x)  # 更新x的值\n",
    "        trace_x = np.concatenate([trace_x, x.reshape(1, -1)])\n",
    "        i += 1\n",
    "        if i % 5 == 0:\n",
    "            print(f\"迭代次数: {i}, 目标函数值f: {f(x):.6f}\")\n",
    "\n",
    "        if np.sum(np.abs(trace_x[-1] - trace_x[-2])) < 1e-3:  # 停止条件\n",
    "            break\n",
    "\n",
    "    print(f\"共迭代{len(trace_x)}次, 目标函数: {f(x)}, 最优参数值: {x.tolist()}\")\n",
    "    return trace_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = gd_numpy(f, grad_f, x0=np.array([3, 3]), learn_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, x1 = res[:, 0], res[:, 1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x0, x1, \"-o\", color=\"#ff7f0e\")\n",
    "x0 = np.arange(-5.5, 5.0, 0.1)\n",
    "x1 = np.arange(min(-3.0, min(x1) - 1), max(1.0, max(x1) + 1), 0.1)\n",
    "x0, x1 = np.meshgrid(x0, x1)\n",
    "plt.contour(x0, x1, f([x0, x1]), colors=\"#1f77b4\", linewidths=1, linestyles=\"dashed\")\n",
    "plt.xlabel(\"x0\")\n",
    "plt.ylabel(\"x1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch`实现更通用的优化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_desc(lossfunc, w, x_dict, max_iters=200, learn_rate=0.05):\n",
    "    \"\"\"\n",
    "    f: 待优化目标函数, grad_f: f的梯度, w: 参数初值, x_dict:固定参数值, learn_rate: 学习率\n",
    "    \"\"\"\n",
    "    trace_w = w.clone().data.reshape(1, -1)\n",
    "    i = 0\n",
    "    for i in range(max_iters):\n",
    "        l = lossfunc(w=w, **x_dict)\n",
    "        l.backward()\n",
    "        w.data.sub_(learn_rate * w.grad.data)\n",
    "        with torch.no_grad():\n",
    "            trace_w = torch.cat([trace_w, w.detach().data.reshape(1, -1)], 0)\n",
    "            if (i + 1) % 10 == 0:\n",
    "                loss = lossfunc(w=w, **x_dict).data.numpy()\n",
    "                print(f\"迭代次数: {i+1}, 损失函数值: {loss:.4f}\")\n",
    "            \n",
    "            if torch.sum(torch.abs(trace_w[-1] - trace_w[-2])) < 1e-3:  # 停止条件\n",
    "                break\n",
    "\n",
    "        w.grad.zero_()\n",
    "\n",
    "    print(f\"共迭代{i}次, 损失函数值: {lossfunc(w=w, **x_dict).data.numpy():.4f}, 最优参数值: {w.tolist()}\")\n",
    "    return trace_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.randn(num_inputs, 1)\n",
    "w.requires_grad_(True)\n",
    "trace_w = grad_desc(meanSquaredLoss, w, x_dict = {'X': features, 'y': labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目标函数有关自变量的梯度代表了目标函数在自变量当前位置下降最快的方向。因此，梯度下降也叫作最陡下降（steepest descent）。在每次迭代中，梯度下降根据自变量当前位置，沿着当前位置的梯度更新自变量。\n",
    "\n",
    "然而，如果自变量的迭代方向仅仅取决于自变量当前位置，这可能会带来一些问题。例如，各维度上的梯度变化量不一致带来的学习率的权衡问题：如果设置过大的学习率，则将错过最优点，出现震荡发散；如果设置过小的学习率，则收率缓慢，在给定迭代时间内未达到最优点。因此，后续提出以动量和自适应算法为代表的以修正迭代算法。以下介绍其中的两种代表性算法：gradient descendent with moment和adam。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 动量梯度下降(gradient descendent with moment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设时间步$t$的自变量为$\\boldsymbol{x}_t$，学习率为$\\eta_t$。在时间步$0$，动量法创建速度变量$\\boldsymbol{v}_0$，并将其元素初始化成0。在时间步$t>0$，动量法对每次迭代的步骤做如下修改：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{v}_t &\\leftarrow \\gamma \\boldsymbol{v}_{t-1} + \\eta_t \\boldsymbol{g}_t, \\\\\n",
    "\\boldsymbol{x}_t &\\leftarrow \\boldsymbol{x}_{t-1} - \\boldsymbol{v}_t,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中，动量超参数$\\gamma$满足$0 \\leq \\gamma < 1$。当$\\gamma=0$时，动量法等价于小批量随机梯度下降。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `numpy`实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_momen_numpy(f, grad_f, x0, beta=0.5, learn_rate=0.05, max_iter=100):\n",
    "    trace_x = np.array([x0])\n",
    "    x = x0\n",
    "    m_0 = 0\n",
    "    i = 1\n",
    "    while i <= max_iter:\n",
    "        grad = grad_f(x)\n",
    "        m_1 = beta*m_0 + learn_rate*grad\n",
    "        x = x - m_1\n",
    "        trace_x = np.concatenate([trace_x, x.reshape(1, -1)])\n",
    "        if i % 5 == 0:\n",
    "            print(f\"迭代次数: {i}, 目标函数值f: {f(x):.6f}\")\n",
    "\n",
    "        if np.sum(np.abs(trace_x[-1] - trace_x[-2])) < 1e-3:  # 停止条件\n",
    "            break\n",
    "\n",
    "        m_0 = m_1\n",
    "        i += 1\n",
    "\n",
    "    print(f\"共迭代{len(trace_x)}次, 目标函数: {f(x)}, 最优参数值: {x.tolist()}\")\n",
    "    return trace_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = gd_momen_numpy(f, grad_f, x0=np.array([3, 3]), learn_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, x1 = res[:, 0], res[:, 1]\n",
    "\n",
    "# display.set_matplotlib_formats(\"svg\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x0, x1, \"-o\", color=\"#ff7f0e\")\n",
    "x0 = np.arange(-5.5, 5.0, 0.1)\n",
    "x1 = np.arange(min(-3.0, min(x1) - 1), max(1.0, max(x1) + 1), 0.1)\n",
    "x0, x1 = np.meshgrid(x0, x1)\n",
    "plt.contour(x0, x1, f([x0, x1]), colors=\"#1f77b4\", linewidths=1, linestyles=\"dashed\")\n",
    "plt.xlabel(\"x0\")\n",
    "plt.ylabel(\"x1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch`实现更通用版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_desc_with_momentum(lossfunc, w, x_dict, beta=0.5, learn_rate=0.05, max_iter=1000):\n",
    "    trace_w = w.clone().data.reshape(1, -1)\n",
    "    v_0 = 0\n",
    "    i = 1\n",
    "    while i <= max_iter:\n",
    "        l = lossfunc(w=w, **x_dict)\n",
    "        l.backward()\n",
    "        v_1 = beta*v_0 + learn_rate*w.grad.data\n",
    "        w.data.sub_(v_1)\n",
    "        with torch.no_grad():\n",
    "            trace_w = torch.cat([trace_w, w.detach().data.reshape(1, -1)], 0)\n",
    "            if i % 10 == 0:\n",
    "                loss = lossfunc(w=w, **x_dict).data.numpy()\n",
    "                print(f\"迭代次数: {i}, 损失函数值: {loss:.4f}\")\n",
    "            \n",
    "            if torch.sum(torch.abs(trace_w[-1] - trace_w[-2])) < 1e-3:  # 停止条件\n",
    "                break\n",
    "   \n",
    "        w.grad.zero_()\n",
    "        v_0 = v_1\n",
    "        i += 1\n",
    "\n",
    "    print(f\"共迭代{i-1}次, 损失函数值: {lossfunc(w=w, **x_dict).data.numpy():.4f}, 最优参数值: {w.tolist()}\")\n",
    "    return trace_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.randn(num_inputs, 1)\n",
    "w.requires_grad_(True)\n",
    "trace_w = grad_desc_with_momentum(meanSquaredLoss, w, x_dict={'X': features, 'y': labels}, learn_rate=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 适应性动量梯度下降(adaptive gradient descendent with moment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定超参数$0 \\leq \\beta_1 < 1$（算法作者建议设为0.9），时间步$t$的动量变量$\\boldsymbol{v}_t$即小批量随机梯度$\\boldsymbol{g}_t$的指数加权移动平均：\n",
    "\n",
    "$$\\boldsymbol{v}_t \\leftarrow \\beta_1 \\boldsymbol{v}_{t-1} + (1 - \\beta_1) \\boldsymbol{g}_t. $$\n",
    "\n",
    "给定超参数$0 \\leq \\beta_2 < 1$（算法作者建议设为0.999），将小批量随机梯度按元素平方后的项$\\boldsymbol{g}_t \\odot \\boldsymbol{g}_t$做指数加权移动平均得到$\\boldsymbol{s}_t$：\n",
    "\n",
    "$$\\boldsymbol{s}_t \\leftarrow \\beta_2 \\boldsymbol{s}_{t-1} + (1 - \\beta_2) \\boldsymbol{g}_t \\odot \\boldsymbol{g}_t. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于我们将$\\boldsymbol{v}_0$和$\\boldsymbol{s}_0$中的元素都初始化为0，\n",
    "在时间步$t$我们得到\n",
    "$$\\boldsymbol{v}_t =  (1-\\beta_1) \\sum_{i=1}^t \\beta_1^{t-i} \\boldsymbol{g}_i.$$\n",
    "\n",
    "将过去各时间步小批量随机梯度的权值相加，得到 \n",
    "$$(1-\\beta_1) \\sum_{i=1}^t \\beta_1^{t-i} = 1 - \\beta_1^t.$$\n",
    "\n",
    "需要注意的是，当$t$较小时，过去各时间步小批量随机梯度权值之和会较小。例如，当$\\beta_1 = 0.9$时，$\\boldsymbol{v}_1 = 0.1\\boldsymbol{g}_1$。为了消除这样的影响，对于任意时间步$t$，我们可以将$\\boldsymbol{v}_t$再除以$1 - \\beta_1^t$，从而使过去各时间步小批量随机梯度权值之和为1。这也叫作**偏差修正**。在Adam算法中，我们对变量$\\boldsymbol{v}_t$和$\\boldsymbol{s}_t$均作偏差修正：\n",
    "$$\\hat{\\boldsymbol{v}}_t \\leftarrow \\frac{\\boldsymbol{v}_t}{1 - \\beta_1^t}, $$\n",
    "\n",
    "$$\\hat{\\boldsymbol{s}}_t \\leftarrow \\frac{\\boldsymbol{s}_t}{1 - \\beta_2^t}. $$\n",
    "\n",
    "接下来，Adam算法使用以上偏差修正后的变量$\\hat{\\boldsymbol{v}}_t$和$\\hat{\\boldsymbol{s}}_t$，将模型参数中每个元素的学习率通过按元素运算重新调整：\n",
    "\n",
    "$$\\boldsymbol{g}_t' \\leftarrow \\frac{\\eta \\hat{\\boldsymbol{v}}_t}{\\sqrt{\\hat{\\boldsymbol{s}}_t} + \\epsilon},$$\n",
    "\n",
    "其中$\\eta$是学习率，$\\epsilon$是为了维持数值稳定性而添加的常数，如$10^{-8}$。和AdaGrad算法、RMSProp算法以及AdaDelta算法一样，目标函数自变量中每个元素都分别拥有自己的学习率。最后，使用$\\boldsymbol{g}_t'$迭代自变量：\n",
    "\n",
    "$$\\boldsymbol{x}_t \\leftarrow \\boldsymbol{x}_{t-1} - \\boldsymbol{g}_t'. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `numpy`实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_numpy(f, grad_f, x0, beta1=0.9, beta2=0.999, learn_rate=0.05, max_iter=100, epsilon=1e-8):\n",
    "    trace_x = np.array([x0])\n",
    "    x = x0\n",
    "    m_0, v_0 = 0, 0\n",
    "    i = 1\n",
    "    while i <= max_iter:\n",
    "        grad = grad_f(x)\n",
    "        m_1 = (beta1 * m_0 + (1 - beta1) * grad) / (1 - beta1**i)\n",
    "        v_1 = (beta2 * v_0 + (1 - beta2) * grad**2) / (1 - beta1**i)\n",
    "        x = x - learn_rate * m_1 / (np.sqrt(v_1) + epsilon)\n",
    "        trace_x = np.concatenate([trace_x, x.reshape(1, -1)])\n",
    "        if i % 5 == 0:\n",
    "            print(f\"迭代次数: {i}, 目标函数值f: {f(x):.6f}\")\n",
    "\n",
    "        if np.sum(np.abs(trace_x[-1] - trace_x[-2])) < 1e-3:  # 停止条件\n",
    "            break\n",
    "\n",
    "        m_0, v_0 = m_1, v_1\n",
    "        i += 1\n",
    "\n",
    "    print(f\"共迭代{len(trace_x)}次, 目标函数: {f(x)}, 最优参数值: {x.tolist()}\")\n",
    "    return trace_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = adam_numpy(f, grad_f, x0=np.array([3, 3]), beta1=0.6, beta2=0.5, max_iter=200, learn_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, x1 = res[:, 0], res[:, 1]\n",
    "# display.set_matplotlib_formats(\"svg\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x0, x1, \"-o\", color=\"#ff7f0e\")\n",
    "x0 = np.arange(-5.5, 5.0, 0.1)\n",
    "x1 = np.arange(min(-3.0, min(x1) - 1), max(1.0, max(x1) + 1), 0.1)\n",
    "x0, x1 = np.meshgrid(x0, x1)\n",
    "plt.contour(x0, x1, f([x0, x1]), colors=\"#1f77b4\", linewidths=1, linestyles=\"dashed\")\n",
    "plt.xlabel(\"x0\")\n",
    "plt.ylabel(\"x1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch`实现更通用版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_momentum(lossfunc, w, x_dict, beta1=0.5, beta2=0.9, learn_rate=0.999, max_iter=1000, epsilon=1e-8):\n",
    "    trace_w = w.clone().data.reshape(1, -1)\n",
    "    v_0, s_0 = 0, 0\n",
    "    i = 1\n",
    "    while i <= max_iter:\n",
    "        l = lossfunc(w=w, **x_dict)\n",
    "        l.backward() \n",
    "        v_1 = (beta1*v_0 + (1 - beta1)*w.grad.data) / (1 - beta1**i)\n",
    "        s_1 = (beta2*s_0 + (1 - beta2)*w.grad.data**2) / (1 - beta2**i)\n",
    "        w.data.sub_(learn_rate * v_1/(torch.sqrt(s_1) + epsilon))\n",
    "        with torch.no_grad():\n",
    "            trace_w = torch.cat([trace_w, w.detach().data.reshape(1, -1)], 0)\n",
    "            if i % 10 == 0:\n",
    "                loss = lossfunc(w=w, **x_dict).data.numpy()\n",
    "                print(f\"迭代次数: {i}, 损失函数值: {loss:.4f}\")\n",
    "            \n",
    "            if torch.sum(torch.abs(trace_w[-1] - trace_w[-2])) < 1e-3:  # 停止条件\n",
    "                break\n",
    "   \n",
    "        w.grad.zero_()\n",
    "        v_0, s_0 = v_1, s_1\n",
    "        i += 1\n",
    "\n",
    "    print(f\"共迭代{i - 1}次, 损失函数值: {lossfunc(w=w, **x_dict).data.numpy():.4f}, 最优参数值: {w.tolist()}\")\n",
    "    return trace_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.randn(num_inputs, 1)\n",
    "w.requires_grad_(True)\n",
    "trace_w = adaptive_momentum(meanSquaredLoss, w, x_dict={'X': features, 'y': labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考资料\n",
    "1. 李航. 统计学习方法. 2017.\n",
    "2. [阿斯顿·张、李沐、扎卡里 C. 立顿、亚历山大 J. 斯莫拉等. 动手学深度学习. 2020.](https://github.com/d2l-ai/d2l-zh)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
