{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = ['svg']\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearModel(X: torch.tensor, w: torch.tensor)-> torch.tensor:\n",
    "    return X@w.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆæ•°æ®\n",
    "true_w = torch.tensor([2, -3.4, 4.2])  # æƒé‡+åç½®\n",
    "num_inputs = true_w.numel()  # æƒé‡æ•°é‡(true_wä¸­å…ƒç´ ä¸ªæ•°)\n",
    "num_examples = 1000  # æ ·æœ¬æ•°é‡\n",
    "features = torch.cat([torch.randn(num_examples, num_inputs - 1), torch.ones(num_examples).reshape(-1, 1)], axis=1 ) # éšæœºç”Ÿæˆ1000ä¸ªç‰¹å¾(1000*2) åŠ ä¸Šåç½®\n",
    "labels = linearModel(features, true_w) + torch.randn(num_examples).reshape(-1, 1) * 0.01  # å°†éšæœºç”Ÿæˆçš„ç‰¹å¾è¾“å…¥linearModelï¼Œç„¶ååŠ å…¥éšæœºé¡¹ï¼ˆè¡¨ç¤ºåå·®ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (8, 3)\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "# yéšç¬¬1ä¸ªç‰¹å¾x_0çš„æ•£ç‚¹å›¾\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.set_xlabel(\"$x_0$\")\n",
    "ax1.set_ylabel(\"y\")\n",
    "ax1.scatter(features[:, 0].numpy(), labels.numpy(), s=1)\n",
    "# yéšç¬¬2ä¸ªç‰¹å¾x_1çš„æ•£ç‚¹å›¾\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.set_xlabel(\"$x_1$\")\n",
    "ax2.set_ylabel(\"y\")\n",
    "ax2.scatter(features[:, 1].numpy(), labels.numpy(), s=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `mse`æŸå¤±å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanSquaredLoss(X: torch.tensor, y: torch.tensor, w: torch.tensor) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    y: æ ‡ç­¾\n",
    "    \"\"\"\n",
    "    hat_y = X @ w.reshape(-1, 1)\n",
    "    sLoss = torch.dot(hat_y.reshape(-1) - y.reshape(-1), hat_y.reshape(-1) - y.reshape(-1)) / y.numel()\n",
    "    return sLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_mse_w(X, y, w):\n",
    "    '''\n",
    "    X: N*a, Nä¸ºæ ·æœ¬æ•°é‡ï¼Œaä¸ºï¼ˆå¢å¹¿ï¼‰ç‰¹å¾ç»´åº¦\n",
    "    y: N\n",
    "    w: a*1\n",
    "    '''\n",
    "    return 2*(X.t()@(X@w.reshape(-1, 1) - y.reshape(-1, 1))) / y.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. æ¢¯åº¦ä¸‹é™(gradient descendent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- è§£é‡Š1\n",
    "\n",
    "å‡è®¾ç›®æ ‡å‡½æ•°$f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$çš„è¾“å…¥æ˜¯ä¸€ä¸ª$d$ç»´å‘é‡$\\boldsymbol{x} = [x_1, x_2, \\ldots, x_d]^\\top$ã€‚ç›®æ ‡å‡½æ•°$f(\\boldsymbol{x})$æœ‰å…³$\\boldsymbol{x}$çš„æ¢¯åº¦æ˜¯ä¸€ä¸ªç”±$d$ä¸ªåå¯¼æ•°ç»„æˆçš„å‘é‡ï¼š\n",
    "\n",
    "$$\\nabla_{\\boldsymbol{x}} f(\\boldsymbol{x}) = \\bigg[\\frac{\\partial f(\\boldsymbol{x})}{\\partial x_1}, \\frac{\\partial f(\\boldsymbol{x})}{\\partial x_2}, \\ldots, \\frac{\\partial f(\\boldsymbol{x})}{\\partial x_d}\\bigg]^\\top.$$\n",
    "\n",
    "ä¸ºè¡¨ç¤ºç®€æ´ï¼Œæˆ‘ä»¬ç”¨$\\nabla f(\\boldsymbol{x})$ä»£æ›¿$\\nabla_{\\boldsymbol{x}} f(\\boldsymbol{x})$ã€‚æ¢¯åº¦ä¸­æ¯ä¸ªåå¯¼æ•°å…ƒç´ $\\partial f(\\boldsymbol{x})/\\partial x_i$ä»£è¡¨ç€$f$åœ¨$\\boldsymbol{x}$æœ‰å…³è¾“å…¥$x_i$çš„å˜åŒ–ç‡ã€‚ä¸ºäº†æµ‹é‡$f$æ²¿ç€å•ä½å‘é‡$\\boldsymbol{u}$ï¼ˆå³$\\|\\boldsymbol{u}\\|=1$ï¼‰æ–¹å‘ä¸Šçš„å˜åŒ–ç‡ï¼Œåœ¨å¤šå…ƒå¾®ç§¯åˆ†ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰$f$åœ¨$\\boldsymbol{x}$ä¸Šæ²¿ç€$\\boldsymbol{u}$æ–¹å‘çš„æ–¹å‘å¯¼æ•°ä¸º\n",
    "\n",
    "$$\\text{D}_{\\boldsymbol{u}} f(\\boldsymbol{x}) = \\lim_{h \\rightarrow 0}  \\frac{f(\\boldsymbol{x} + h \\boldsymbol{u}) - f(\\boldsymbol{x})}{h}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»¥ä¸Šæ–¹å‘å¯¼æ•°å¯ä»¥æ”¹å†™ä¸º\n",
    "\n",
    "$$\\text{D}_{\\boldsymbol{u}} f(\\boldsymbol{x}) = \\nabla f(\\boldsymbol{x}) \\cdot \\boldsymbol{u}.$$\n",
    "\n",
    "æ–¹å‘å¯¼æ•°$\\text{D}_{\\boldsymbol{u}} f(\\boldsymbol{x})$ç»™å‡ºäº†$f$åœ¨$\\boldsymbol{x}$ä¸Šæ²¿ç€æ‰€æœ‰å¯èƒ½æ–¹å‘çš„å˜åŒ–ç‡ã€‚ä¸ºäº†æœ€å°åŒ–$f$ï¼Œæˆ‘ä»¬å¸Œæœ›æ‰¾åˆ°$f$èƒ½è¢«é™ä½æœ€å¿«çš„æ–¹å‘ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å•ä½å‘é‡$\\boldsymbol{u}$æ¥æœ€å°åŒ–æ–¹å‘å¯¼æ•°$\\text{D}_{\\boldsymbol{u}} f(\\boldsymbol{x})$ã€‚\n",
    "\n",
    "ç”±äº$\\text{D}_{\\boldsymbol{u}} f(\\boldsymbol{x}) = \\|\\nabla f(\\boldsymbol{x})\\| \\cdot \\|\\boldsymbol{u}\\|  \\cdot \\text{cos} (\\theta) = \\|\\nabla f(\\boldsymbol{x})\\|  \\cdot \\text{cos} (\\theta)$ï¼Œ\n",
    "å…¶ä¸­$\\theta$ä¸ºæ¢¯åº¦$\\nabla f(\\boldsymbol{x})$å’Œå•ä½å‘é‡$\\boldsymbol{u}$ä¹‹é—´çš„å¤¹è§’ï¼Œå½“$\\theta = \\pi$æ—¶ï¼Œ$\\text{cos}(\\theta)$å–å¾—æœ€å°å€¼$-1$ã€‚å› æ­¤ï¼Œå½“$\\boldsymbol{u}$åœ¨æ¢¯åº¦æ–¹å‘$\\nabla f(\\boldsymbol{x})$çš„ç›¸åæ–¹å‘æ—¶ï¼Œæ–¹å‘å¯¼æ•°$\\text{D}_{\\boldsymbol{u}} f(\\boldsymbol{x})$è¢«æœ€å°åŒ–ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯èƒ½é€šè¿‡æ¢¯åº¦ä¸‹é™ç®—æ³•æ¥ä¸æ–­é™ä½ç›®æ ‡å‡½æ•°$f$çš„å€¼ï¼š\n",
    "\n",
    "$$\\boldsymbol{x} \\leftarrow \\boldsymbol{x} - \\eta \\nabla f(\\boldsymbol{x}).$$\n",
    "\n",
    "åŒæ ·ï¼Œå…¶ä¸­$\\eta$ï¼ˆå–æ­£æ•°ï¼‰ç§°ä½œå­¦ä¹ ç‡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- è§£é‡Š2\n",
    "\n",
    "è€ƒè™‘$\\mathbf{x} \\in \\mathbb{R}^d$, å…¶ä¸­$\\mathbf{x}$ä¸ºå‘é‡, ç›®æ ‡å‡½æ•°$f: \\mathbb{R}^d \\to \\mathbb{R}$æ˜ å°„è‡³æ ‡é‡ã€‚å¯¹åº”çš„$f$å…³äº$\\mathbf{x}$æ¯ä¸€ç»´åº¦$x_i$çš„åå¯¼æ„æˆæ¢¯åº¦\n",
    "\n",
    "$$\\nabla f(\\mathbf{x}) = \\bigg[\\frac{\\partial f(\\mathbf{x})}{\\partial x_1}, \\frac{\\partial f(\\mathbf{x})}{\\partial x_2}, \\ldots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_d}\\bigg]^\\top.$$\n",
    "\n",
    "åˆ©ç”¨æ³°å‹’å±•å¼€å¼å¯å¾—\n",
    "\n",
    "$$f(\\mathbf{x} + \\Delta x) = f(\\mathbf{x}) + \\Delta\\mathbf{x}^\\top \\nabla f(\\mathbf{x}) + O(|\\mathbf{\\Delta x}|^2).$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¢è€Œè¨€ä¹‹ï¼Œæœ€é€Ÿä¸‹é™æ–¹å‘ç”±$-\\nabla f(\\mathbf{x})$ç»™å‡ºã€‚ä»¤$\\Delta \\mathbf{x}=-\\eta \\nabla f(\\mathbf{x})$\n",
    "\n",
    "$$f(\\mathbf{x} + \\Delta x) = f(\\mathbf{x}) - \\eta \\nabla f(\\mathbf{x})^T \\nabla f(\\mathbf{x}) + O(|\\Delta x|^2).$$ \n",
    "\n",
    "é€‰å®šåˆé€‚çš„å­¦ä¹ ç‡$\\eta > 0$ï¼Œåˆ™å¯å¾—æ¢¯åº¦ä¸‹é™æ›´æ–°å…¬å¼\n",
    "\n",
    "$$\\mathbf{x} \\leftarrow \\mathbf{x} - \\eta \\nabla f(\\mathbf{x}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‰¹é‡æ¢¯åº¦ä¸‹é™æ³•åœ¨æ¯æ¬¡è¿­ä»£æ—¶éœ€è¦è®¡ç®—æ¯ä¸ªæ ·æœ¬ä¸ŠæŸå¤±å‡½æ•°çš„æ¢¯åº¦å¹¶æ±‚å’Œã€‚å½“è®­ç»ƒé›†ä¸­çš„æ ·æœ¬æ•°é‡ğ‘å¾ˆå¤§æ—¶ï¼Œç©ºé—´å¤æ‚åº¦æ¯”è¾ƒé«˜ï¼Œæ¯æ¬¡è¿­ä»£çš„è®¡ç®—å¼€é”€ä¹Ÿå¾ˆå¤§ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**éšæœºæ¢¯åº¦ä¸‹é™**\n",
    "\n",
    "> $f_i(\\boldsymbol{x})$æ˜¯æœ‰å…³ç´¢å¼•ä¸º$i$çš„è®­ç»ƒæ•°æ®æ ·æœ¬çš„æŸå¤±å‡½æ•°ï¼Œ$n$æ˜¯è®­ç»ƒæ•°æ®æ ·æœ¬æ•°ï¼Œ$\\boldsymbol{x}$æ˜¯æ¨¡å‹çš„å‚æ•°å‘é‡ï¼Œé‚£ä¹ˆç›®æ ‡å‡½æ•°å®šä¹‰ä¸º\n",
    "> \n",
    "> $$f(\\boldsymbol{x}) = \\frac{1}{n} \\sum_{i = 1}^n f_i(\\boldsymbol{x}).$$\n",
    "> \n",
    "> ç›®æ ‡å‡½æ•°åœ¨$\\boldsymbol{x}$å¤„çš„æ¢¯åº¦è®¡ç®—ä¸º\n",
    "> \n",
    ">$$\\nabla f(\\boldsymbol{x}) = \\frac{1}{n} \\sum_{i = 1}^n \\nabla f_i(\\boldsymbol{x}).$$\n",
    ">\n",
    "> å¦‚æœä½¿ç”¨æ¢¯åº¦ä¸‹é™ï¼Œæ¯æ¬¡è‡ªå˜é‡è¿­ä»£çš„è®¡ç®—å¼€é”€ä¸º$\\mathcal{O}(n)$ï¼Œå®ƒéšç€$n$çº¿æ€§å¢é•¿ã€‚å› æ­¤ï¼Œå½“è®­ç»ƒæ•°æ®æ ·æœ¬æ•°å¾ˆå¤§æ—¶ï¼Œæ¢¯åº¦ä¸‹é™æ¯æ¬¡è¿­ä»£çš„è®¡ç®—å¼€é”€å¾ˆé«˜ã€‚\n",
    ">\n",
    "> éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆstochastic gradient descentï¼ŒSGDï¼‰å‡å°‘äº†æ¯æ¬¡è¿­ä»£çš„è®¡ç®—å¼€é”€ã€‚åœ¨éšæœºæ¢¯åº¦ä¸‹é™çš„æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæˆ‘ä»¬éšæœºå‡åŒ€é‡‡æ ·çš„ä¸€ä¸ªæ ·æœ¬ç´¢å¼•$i\\in\\{1,\\ldots,n\\}$ï¼Œå¹¶è®¡ç®—æ¢¯åº¦$\\nabla f_i(\\boldsymbol{x})$æ¥è¿­ä»£$\\boldsymbol{x}$ï¼š\n",
    ">\n",
    "> $$\\boldsymbol{x} \\leftarrow \\boldsymbol{x} - \\eta \\nabla f_i(\\boldsymbol{x}).$$\n",
    ">\n",
    "> è¿™é‡Œ$\\eta$åŒæ ·æ˜¯å­¦ä¹ ç‡ã€‚å¯ä»¥çœ‹åˆ°æ¯æ¬¡è¿­ä»£çš„è®¡ç®—å¼€é”€ä»æ¢¯åº¦ä¸‹é™çš„$\\mathcal{O}(n)$é™åˆ°äº†å¸¸æ•°$\\mathcal{O}(1)$ã€‚\n",
    ">\n",
    "> å€¼å¾—å¼ºè°ƒçš„æ˜¯ï¼Œéšæœºæ¢¯åº¦$\\nabla f_i(\\boldsymbol{x})$æ˜¯å¯¹æ¢¯åº¦$\\nabla f(\\boldsymbol{x})$çš„æ— åä¼°è®¡ï¼š\n",
    ">\n",
    "> $$E_i \\nabla f_i(\\boldsymbol{x}) = \\frac{1}{n} \\sum_{i = 1}^n \\nabla f_i(\\boldsymbol{x}) = \\nabla f(\\boldsymbol{x}).$$\n",
    ">\n",
    "> è¿™æ„å‘³ç€ï¼Œå¹³å‡æ¥è¯´ï¼Œéšæœºæ¢¯åº¦æ˜¯å¯¹æ¢¯åº¦çš„ä¸€ä¸ªè‰¯å¥½çš„ä¼°è®¡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `numpy`å®ç°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"\n",
    "    å‡½æ•°: f(x0, x1) = x0**2 + x1**2\n",
    "    \"\"\"\n",
    "    return x[0] ** 2 + 2 * x[1] ** 2  # objective\n",
    "\n",
    "def grad_f(x):\n",
    "    \"\"\"\n",
    "    f(x)çš„æ¢¯åº¦\n",
    "    \"\"\"\n",
    "    return np.array([2 * x[0], 4 * x[1]])  # gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_numpy(f, grad_f, x0, learn_rate=0.05, max_iter=100):\n",
    "    \"\"\"\n",
    "    f: å¾…ä¼˜åŒ–ç›®æ ‡å‡½æ•°, grad_f: fçš„æ¢¯åº¦, x0: å‚æ•°åˆå€¼, learn_rate: å­¦ä¹ ç‡\n",
    "    \"\"\"\n",
    "    trace_x = np.array([x0])  # xçš„å†å²è®°å½•\n",
    "    x = x0\n",
    "    i = 1\n",
    "    while i <= max_iter:\n",
    "        x = x - learn_rate * grad_f(x)  # æ›´æ–°xçš„å€¼\n",
    "        trace_x = np.concatenate([trace_x, x.reshape(1, -1)])\n",
    "        i += 1\n",
    "        if i % 5 == 0:\n",
    "            print(f\"è¿­ä»£æ¬¡æ•°: {i}, ç›®æ ‡å‡½æ•°å€¼f: {f(x):.6f}\")\n",
    "\n",
    "        if np.sum(np.abs(trace_x[-1] - trace_x[-2])) < 1e-3:  # åœæ­¢æ¡ä»¶\n",
    "            break\n",
    "\n",
    "    print(f\"å…±è¿­ä»£{len(trace_x)}æ¬¡, ç›®æ ‡å‡½æ•°: {f(x)}, æœ€ä¼˜å‚æ•°å€¼: {x.tolist()}\")\n",
    "    return trace_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = gd_numpy(f, grad_f, x0=np.array([3, 3]), learn_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, x1 = res[:, 0], res[:, 1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x0, x1, \"-o\", color=\"#ff7f0e\")\n",
    "x0 = np.arange(-5.5, 5.0, 0.1)\n",
    "x1 = np.arange(min(-3.0, min(x1) - 1), max(1.0, max(x1) + 1), 0.1)\n",
    "x0, x1 = np.meshgrid(x0, x1)\n",
    "plt.contour(x0, x1, f([x0, x1]), colors=\"#1f77b4\", linewidths=1, linestyles=\"dashed\")\n",
    "plt.xlabel(\"x0\")\n",
    "plt.ylabel(\"x1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch`å®ç°æ›´é€šç”¨çš„ä¼˜åŒ–å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_desc(lossfunc, w, x_dict, max_iters=200, learn_rate=0.05):\n",
    "    \"\"\"\n",
    "    f: å¾…ä¼˜åŒ–ç›®æ ‡å‡½æ•°, grad_f: fçš„æ¢¯åº¦, w: å‚æ•°åˆå€¼, x_dict:å›ºå®šå‚æ•°å€¼, learn_rate: å­¦ä¹ ç‡\n",
    "    \"\"\"\n",
    "    trace_w = w.clone().data.reshape(1, -1)\n",
    "    i = 0\n",
    "    for i in range(max_iters):\n",
    "        l = lossfunc(w=w, **x_dict)\n",
    "        l.backward()\n",
    "        w.data.sub_(learn_rate * w.grad.data)\n",
    "        with torch.no_grad():\n",
    "            trace_w = torch.cat([trace_w, w.detach().data.reshape(1, -1)], 0)\n",
    "            if (i + 1) % 10 == 0:\n",
    "                loss = lossfunc(w=w, **x_dict).data.numpy()\n",
    "                print(f\"è¿­ä»£æ¬¡æ•°: {i+1}, æŸå¤±å‡½æ•°å€¼: {loss:.4f}\")\n",
    "            \n",
    "            if torch.sum(torch.abs(trace_w[-1] - trace_w[-2])) < 1e-3:  # åœæ­¢æ¡ä»¶\n",
    "                break\n",
    "\n",
    "        w.grad.zero_()\n",
    "\n",
    "    print(f\"å…±è¿­ä»£{i}æ¬¡, æŸå¤±å‡½æ•°å€¼: {lossfunc(w=w, **x_dict).data.numpy():.4f}, æœ€ä¼˜å‚æ•°å€¼: {w.tolist()}\")\n",
    "    return trace_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.randn(num_inputs, 1)\n",
    "w.requires_grad_(True)\n",
    "trace_w = grad_desc(meanSquaredLoss, w, x_dict = {'X': features, 'y': labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç›®æ ‡å‡½æ•°æœ‰å…³è‡ªå˜é‡çš„æ¢¯åº¦ä»£è¡¨äº†ç›®æ ‡å‡½æ•°åœ¨è‡ªå˜é‡å½“å‰ä½ç½®ä¸‹é™æœ€å¿«çš„æ–¹å‘ã€‚å› æ­¤ï¼Œæ¢¯åº¦ä¸‹é™ä¹Ÿå«ä½œæœ€é™¡ä¸‹é™ï¼ˆsteepest descentï¼‰ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæ¢¯åº¦ä¸‹é™æ ¹æ®è‡ªå˜é‡å½“å‰ä½ç½®ï¼Œæ²¿ç€å½“å‰ä½ç½®çš„æ¢¯åº¦æ›´æ–°è‡ªå˜é‡ã€‚\n",
    "\n",
    "ç„¶è€Œï¼Œå¦‚æœè‡ªå˜é‡çš„è¿­ä»£æ–¹å‘ä»…ä»…å–å†³äºè‡ªå˜é‡å½“å‰ä½ç½®ï¼Œè¿™å¯èƒ½ä¼šå¸¦æ¥ä¸€äº›é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œå„ç»´åº¦ä¸Šçš„æ¢¯åº¦å˜åŒ–é‡ä¸ä¸€è‡´å¸¦æ¥çš„å­¦ä¹ ç‡çš„æƒè¡¡é—®é¢˜ï¼šå¦‚æœè®¾ç½®è¿‡å¤§çš„å­¦ä¹ ç‡ï¼Œåˆ™å°†é”™è¿‡æœ€ä¼˜ç‚¹ï¼Œå‡ºç°éœ‡è¡å‘æ•£ï¼›å¦‚æœè®¾ç½®è¿‡å°çš„å­¦ä¹ ç‡ï¼Œåˆ™æ”¶ç‡ç¼“æ…¢ï¼Œåœ¨ç»™å®šè¿­ä»£æ—¶é—´å†…æœªè¾¾åˆ°æœ€ä¼˜ç‚¹ã€‚å› æ­¤ï¼Œåç»­æå‡ºä»¥åŠ¨é‡å’Œè‡ªé€‚åº”ç®—æ³•ä¸ºä»£è¡¨çš„ä»¥ä¿®æ­£è¿­ä»£ç®—æ³•ã€‚ä»¥ä¸‹ä»‹ç»å…¶ä¸­çš„ä¸¤ç§ä»£è¡¨æ€§ç®—æ³•ï¼šgradient descendent with momentå’Œadamã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. åŠ¨é‡æ¢¯åº¦ä¸‹é™(gradient descendent with moment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è®¾æ—¶é—´æ­¥$t$çš„è‡ªå˜é‡ä¸º$\\boldsymbol{x}_t$ï¼Œå­¦ä¹ ç‡ä¸º$\\eta_t$ã€‚åœ¨æ—¶é—´æ­¥$0$ï¼ŒåŠ¨é‡æ³•åˆ›å»ºé€Ÿåº¦å˜é‡$\\boldsymbol{v}_0$ï¼Œå¹¶å°†å…¶å…ƒç´ åˆå§‹åŒ–æˆ0ã€‚åœ¨æ—¶é—´æ­¥$t>0$ï¼ŒåŠ¨é‡æ³•å¯¹æ¯æ¬¡è¿­ä»£çš„æ­¥éª¤åšå¦‚ä¸‹ä¿®æ”¹ï¼š\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{v}_t &\\leftarrow \\gamma \\boldsymbol{v}_{t-1} + \\eta_t \\boldsymbol{g}_t, \\\\\n",
    "\\boldsymbol{x}_t &\\leftarrow \\boldsymbol{x}_{t-1} - \\boldsymbol{v}_t,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ï¼ŒåŠ¨é‡è¶…å‚æ•°$\\gamma$æ»¡è¶³$0 \\leq \\gamma < 1$ã€‚å½“$\\gamma=0$æ—¶ï¼ŒåŠ¨é‡æ³•ç­‰ä»·äºå°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `numpy`å®ç°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_momen_numpy(f, grad_f, x0, beta=0.5, learn_rate=0.05, max_iter=100):\n",
    "    trace_x = np.array([x0])\n",
    "    x = x0\n",
    "    m_0 = 0\n",
    "    i = 1\n",
    "    while i <= max_iter:\n",
    "        grad = grad_f(x)\n",
    "        m_1 = beta*m_0 + learn_rate*grad\n",
    "        x = x - m_1\n",
    "        trace_x = np.concatenate([trace_x, x.reshape(1, -1)])\n",
    "        if i % 5 == 0:\n",
    "            print(f\"è¿­ä»£æ¬¡æ•°: {i}, ç›®æ ‡å‡½æ•°å€¼f: {f(x):.6f}\")\n",
    "\n",
    "        if np.sum(np.abs(trace_x[-1] - trace_x[-2])) < 1e-3:  # åœæ­¢æ¡ä»¶\n",
    "            break\n",
    "\n",
    "        m_0 = m_1\n",
    "        i += 1\n",
    "\n",
    "    print(f\"å…±è¿­ä»£{len(trace_x)}æ¬¡, ç›®æ ‡å‡½æ•°: {f(x)}, æœ€ä¼˜å‚æ•°å€¼: {x.tolist()}\")\n",
    "    return trace_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = gd_momen_numpy(f, grad_f, x0=np.array([3, 3]), learn_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, x1 = res[:, 0], res[:, 1]\n",
    "\n",
    "# display.set_matplotlib_formats(\"svg\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x0, x1, \"-o\", color=\"#ff7f0e\")\n",
    "x0 = np.arange(-5.5, 5.0, 0.1)\n",
    "x1 = np.arange(min(-3.0, min(x1) - 1), max(1.0, max(x1) + 1), 0.1)\n",
    "x0, x1 = np.meshgrid(x0, x1)\n",
    "plt.contour(x0, x1, f([x0, x1]), colors=\"#1f77b4\", linewidths=1, linestyles=\"dashed\")\n",
    "plt.xlabel(\"x0\")\n",
    "plt.ylabel(\"x1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch`å®ç°æ›´é€šç”¨ç‰ˆæœ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_desc_with_momentum(lossfunc, w, x_dict, beta=0.5, learn_rate=0.05, max_iter=1000):\n",
    "    trace_w = w.clone().data.reshape(1, -1)\n",
    "    v_0 = 0\n",
    "    i = 1\n",
    "    while i <= max_iter:\n",
    "        l = lossfunc(w=w, **x_dict)\n",
    "        l.backward()\n",
    "        v_1 = beta*v_0 + learn_rate*w.grad.data\n",
    "        w.data.sub_(v_1)\n",
    "        with torch.no_grad():\n",
    "            trace_w = torch.cat([trace_w, w.detach().data.reshape(1, -1)], 0)\n",
    "            if i % 10 == 0:\n",
    "                loss = lossfunc(w=w, **x_dict).data.numpy()\n",
    "                print(f\"è¿­ä»£æ¬¡æ•°: {i}, æŸå¤±å‡½æ•°å€¼: {loss:.4f}\")\n",
    "            \n",
    "            if torch.sum(torch.abs(trace_w[-1] - trace_w[-2])) < 1e-3:  # åœæ­¢æ¡ä»¶\n",
    "                break\n",
    "   \n",
    "        w.grad.zero_()\n",
    "        v_0 = v_1\n",
    "        i += 1\n",
    "\n",
    "    print(f\"å…±è¿­ä»£{i-1}æ¬¡, æŸå¤±å‡½æ•°å€¼: {lossfunc(w=w, **x_dict).data.numpy():.4f}, æœ€ä¼˜å‚æ•°å€¼: {w.tolist()}\")\n",
    "    return trace_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.randn(num_inputs, 1)\n",
    "w.requires_grad_(True)\n",
    "trace_w = grad_desc_with_momentum(meanSquaredLoss, w, x_dict={'X': features, 'y': labels}, learn_rate=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. é€‚åº”æ€§åŠ¨é‡æ¢¯åº¦ä¸‹é™(adaptive gradient descendent with moment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç»™å®šè¶…å‚æ•°$0 \\leq \\beta_1 < 1$ï¼ˆç®—æ³•ä½œè€…å»ºè®®è®¾ä¸º0.9ï¼‰ï¼Œæ—¶é—´æ­¥$t$çš„åŠ¨é‡å˜é‡$\\boldsymbol{v}_t$å³å°æ‰¹é‡éšæœºæ¢¯åº¦$\\boldsymbol{g}_t$çš„æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡ï¼š\n",
    "\n",
    "$$\\boldsymbol{v}_t \\leftarrow \\beta_1 \\boldsymbol{v}_{t-1} + (1 - \\beta_1) \\boldsymbol{g}_t. $$\n",
    "\n",
    "ç»™å®šè¶…å‚æ•°$0 \\leq \\beta_2 < 1$ï¼ˆç®—æ³•ä½œè€…å»ºè®®è®¾ä¸º0.999ï¼‰ï¼Œå°†å°æ‰¹é‡éšæœºæ¢¯åº¦æŒ‰å…ƒç´ å¹³æ–¹åçš„é¡¹$\\boldsymbol{g}_t \\odot \\boldsymbol{g}_t$åšæŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡å¾—åˆ°$\\boldsymbol{s}_t$ï¼š\n",
    "\n",
    "$$\\boldsymbol{s}_t \\leftarrow \\beta_2 \\boldsymbol{s}_{t-1} + (1 - \\beta_2) \\boldsymbol{g}_t \\odot \\boldsymbol{g}_t. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç”±äºæˆ‘ä»¬å°†$\\boldsymbol{v}_0$å’Œ$\\boldsymbol{s}_0$ä¸­çš„å…ƒç´ éƒ½åˆå§‹åŒ–ä¸º0ï¼Œ\n",
    "åœ¨æ—¶é—´æ­¥$t$æˆ‘ä»¬å¾—åˆ°\n",
    "$$\\boldsymbol{v}_t =  (1-\\beta_1) \\sum_{i=1}^t \\beta_1^{t-i} \\boldsymbol{g}_i.$$\n",
    "\n",
    "å°†è¿‡å»å„æ—¶é—´æ­¥å°æ‰¹é‡éšæœºæ¢¯åº¦çš„æƒå€¼ç›¸åŠ ï¼Œå¾—åˆ° \n",
    "$$(1-\\beta_1) \\sum_{i=1}^t \\beta_1^{t-i} = 1 - \\beta_1^t.$$\n",
    "\n",
    "éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå½“$t$è¾ƒå°æ—¶ï¼Œè¿‡å»å„æ—¶é—´æ­¥å°æ‰¹é‡éšæœºæ¢¯åº¦æƒå€¼ä¹‹å’Œä¼šè¾ƒå°ã€‚ä¾‹å¦‚ï¼Œå½“$\\beta_1 = 0.9$æ—¶ï¼Œ$\\boldsymbol{v}_1 = 0.1\\boldsymbol{g}_1$ã€‚ä¸ºäº†æ¶ˆé™¤è¿™æ ·çš„å½±å“ï¼Œå¯¹äºä»»æ„æ—¶é—´æ­¥$t$ï¼Œæˆ‘ä»¬å¯ä»¥å°†$\\boldsymbol{v}_t$å†é™¤ä»¥$1 - \\beta_1^t$ï¼Œä»è€Œä½¿è¿‡å»å„æ—¶é—´æ­¥å°æ‰¹é‡éšæœºæ¢¯åº¦æƒå€¼ä¹‹å’Œä¸º1ã€‚è¿™ä¹Ÿå«ä½œ**åå·®ä¿®æ­£**ã€‚åœ¨Adamç®—æ³•ä¸­ï¼Œæˆ‘ä»¬å¯¹å˜é‡$\\boldsymbol{v}_t$å’Œ$\\boldsymbol{s}_t$å‡ä½œåå·®ä¿®æ­£ï¼š\n",
    "$$\\hat{\\boldsymbol{v}}_t \\leftarrow \\frac{\\boldsymbol{v}_t}{1 - \\beta_1^t}, $$\n",
    "\n",
    "$$\\hat{\\boldsymbol{s}}_t \\leftarrow \\frac{\\boldsymbol{s}_t}{1 - \\beta_2^t}. $$\n",
    "\n",
    "æ¥ä¸‹æ¥ï¼ŒAdamç®—æ³•ä½¿ç”¨ä»¥ä¸Šåå·®ä¿®æ­£åçš„å˜é‡$\\hat{\\boldsymbol{v}}_t$å’Œ$\\hat{\\boldsymbol{s}}_t$ï¼Œå°†æ¨¡å‹å‚æ•°ä¸­æ¯ä¸ªå…ƒç´ çš„å­¦ä¹ ç‡é€šè¿‡æŒ‰å…ƒç´ è¿ç®—é‡æ–°è°ƒæ•´ï¼š\n",
    "\n",
    "$$\\boldsymbol{g}_t' \\leftarrow \\frac{\\eta \\hat{\\boldsymbol{v}}_t}{\\sqrt{\\hat{\\boldsymbol{s}}_t} + \\epsilon},$$\n",
    "\n",
    "å…¶ä¸­$\\eta$æ˜¯å­¦ä¹ ç‡ï¼Œ$\\epsilon$æ˜¯ä¸ºäº†ç»´æŒæ•°å€¼ç¨³å®šæ€§è€Œæ·»åŠ çš„å¸¸æ•°ï¼Œå¦‚$10^{-8}$ã€‚å’ŒAdaGradç®—æ³•ã€RMSPropç®—æ³•ä»¥åŠAdaDeltaç®—æ³•ä¸€æ ·ï¼Œç›®æ ‡å‡½æ•°è‡ªå˜é‡ä¸­æ¯ä¸ªå…ƒç´ éƒ½åˆ†åˆ«æ‹¥æœ‰è‡ªå·±çš„å­¦ä¹ ç‡ã€‚æœ€åï¼Œä½¿ç”¨$\\boldsymbol{g}_t'$è¿­ä»£è‡ªå˜é‡ï¼š\n",
    "\n",
    "$$\\boldsymbol{x}_t \\leftarrow \\boldsymbol{x}_{t-1} - \\boldsymbol{g}_t'. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `numpy`å®ç°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_numpy(f, grad_f, x0, beta1=0.9, beta2=0.999, learn_rate=0.05, max_iter=100, epsilon=1e-8):\n",
    "    trace_x = np.array([x0])\n",
    "    x = x0\n",
    "    m_0, v_0 = 0, 0\n",
    "    i = 1\n",
    "    while i <= max_iter:\n",
    "        grad = grad_f(x)\n",
    "        m_1 = (beta1 * m_0 + (1 - beta1) * grad) / (1 - beta1**i)\n",
    "        v_1 = (beta2 * v_0 + (1 - beta2) * grad**2) / (1 - beta1**i)\n",
    "        x = x - learn_rate * m_1 / (np.sqrt(v_1) + epsilon)\n",
    "        trace_x = np.concatenate([trace_x, x.reshape(1, -1)])\n",
    "        if i % 5 == 0:\n",
    "            print(f\"è¿­ä»£æ¬¡æ•°: {i}, ç›®æ ‡å‡½æ•°å€¼f: {f(x):.6f}\")\n",
    "\n",
    "        if np.sum(np.abs(trace_x[-1] - trace_x[-2])) < 1e-3:  # åœæ­¢æ¡ä»¶\n",
    "            break\n",
    "\n",
    "        m_0, v_0 = m_1, v_1\n",
    "        i += 1\n",
    "\n",
    "    print(f\"å…±è¿­ä»£{len(trace_x)}æ¬¡, ç›®æ ‡å‡½æ•°: {f(x)}, æœ€ä¼˜å‚æ•°å€¼: {x.tolist()}\")\n",
    "    return trace_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = adam_numpy(f, grad_f, x0=np.array([3, 3]), beta1=0.6, beta2=0.5, max_iter=200, learn_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, x1 = res[:, 0], res[:, 1]\n",
    "# display.set_matplotlib_formats(\"svg\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x0, x1, \"-o\", color=\"#ff7f0e\")\n",
    "x0 = np.arange(-5.5, 5.0, 0.1)\n",
    "x1 = np.arange(min(-3.0, min(x1) - 1), max(1.0, max(x1) + 1), 0.1)\n",
    "x0, x1 = np.meshgrid(x0, x1)\n",
    "plt.contour(x0, x1, f([x0, x1]), colors=\"#1f77b4\", linewidths=1, linestyles=\"dashed\")\n",
    "plt.xlabel(\"x0\")\n",
    "plt.ylabel(\"x1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch`å®ç°æ›´é€šç”¨ç‰ˆæœ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_momentum(lossfunc, w, x_dict, beta1=0.5, beta2=0.9, learn_rate=0.999, max_iter=1000, epsilon=1e-8):\n",
    "    trace_w = w.clone().data.reshape(1, -1)\n",
    "    v_0, s_0 = 0, 0\n",
    "    i = 1\n",
    "    while i <= max_iter:\n",
    "        l = lossfunc(w=w, **x_dict)\n",
    "        l.backward() \n",
    "        v_1 = (beta1*v_0 + (1 - beta1)*w.grad.data) / (1 - beta1**i)\n",
    "        s_1 = (beta2*s_0 + (1 - beta2)*w.grad.data**2) / (1 - beta2**i)\n",
    "        w.data.sub_(learn_rate * v_1/(torch.sqrt(s_1) + epsilon))\n",
    "        with torch.no_grad():\n",
    "            trace_w = torch.cat([trace_w, w.detach().data.reshape(1, -1)], 0)\n",
    "            if i % 10 == 0:\n",
    "                loss = lossfunc(w=w, **x_dict).data.numpy()\n",
    "                print(f\"è¿­ä»£æ¬¡æ•°: {i}, æŸå¤±å‡½æ•°å€¼: {loss:.4f}\")\n",
    "            \n",
    "            if torch.sum(torch.abs(trace_w[-1] - trace_w[-2])) < 1e-3:  # åœæ­¢æ¡ä»¶\n",
    "                break\n",
    "   \n",
    "        w.grad.zero_()\n",
    "        v_0, s_0 = v_1, s_1\n",
    "        i += 1\n",
    "\n",
    "    print(f\"å…±è¿­ä»£{i - 1}æ¬¡, æŸå¤±å‡½æ•°å€¼: {lossfunc(w=w, **x_dict).data.numpy():.4f}, æœ€ä¼˜å‚æ•°å€¼: {w.tolist()}\")\n",
    "    return trace_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.randn(num_inputs, 1)\n",
    "w.requires_grad_(True)\n",
    "trace_w = adaptive_momentum(meanSquaredLoss, w, x_dict={'X': features, 'y': labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å‚è€ƒèµ„æ–™\n",
    "1. æèˆª. ç»Ÿè®¡å­¦ä¹ æ–¹æ³•. 2017.\n",
    "2. [é˜¿æ–¯é¡¿Â·å¼ ã€ææ²ã€æ‰å¡é‡Œ C. ç«‹é¡¿ã€äºšå†å±±å¤§ J. æ–¯è«æ‹‰ç­‰. åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ . 2020.](https://github.com/d2l-ai/d2l-zh)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
