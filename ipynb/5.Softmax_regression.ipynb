{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 五. `softmax`回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 概述\n",
    "\n",
    "Softmax回归（Softmax Regression），也称为多项（Multinomial）或多类（Multi-Class）的Logistic回归，是Logistic回归在多分类问题上的推广。\n",
    "\n",
    "对于多类问题，类别标签$y \\in {1, 2,..., C}$ 可以有C个取值．给定一个样本x，Softmax 回归预测的属于类别c的条件概率为\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(y=c|\\mathbf{x})&=\\mathrm{softmax}(\\mathbf{w^T_cx})\\\\\n",
    "&=\\frac{\\exp(\\mathbf{w^T_cx})}{\\sum_{i=1}^C \\exp(\\mathbf{w^T_ix})}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中$\\mathbf{w_i}$是第i类的权重向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = ['svg']\n",
    "%matplotlib inline\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, W):\n",
    "    \"\"\"\n",
    "    X: torch.FloatTensor, N*a, N样本数量, a为特征的维度\n",
    "    W: torch.FloatTensor, a*C, C为类别数量\n",
    "    \"\"\"\n",
    "    C = torch.exp(X@W)  # hat_y, N*C\n",
    "    return C / torch.sum(C, axis=1).reshape(X.shape[0], -1)  # 各样本对应类别的标准化概率分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1584, 0.3256, 0.5161],\n",
       "        [0.0928, 0.5208, 0.3864],\n",
       "        [0.2508, 0.4578, 0.2914],\n",
       "        [0.0253, 0.8853, 0.0895],\n",
       "        [0.3327, 0.1074, 0.5599],\n",
       "        [0.7337, 0.0700, 0.1963],\n",
       "        [0.4860, 0.1457, 0.3683],\n",
       "        [0.2206, 0.1274, 0.6520],\n",
       "        [0.0067, 0.9760, 0.0172],\n",
       "        [0.7444, 0.0281, 0.2275]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(10, 5)\n",
    "W = torch.randn(5, 3)\n",
    "softmax(X, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Softmax回归的决策函数\n",
    "\n",
    "Softmax回归的决策函数可以表示为\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{y}&=\\text{arg}\\max_{i=1}^{C}p(y=c|\\mathbf{x})\\\\\n",
    "&=\\text{arg}\\max_{i=1}^{C}\\mathbf{w_i^Tx}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hat_y(X, W):\n",
    "    S = softmax(X, W)  # 各样本在各类别上的概率\n",
    "    max_indices = torch.max(S, dim=1)[1]\n",
    "    pred_y = torch.zeros_like(S)\n",
    "    pred_y[torch.arange(S.shape[0]), max_indices] = 1\n",
    "    return max_indices, pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = hat_y(X, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 1, 1, 2, 0, 0, 2, 1, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 与`Logistic`回归的关系。当类别数$C=2$时，softmax回归的决策函数为\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{y}&=\\text{arg}\\max_{i\\in\\{1,2\\}}p(y=c|\\mathbf{x})\\\\\n",
    "&=\\text{arg}\\max_{i\\in\\{1,2\\}}\\mathbf{w_i^Tx}\\\\\n",
    "&=I(\\mathbf{(w_2-w_1)^Tx}>0))\n",
    "\\end{aligned}\n",
    "$$\n",
    "其中$I(\\cdot)$是指示函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 准则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定N个训练样本，Softmax回归使用交叉熵损失函数学习最有的参数矩阵$W$。为了方便起见，使用C维的`one-hot`向量表示类别标签，对于类别i，其向量表示为\n",
    "$$\n",
    "y = [I(i=1), I(i=2), ..., I(i=C)]\n",
    "$$\n",
    "\n",
    "采用交叉熵损失函数，Softmax回归模型的风险函数是\n",
    "$$\n",
    "\\begin{aligned}\n",
    "R(\\mathbf{W})&=-\\frac{1}{N}\\sum_{n=1}^N\\sum_{i=1}^{C}y_c^{(n)}\\log \\hat{y}_c^{(n)}\\\\\n",
    "&=-\\frac{1}{N}\\sum_{n=1}^N(\\mathbf{y^{(n)}})^T\\log \\hat{y}_c^{(n)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "其中，$\\hat{y}_c^{(n)}=\\text{softmax}(\\mathbf{W^Tx^{(n)}})$为样本$x^{(n)}$在每个类别的后验概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(X, y, W):\n",
    "    \"\"\"\n",
    "    X: N*(a+1), N个样本, 特征数量为为a, 外加1维偏置\n",
    "    y: N*C, y为N个C维的one-hot向量\n",
    "    W: (a+1)*C\n",
    "    \"\"\"\n",
    "    p_y = softmax(X, W) # N*C, N个样本分别在C个类别的后验概率\n",
    "    crossEnt = -torch.dot(y.reshape(-1), torch.log2(p_y).reshape(-1)) / y.shape[0]  # 展开成1维，点积\n",
    "    return crossEnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(10, 5)\n",
    "y = torch.zeros(10, 3)\n",
    "y[torch.arange(10), torch.randint(low=0, high=y.shape[1] - 1, size=(10,))] = 1\n",
    "W = torch.randn(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(10)\n",
    "b = torch.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(285), tensor(285))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(a * b), torch.dot(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.0633)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(X, y, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_y = softmax(X, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.0633)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.dot(y.reshape(-1), torch.log2(prob_y).reshape(-1)) / y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 风险函数$\\mathbf{R(W)}$关于$W$的梯度为\n",
    "$$\n",
    "\\frac{\\partial R(W)}{\\partial W}=-\\frac{1}{N}\\sum_{n=1}^N\\mathbf{x^{(n)}(y^{(n)}-\\hat{y}^{(n)})}^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_crosEnt_W(X, y, W):\n",
    "    '''\n",
    "    X: N*(a+1), N个样本, 特征数量为为a, 外加1维偏置\n",
    "    y: N*C, y为N个C维的one-hot向量\n",
    "    W: (a+1)*C\n",
    "    '''\n",
    "    hat_y = softmax(X, W)\n",
    "    a = (X.t() @ (y - hat_y)) / y.shape[0]  # (a+1)*N | N*C\n",
    "    return -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1000x8 and 5x3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-883cc330e715>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrad_crosEnt_W\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-ecf20796c26d>\u001b[0m in \u001b[0;36mgrad_crosEnt_W\u001b[0;34m(X, y, W)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mW\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     '''\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mhat_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mhat_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# (a+1)*N | N*C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-18590b2d509a>\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(X, W)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mW\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC为类别数量\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# hat_y, N*C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 各样本对应类别的标准化概率分布\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1000x8 and 5x3)"
     ]
    }
   ],
   "source": [
    "grad_crosEnt_W(X, y, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 学习方法\n",
    "\n",
    "- 输入: 训练集X，`one-hot`形式的标签y\n",
    "- 输出：最优参数$w^*$\n",
    "- 算法过程\n",
    "    - 初始化$W_0:=0$，最大迭代次数$T$\n",
    "    - 然后通过下式进行参数的迭代更新\n",
    "    $$\n",
    "    W_{t+1}:=W_t+\\eta\\left(\\frac{1}{N}\\sum_{n=1}^N\\mathbf{x^{(n)}(y^{(n)}-\\hat{y}^{(n)})}^T\\right) \n",
    "    $$\n",
    "    - 直到满足指定迭代次数，令$w^*=w^T$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 预测的正确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_rate(X, y, W, X_with_bias=False):\n",
    "    if X_with_bias:\n",
    "        hat_X = X\n",
    "    else:\n",
    "        hat_X = torch.cat([X, torch.ones(X.shape[0], 1)], axis=1)  # 增广X\n",
    "        \n",
    "    pred_y = hat_y(hat_X, W)\n",
    "    precision = torch.sum(pred_y[0] == torch.max(y, axis=1)[1]).numpy() / pred_y[0].numel()\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 生成模拟数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(1000, 8)\n",
    "hat_X = torch.cat([X, torch.ones(X.shape[0], 1)], axis=1)  # 增广\n",
    "true_W = torch.randn(hat_X.shape[1], 5)  # 增广\n",
    "indices_y, y = hat_y(hat_X, true_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = X[:800], y[:800]\n",
    "train_indices_y = indices_y[:800]\n",
    "hat_train_X = hat_X[:800]\n",
    "test_X, test_y = X[800:], y[800:]\n",
    "test_indices_y = indices_y[800:]\n",
    "hat_test_X = hat_X[800:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 方法1: 梯度下降-人工求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_sgd(X, y, num_steps=100, lr=0.1):\n",
    "    '''\n",
    "    X: N*(a+1), N个样本, 特征数量为为a, 外加1维偏置\n",
    "    y: N*C, y为N个C维的one-hot向量\n",
    "    W: (a+1)*C\n",
    "    '''\n",
    "    hat_X = torch.cat([X, torch.ones(X.shape[0], 1)], axis=1)  # 增广X\n",
    "    W = torch.randn(hat_X.shape[1], y.shape[1])  # 增广参数矩阵\n",
    "    for i in range(num_steps):\n",
    "        W -= lr*grad_crosEnt_W(hat_X, y, W)\n",
    "        loss = cross_entropy(hat_X, y, W)\n",
    "        if (i+1) % 50 == 0:\n",
    "            print(f'训练{i+1}轮, 交叉熵为{loss:.2f}')\n",
    "            \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练50轮, 交叉熵为2.24\n",
      "训练100轮, 交叉熵为1.44\n",
      "训练150轮, 交叉熵为1.08\n",
      "训练200轮, 交叉熵为0.87\n",
      "训练250轮, 交叉熵为0.74\n",
      "训练300轮, 交叉熵为0.66\n",
      "训练350轮, 交叉熵为0.60\n",
      "训练400轮, 交叉熵为0.56\n",
      "训练450轮, 交叉熵为0.54\n",
      "训练500轮, 交叉熵为0.51\n",
      "训练550轮, 交叉熵为0.49\n",
      "训练600轮, 交叉熵为0.48\n",
      "训练650轮, 交叉熵为0.46\n",
      "训练700轮, 交叉熵为0.45\n",
      "训练750轮, 交叉熵为0.44\n",
      "训练800轮, 交叉熵为0.43\n",
      "训练850轮, 交叉熵为0.42\n",
      "训练900轮, 交叉熵为0.41\n",
      "训练950轮, 交叉熵为0.40\n",
      "训练1000轮, 交叉熵为0.39\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.94625, 0.915)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模拟数据\n",
    "est_W = softmax_sgd(train_X, train_y, num_steps=1000)\n",
    "precision_rate(train_X, train_y, est_W, X_with_bias=False), precision_rate(test_X, test_y, est_W, X_with_bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 方法2: 随机梯度下降-自动求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_miniBatch_sgd(X, y, num_epoch=50, batch_size=40, lr=0.05):\n",
    "    '''\n",
    "    X: N*a, N个样本, 特征数量为为a\n",
    "    y: N*C, y为N个C维的one-hot向量\n",
    "    W: a*C\n",
    "    '''\n",
    "    hat_X = torch.cat([X, torch.ones(X.shape[0], 1)], axis=1)  # 增广X\n",
    "    W = torch.randn(hat_X.shape[1], y.shape[1])  # 增广参数矩阵\n",
    "    W.requires_grad_()\n",
    "    dataset = TensorDataset(hat_X, y)\n",
    "    data_iter = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "    for epoch in range(num_epoch):\n",
    "        for t_x, t_y in data_iter:\n",
    "            l = cross_entropy(t_x, t_y, W)        \n",
    "            l.backward()  # 计算损失函数在 W 上的梯度\n",
    "            W.data.sub_(lr*W.grad/batch_size)\n",
    "            W.grad.data.zero_()\n",
    "            \n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            with torch.no_grad():  # 不计算梯度，加速损失函数的运算\n",
    "                train_l = cross_entropy(hat_X, y, W)  # 最近一次的负对数似然率\n",
    "                est_W = W.detach().numpy()  # detach得到一个有着和原tensor相同数据的tensor\n",
    "                print(f'epoch {epoch + 1}, loss: {train_l:.4f}')\n",
    "            \n",
    "    return est_W, train_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, loss: 2.0593\n",
      "epoch 100, loss: 1.3058\n",
      "epoch 150, loss: 0.9535\n",
      "epoch 200, loss: 0.7821\n",
      "epoch 250, loss: 0.6903\n",
      "epoch 300, loss: 0.6345\n",
      "epoch 350, loss: 0.5967\n",
      "epoch 400, loss: 0.5684\n",
      "epoch 450, loss: 0.5459\n",
      "epoch 500, loss: 0.5271\n",
      "epoch 550, loss: 0.5110\n",
      "epoch 600, loss: 0.4968\n",
      "epoch 650, loss: 0.4841\n",
      "epoch 700, loss: 0.4727\n",
      "epoch 750, loss: 0.4623\n",
      "epoch 800, loss: 0.4527\n",
      "epoch 850, loss: 0.4439\n",
      "epoch 900, loss: 0.4357\n",
      "epoch 950, loss: 0.4281\n",
      "epoch 1000, loss: 0.4210\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.94625, 0.93)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模拟数据\n",
    "est_W, train_l = softmax_miniBatch_sgd(train_X, train_y, num_epoch=1000, batch_size=40, lr=0.1)\n",
    "precision_rate(train_X, train_y, est_W), precision_rate(test_X, test_y, est_W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 方法3: torch.nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SofmaxRegresModel(torch.nn.Module): \n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        # 首先找到LinearModel的父类torch.nn.Module，然后把类LinearModel的对象转换为类torch.nn.Module的对象, \n",
    "        # 即执行父类torch.nn.Module的初始化__init__()\n",
    "        super(SofmaxRegresModel, self).__init__() \n",
    "        self.layer1 = torch.nn.Linear(dim_in, dim_out, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = self.layer1(x)\n",
    "        return torch.nn.functional.softmax(y_pred, dim=1)  # softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义训练算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 8]), torch.Size([1000, 5]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_in = X.shape[1]\n",
    "dim_out = y.shape[1]\n",
    "# 实例化1个网络\n",
    "net = SofmaxRegresModel(dim_in, dim_out)\n",
    "# 初始化网络参数和偏置\n",
    "net.layer1.weight.data = torch.randn(dim_out, dim_in)\n",
    "net.layer1.bias.data = torch.Tensor(dim_out)\n",
    "# 损失函数|\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "# 随机梯度下降算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20, loss 1.4764677286148071\n",
      "epoch 40, loss 1.3729668855667114\n",
      "epoch 60, loss 1.3075109720230103\n",
      "epoch 80, loss 1.2599815130233765\n",
      "epoch 100, loss 1.2198939323425293\n",
      "epoch 120, loss 1.1919025182724\n",
      "epoch 140, loss 1.1810524463653564\n",
      "epoch 160, loss 1.1743334531784058\n",
      "epoch 180, loss 1.1692289113998413\n",
      "epoch 200, loss 1.165086269378662\n"
     ]
    }
   ],
   "source": [
    "trainer = torch.optim.SGD(net.parameters(), lr=0.05)\n",
    "# 加载数据\n",
    "batch_size = 20\n",
    "num_epochs = 200\n",
    "dataset = TensorDataset(train_X, train_indices_y)\n",
    "data_iter = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "# 开始训练\n",
    "for epoch in range(num_epochs):\n",
    "    for t_x, t_y in data_iter:\n",
    "        l = loss(net(t_x), t_y)  # 计算当前批量的交叉熵损失\n",
    "        trainer.zero_grad()  # 参数梯度清零\n",
    "        l.backward()  # 反向传播，计算梯度\n",
    "        trainer.step()  # 更新参数\n",
    "        \n",
    "    if (epoch+1) % 20 == 0:\n",
    "        with torch.no_grad():  # 不计算梯度，加速损失函数的运算\n",
    "            l_epoch = loss(net(train_X), train_indices_y) \n",
    "            print('epoch {}, loss {}'.format(epoch+1, l_epoch)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0.79\n",
      "test 0.79\n"
     ]
    }
   ],
   "source": [
    "w, b = net.parameters()\n",
    "W = torch.cat([w.data, b.data.reshape(-1, 1)], axis=1)\n",
    "\n",
    "pred_train_y = torch.max(net(train_X), axis=1)[1]\n",
    "pred_test_y = torch.max(net(test_X), axis=1)[1]\n",
    "\n",
    "print('train', torch.sum(pred_train_y == train_indices_y).numpy() / train_indices_y.numel())\n",
    "print('test', torch.sum(pred_test_y == test_indices_y).numpy() / test_indices_y.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 案例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "鸢尾花数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "d = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_labels, y_labels = d['feature_names'], d['target_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " array(['setosa', 'versicolor', 'virginica'], dtype='<U10'))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_labels, y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_idx = np.arange(len(d['target']))\n",
    "np.random.shuffle(rand_idx)\n",
    "t_idx = rand_idx[:100]\n",
    "v_idx = rand_idx[100:]\n",
    "x_train, y_train = torch.from_numpy(d['data'][t_idx]).type(torch.FloatTensor), torch.from_numpy(d['target'][t_idx])\n",
    "x_valid, y_valid = torch.from_numpy(d['data'][v_idx]).type(torch.FloatTensor), torch.from_numpy(d['target'][v_idx])\n",
    "onehot_y_train = torch.zeros(x_train.shape[0], 3)\n",
    "onehot_y_train[torch.arange(x_train.shape[0]), y_train] = 1\n",
    "onehot_y_valid = torch.zeros(x_valid.shape[0], 3)\n",
    "onehot_y_valid[torch.arange(x_valid.shape[0]), y_valid] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 2, 0, 2, 1, 1, 1, 0, 1, 0, 1, 1, 0, 2, 0, 2, 0, 0, 0, 2, 2, 0, 0,\n",
       "        0, 2, 2, 2, 2, 0, 1, 2, 0, 2, 1, 2, 1, 1, 0, 2, 1, 0, 0, 2, 1, 2, 2, 2,\n",
       "        2, 2, 0, 2, 1, 2, 0, 2, 0, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 2, 0, 1, 1, 1,\n",
       "        2, 2, 1, 1, 0, 2, 0, 2, 0, 0, 1, 0, 0, 0, 0, 1, 1, 2, 2, 0, 1, 2, 0, 2,\n",
       "        1, 1, 0, 1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 方法1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练50轮, 交叉熵为0.72\n",
      "训练100轮, 交叉熵为0.55\n",
      "训练150轮, 交叉熵为0.42\n",
      "训练200轮, 交叉熵为0.37\n",
      "训练250轮, 交叉熵为0.34\n",
      "训练300轮, 交叉熵为0.32\n",
      "训练350轮, 交叉熵为0.30\n",
      "训练400轮, 交叉熵为0.28\n",
      "训练450轮, 交叉熵为0.27\n",
      "训练500轮, 交叉熵为0.25\n",
      "训练550轮, 交叉熵为0.24\n",
      "训练600轮, 交叉熵为0.23\n",
      "训练650轮, 交叉熵为0.23\n",
      "训练700轮, 交叉熵为0.22\n",
      "训练750轮, 交叉熵为0.21\n",
      "训练800轮, 交叉熵为0.21\n",
      "训练850轮, 交叉熵为0.20\n",
      "训练900轮, 交叉熵为0.20\n",
      "训练950轮, 交叉熵为0.19\n",
      "训练1000轮, 交叉熵为0.19\n",
      "Train accuracy rate: 0.98\n",
      "Valid accuracy rate: 1.0\n"
     ]
    }
   ],
   "source": [
    "est_W = softmax_sgd(x_train, onehot_y_train, num_steps=1000)\n",
    "print(f\"Train accuracy rate: {precision_rate(x_train, onehot_y_train, est_W)}\")\n",
    "print(f\"Valid accuracy rate: {precision_rate(x_valid, onehot_y_valid, est_W)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 方法2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, loss: 4.2735\n",
      "epoch 100, loss: 1.6804\n",
      "epoch 150, loss: 1.2477\n",
      "epoch 200, loss: 1.0474\n",
      "epoch 250, loss: 0.9341\n",
      "epoch 300, loss: 0.8604\n",
      "epoch 350, loss: 0.8079\n",
      "epoch 400, loss: 0.7672\n",
      "epoch 450, loss: 0.7343\n",
      "epoch 500, loss: 0.7063\n",
      "epoch 550, loss: 0.6821\n",
      "epoch 600, loss: 0.6610\n",
      "epoch 650, loss: 0.6417\n",
      "epoch 700, loss: 0.6245\n",
      "epoch 750, loss: 0.6090\n",
      "epoch 800, loss: 0.5947\n",
      "epoch 850, loss: 0.5806\n",
      "epoch 900, loss: 0.5679\n",
      "epoch 950, loss: 0.5561\n",
      "epoch 1000, loss: 0.5455\n",
      "Train accuracy rate: 0.93\n",
      "Valid accuracy rate: 0.94\n"
     ]
    }
   ],
   "source": [
    "# 鸢尾花\n",
    "est_W, _ = softmax_miniBatch_sgd(x_train, onehot_y_train, num_epoch=1000, batch_size=40, lr=0.1)\n",
    "print(f\"Train accuracy rate: {precision_rate(x_train, onehot_y_train, est_W)}\")\n",
    "print(f\"Valid accuracy rate: {precision_rate(x_valid, onehot_y_valid, est_W)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 方法3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, loss 0.9097030162811279\n",
      "epoch 100, loss 0.903800904750824\n",
      "epoch 150, loss 0.9007967114448547\n",
      "epoch 200, loss 0.8989772200584412\n",
      "epoch 250, loss 0.8977570533752441\n",
      "epoch 300, loss 0.8968818187713623\n",
      "epoch 350, loss 0.896223247051239\n",
      "epoch 400, loss 0.8957096338272095\n",
      "epoch 450, loss 0.8952977657318115\n",
      "epoch 500, loss 0.8949601054191589\n",
      "epoch 550, loss 0.8946780562400818\n",
      "epoch 600, loss 0.8944389224052429\n",
      "epoch 650, loss 0.8942336440086365\n",
      "epoch 700, loss 0.8940551280975342\n",
      "epoch 750, loss 0.8938988447189331\n",
      "epoch 800, loss 0.8937607407569885\n",
      "epoch 850, loss 0.8936377167701721\n",
      "epoch 900, loss 0.8935274481773376\n",
      "epoch 950, loss 0.8934279084205627\n",
      "epoch 1000, loss 0.8933377861976624\n",
      "epoch 1050, loss 0.8932558298110962\n",
      "epoch 1100, loss 0.893180787563324\n",
      "epoch 1150, loss 0.8931118249893188\n",
      "epoch 1200, loss 0.8930484652519226\n",
      "epoch 1250, loss 0.8929897546768188\n",
      "epoch 1300, loss 0.8929353952407837\n",
      "epoch 1350, loss 0.8928847312927246\n",
      "epoch 1400, loss 0.8928375840187073\n",
      "epoch 1450, loss 0.892793595790863\n",
      "epoch 1500, loss 0.8927522897720337\n",
      "epoch 1550, loss 0.8927134871482849\n",
      "epoch 1600, loss 0.8926770091056824\n",
      "epoch 1650, loss 0.8926427364349365\n",
      "epoch 1700, loss 0.8926102519035339\n",
      "epoch 1750, loss 0.8925795555114746\n",
      "epoch 1800, loss 0.892550528049469\n",
      "epoch 1850, loss 0.8925229907035828\n",
      "epoch 1900, loss 0.8924968242645264\n",
      "epoch 1950, loss 0.8924719095230103\n",
      "epoch 2000, loss 0.8924482464790344\n"
     ]
    }
   ],
   "source": [
    "# 实例化\n",
    "dim_in = 4  # 特征数量\n",
    "dim_out = 3  # 类别数量\n",
    "net = SofmaxRegresModel(dim_in, dim_out)\n",
    "# 初始化网络参数和偏置\n",
    "net.layer1.weight.data = torch.randn(dim_out, dim_in)\n",
    "net.layer1.bias.data = torch.randn(dim_out)\n",
    "# 损失函数\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "# 随机梯度下降算法\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.04)\n",
    "# 加载数据\n",
    "batch_size = 20\n",
    "num_epochs = 2000\n",
    "dataset = TensorDataset(x_train, y_train)\n",
    "data_iter = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 训练\n",
    "for epoch in range(num_epochs):\n",
    "    for t_x, t_y in data_iter:\n",
    "        l = loss(net(t_x), t_y)  # 计算当前批量的交叉熵损失\n",
    "        trainer.zero_grad()  # 参数梯度清零\n",
    "        l.backward()  # 反向传播，计算梯度\n",
    "        trainer.step()  # 更新参数\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        with torch.no_grad():  # 不计算梯度，加速损失函数的运算\n",
    "            l_epoch = loss(net(x_train), y_train) \n",
    "            print('epoch {}, loss {}'.format(epoch+1, l_epoch)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = net.parameters()\n",
    "W = torch.cat([w.data, b.data.reshape(-1, 1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_train = torch.max(net(x_train), axis=1)[1]\n",
    "pred_y_valid = torch.max(net(x_valid), axis=1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(pred_y_train == y_train).numpy() / y_train.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(pred_y_valid == y_valid).numpy() / y_valid.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 附. 熵相关概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一条信息的信息量大小和它的不确定性有很大的关系。一句话如果需要很多外部信息才能确定，我们就称这句话的信息量比较大。比如你听到“云南西双版纳下雪了”，那你需要去看天气预报、问当地人等等查证（因为云南西双版纳从没下过雪）。相反，如果和你说“人一天要吃三顿饭”，那这条信息的信息量就很小，因为这条信息的确定性很高，我们不需要用很多信息取证明它。因此，可将事件$x_0$的信息量表示为：\n",
    "$$\n",
    "I(x_0)=-\\log p(x_0)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A1. 熵\n",
    "信息量是对于单个事件来说的，但是实际情况一件事有很多种发生的可能，比如掷骰子有可能出现6种情况，明天的天气可能晴、多云或者下雨等等。因此，我们需要评估事件对应的所有可能性。\n",
    "\n",
    "熵（entropy）是表示随机变量不确定的度量，是对表征所有可能发生的事件所需信息量的期望。\n",
    "\n",
    "设$X$是一个取有限个值的随机变量，其概率分布为\n",
    "$$\n",
    "P(X=x_i)=p_i,i=1,2,...,n\n",
    "$$\n",
    "熵定义为\n",
    "$$\n",
    "H(x)=\\sum_{i=0} p(x_i) I(x_i)=-\\sum_{i=1}^n p(x_i) \\log p(x_i)\n",
    "$$\n",
    "上式中，若$p_i=0$，则定义$0\\log 0=0$；对数以2或者e为底，这时熵的单位分别称为比特(bit)或者纳特(nat)。熵只依赖于X的分布，与其取值无关，因此也可将X的熵记作$H(p)$, 即\n",
    "$$\n",
    "H(p)=-\\sum_{i=1}^n p_i \\log p_i\n",
    "$$\n",
    "熵越大，不确定越大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(P):\n",
    "    '''\n",
    "    P为概率分布\n",
    "    '''\n",
    "    return -np.sum([p*np.log2(p) if p > 0 else 0 for p in P])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.321928094887362, -0.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P1 = np.ones(10) / 10\n",
    "P2 = np.zeros(10)\n",
    "P2[3] = 1\n",
    "entropy(P1), entropy(P2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2. 条件熵\n",
    "条件熵(conditional entropy): 表示在已知随机变量X的条件下随机变量Y的不确定性。\n",
    "\n",
    "$$\n",
    "H(Y|X)=\\sum_{i=1}^n P(X=x_i)H(Y|X=x_i)\n",
    "$$\n",
    "\n",
    "其中，$H(Y|X=x_i)=-\\sum_j P(Y=y_j|X=x_i)\\log P(Y=y_j|X=x_i)$，表示在$X=x_i$时Y的不确定程度；$p(Y=y_j|X=x_i) = \\frac{p(X=x_i, Y=y_j)}{p(X=x_i)}$。\n",
    "> 如果X与Y无关，则有$H(Y|X)=H(Y)$；如果Y由X唯一决定，则有$H(Y|X)=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_entropy(P_XY):\n",
    "    '''\n",
    "    P_XY为X和Y的联合概率分布shape(x_size, y_z)\n",
    "    '''\n",
    "    return np.sum([np.sum(P_XY[i]) * entropy(P_XY[i, :]/np.sum(P_XY[i])) \n",
    "                   for i in P_XY.shape[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A3. KL散度（相对熵）\n",
    "\n",
    "相对熵(`relative entropy`)或KL散度(`Kullback-Leibler divergence`)：度量一个概率分布$p(x)$相对另一个概率分布$q(X)$的差异\n",
    "\n",
    "$$\n",
    "\\text{KL(p||q)}=-\\sum_x p(x)\\log\\frac{q(x)}{p(x)}\n",
    "$$\n",
    "\n",
    "由`Jesen`不等式可证明，$\\text{KL(p||q)}\\geq 0$，当且仅当对于所有$x$有$p(x)=q(x)$时，取等号。\n",
    "\n",
    "此外，需注意，$\\text{KL(p||q)}\\neq \\text{KL(q||p)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL(p_x, q_x):\n",
    "    return -np.sum([p_x[i]*np.log(q_x[i]/p_x[i]) if p_x[i] > 0 and q_x[i] > 0 else 0 \n",
    "                    for i in range(len(p_x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.2302585092994046, -0.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KL(P1, P2), KL(P1, P1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A4. 交叉熵\n",
    "\n",
    "交叉熵定义如下:\n",
    "\n",
    "$$\n",
    "\\text{crossEntropy(p(x), q(x))} = -\\sum_x p(x)\\log q(x)\n",
    "$$\n",
    "\n",
    "- 与KL散度的关系\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{KL(p||q)} &= -\\sum_x p(x)\\log\\frac{q(x)}{p(x)}\\\\\n",
    "&= -\\sum_x p(x)\\log q(x) + \\sum_x p(x)\\log p(x) \\\\\n",
    "&= \\text{crossEntropy(p(x), q(x))} - H\\left(p(x)\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "即有$\\text{crossEntropy(p(x), q(x))} = \\text{KL(p||q)} + H\\left(p(x)\\right)$\n",
    "\n",
    "由于$H\\left(p(x)\\right)$为定值，针对q最小化交叉熵等价于最小化`KL(p||q)`，即使理论分布与抽样分布之间的差异最小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(p_x, q_x):\n",
    "    return -np.sum([p_x[i]*np.log(q_x[i]) if q_x[i] > 0 else 0 for i in range(len(p_x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.0, 2.3025850929940455, -0.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(P1, P2), cross_entropy(P1, P1), cross_entropy(P2, P2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考资料\n",
    "1. 李航. 统计学习方法. 2017.\n",
    "2. 邱锡鹏. 神经网络与机器学习. 2020."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
