{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor, nn\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.Tensor是一种包含单一数据类型元素的多维矩阵。Torch定义了七种CPU tensor类型和八种GPU tensor类型：\n",
    "\n",
    "|Data type|\t CPU tensor | GPU tensor|\n",
    "|:---|:---|:---|\n",
    "|32-bit floating point|\ttorch.FloatTensor|\ttorch.cuda.FloatTensor|\n",
    "|64-bit floating point|\ttorch.DoubleTensor|\ttorch.cuda.DoubleTensor|\n",
    "|16-bit floating point|\tN/A|\ttorch.cuda.HalfTensor|\n",
    "|8-bit integer (unsigned)|\ttorch.ByteTensor|\ttorch.cuda.ByteTensor|\n",
    "|8-bit integer (signed)|\ttorch.CharTensor|\ttorch.cuda.CharTensor|\n",
    "|16-bit integer (signed)|\ttorch.ShortTensor|\ttorch.cuda.ShortTensor|\n",
    "|32-bit integer (signed)|\ttorch.IntTensor|\ttorch.cuda.IntTensor|\n",
    "|64-bit integer (signed)|\ttorch.LongTensor|\ttorch.cuda.LongTensor|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.tensor`和`torch.Tensor`的区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([5, 4]).type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 4.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([5, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.LongTensor'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([5, 4]).type()  # tensor接受已经存在的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([5, 4])  # tensor接受已经存在的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.5324e-16, 4.5586e-41, 6.5324e-16, 4.5586e-41],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(5, 4)  #  Tensor创建一个多维矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0.])\n",
      "tensor([5.])\n",
      "tensor(5)\n"
     ]
    }
   ],
   "source": [
    "print(torch.Tensor(5), torch.Tensor([5]), torch.tensor(5), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 不同类别的`Tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]], dtype=torch.int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.IntTensor([[1, 2, 3], [4, 5, 6]])  # 一个张量tensor可以从Python的list或序列构建："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]], dtype=torch.int16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ShortTensor(2, 4).zero_()  # 一个空张量tensor可以通过规定其大小来构建："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ByteTensor(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ShortTensor(np.arange(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.LongTensor'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = np.random.randint(10, size=(2, 10))\n",
    "u = torch.from_numpy(u)  # 默认转为LongTensor\n",
    "u.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6, 2, 6, 6, 4, 6, 8, 1, 7, 4],\n",
       "        [2, 3, 9, 7, 6, 3, 5, 8, 2, 7]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.LongTensor'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = u.type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.requires_grad = True  # 附加梯度，反向传播时计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6., 2., 6., 6., 4., 6., 8., 1., 7., 4.],\n",
       "        [2., 3., 9., 7., 6., 3., 5., 8., 2., 7.]], requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.常用函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[30, 43, 43, 22, 29],\n",
       "        [14, 36,  5,  2, 41],\n",
       "        [ 9, 16, 26, 17, 29],\n",
       "        [31, 40,  8, 33,  1],\n",
       "        [33, 23, 17, 10, 48],\n",
       "        [ 1, 21, 26, 17,  8],\n",
       "        [34,  8, 34, 24,  8],\n",
       "        [48, 30, 37, 26, 12],\n",
       "        [ 3,  4, 10,  1, 18],\n",
       "        [41, 42, 31, 32, 23]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randint(1, 50, size=(10, 5))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape  # 各维度上的数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[30, 43, 43, 22, 29, 14, 36,  5,  2, 41],\n",
       "        [ 9, 16, 26, 17, 29, 31, 40,  8, 33,  1],\n",
       "        [33, 23, 17, 10, 48,  1, 21, 26, 17,  8],\n",
       "        [34,  8, 34, 24,  8, 48, 30, 37, 26, 12],\n",
       "        [ 3,  4, 10,  1, 18, 41, 42, 31, 32, 23]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(5, 10)  # 重塑形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[30, 43, 43, 22, 29, 14, 36,  5,  2, 41],\n",
       "        [ 9, 16, 26, 17, 29, 31, 40,  8, 33,  1],\n",
       "        [33, 23, 17, 10, 48,  1, 21, 26, 17,  8],\n",
       "        [34,  8, 34, 24,  8, 48, 30, 37, 26, 12],\n",
       "        [ 3,  4, 10,  1, 18, 41, 42, 31, 32, 23]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(5, -1)  # 如果某一维度为-1，则根据总元素个数自动计算该轴长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numel()  # 元素个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30, 43, 43, 22, 29, 14, 36,  5,  2, 41,  9, 16, 26, 17, 29, 31, 40,  8,\n",
       "        33,  1, 33, 23, 17, 10, 48,  1, 21, 26, 17,  8, 34,  8, 34, 24,  8, 48,\n",
       "        30, 37, 26, 12,  3,  4, 10,  1, 18, 41, 42, 31, 32, 23])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 5, 7, 9])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(1, 10, 2)  # torch.range()也可用，建议用前者"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000,  1.4737,  1.9474,  2.4211,  2.8947,  3.3684,  3.8421,  4.3158,\n",
       "         4.7895,  5.2632,  5.7368,  6.2105,  6.6842,  7.1579,  7.6316,  8.1053,\n",
       "         8.5789,  9.0526,  9.5263, 10.0000])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(1, 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(size=x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0572, 2.5964],\n",
       "        [1.9207, 2.1987],\n",
       "        [2.6952, 1.6901],\n",
       "        [3.6129, 1.8273],\n",
       "        [1.8932, 2.2636],\n",
       "        [1.3650, 1.8002],\n",
       "        [1.8786, 2.5397],\n",
       "        [2.0811, 2.2884],\n",
       "        [1.6557, 2.7054],\n",
       "        [1.6161, 1.9907]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(10, 2) * 0.5 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.1858]]],\n",
       "\n",
       "\n",
       "        [[[ 0.9533]]],\n",
       "\n",
       "\n",
       "        [[[-1.2399]]],\n",
       "\n",
       "\n",
       "        [[[-0.5443]]],\n",
       "\n",
       "\n",
       "        [[[ 0.7691]]],\n",
       "\n",
       "\n",
       "        [[[-0.3276]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1924]]],\n",
       "\n",
       "\n",
       "        [[[-1.8608]]],\n",
       "\n",
       "\n",
       "        [[[ 0.7461]]],\n",
       "\n",
       "\n",
       "        [[[-0.2815]]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.randn(10, 1, 1, 1)  # 10*1*1*1\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1858,  0.9533, -1.2399, -0.5443,  0.7691, -0.3276,  0.1924, -1.8608,\n",
       "         0.7461, -0.2815])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.squeeze(y)  # 10\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1858,  0.9533, -1.2399, -0.5443,  0.7691, -0.3276,  0.1924, -1.8608,\n",
       "         0.7461, -0.2815])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4],\n",
       "        [5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.arange(10).reshape(2, 5)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 5],\n",
       "        [1, 6],\n",
       "        [2, 7],\n",
       "        [3, 8],\n",
       "        [4, 9]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.t()  # 输入一个矩阵（2维张量），并转置0, 1维。 可以被视为函数transpose(input, 0, 1)的简写函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5],\n",
       "        [6, 7],\n",
       "        [8, 9]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.reshape(z.shape[1], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  1., -1., -1.,  1., -1.,  1., -1.,  1., -1.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sign(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7660, 0.7218, 0.2244, 0.3672, 0.6833, 0.4188, 0.5480, 0.1346, 0.6783,\n",
       "        0.4301])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(y)  # 返回一个新张量，包含输入input张量每个元素的sigmoid值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9972)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(y, p=2)  # 返回输入张量input 的p 范数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.2815)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.median(y)  # 返回输入张量给定维度每行的中位数，同时返回一个包含中位数的索引的LongTensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0145)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.prod(y)  # 返回输入张量input 所有元素的积。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([ 1.1858,  0.9533,  0.7691,  0.7461,  0.1924, -0.2815, -0.3276, -0.5443,\n",
       "        -1.2399, -1.8608]),\n",
       "indices=tensor([0, 1, 4, 8, 6, 9, 5, 3, 2, 7]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sort(y, dim=0, descending=True)  # 对输入张量input沿着指定维按升序排序。如果不给定dim，则默认为输入的最后一维。如果指定参数descending为True，则按降序排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0129, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.6724])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.relu(torch.randn(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 二元运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(10)\n",
    "b = torch.arange(10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,  11,  24,  39,  56,  75,  96, 119, 144, 171])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * b  # 按元素乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,  11,  24,  39,  56,  75,  96, 119, 144, 171])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(a, b)  # 按元素乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(735)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[735]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(a.reshape(1, -1), b.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(735)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(a, b)  # 內积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(735)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(4, 3)\n",
    "y = torch.rand(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4106, 0.6260, 0.6398, 0.6742],\n",
       "        [0.9696, 0.5561, 0.7245, 0.8263],\n",
       "        [0.1044, 0.0935, 0.0824, 0.0935],\n",
       "        [0.7852, 0.8291, 0.9147, 0.9885]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(x, y)  # 矩阵乘法: 对矩阵mat1和mat2进行相乘。 如果mat1 是一个n×m张量，mat2 是一个 m×p张量，将会输出一个 n×p张量out。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4106, 0.6260, 0.6398, 0.6742],\n",
       "        [0.9696, 0.5561, 0.7245, 0.8263],\n",
       "        [0.1044, 0.0935, 0.0824, 0.0935],\n",
       "        [0.7852, 0.8291, 0.9147, 0.9885]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4106, 0.6260, 0.6398, 0.6742],\n",
       "        [0.9696, 0.5561, 0.7245, 0.8263],\n",
       "        [0.1044, 0.0935, 0.0824, 0.0935],\n",
       "        [0.7852, 0.8291, 0.9147, 0.9885]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x @ y  # 矩阵乘法运算符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.rand(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5759, 0.7409, 0.1201, 0.8506])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mv(x, z)  # 矩阵向量乘： 对矩阵mat和向量vec进行相乘。 如果mat 是一个n×m张量，vec 是一个m元 1维张量，将会输出一个n元 1维张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5759, 0.7409, 0.1201, 0.8506])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x @ z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tensor(range(10), dtype=torch.float32).reshape(2, 5)\n",
    "b = tensor(range(10, 20), dtype=torch.float32).reshape(2, 5)\n",
    "c = tensor(range(20, 30), dtype=torch.float32).reshape(2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2., 3., 4.],\n",
       "         [5., 6., 7., 8., 9.]]),\n",
       " tensor([[10., 11., 12., 13., 14.],\n",
       "         [15., 16., 17., 18., 19.]]),\n",
       " tensor([[20., 21., 22., 23., 24.],\n",
       "         [25., 26., 27., 28., 29.]]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.,  4.],\n",
       "        [ 5.,  6.,  7.,  8.,  9.],\n",
       "        [10., 11., 12., 13., 14.],\n",
       "        [15., 16., 17., 18., 19.],\n",
       "        [20., 21., 22., 23., 24.],\n",
       "        [25., 26., 27., 28., 29.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_ = torch.cat([a, b, c], 0)  # 在给定维度上对输入的张量序列进行连接操作，和extend类似\n",
    "cat_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.sum(a) > 10:\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.,  4., 10., 11., 12., 13., 14., 20., 21., 22., 23.,\n",
       "         24.],\n",
       "        [ 5.,  6.,  7.,  8.,  9., 15., 16., 17., 18., 19., 25., 26., 27., 28.,\n",
       "         29.]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([a, b, c], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_ = torch.stack([a, b, c], 0)  # 沿着一个新维度对输入张量序列进行连接。 序列中所有的张量都应该为相同形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2.,  3.,  4.],\n",
       "         [ 5.,  6.,  7.,  8.,  9.]],\n",
       "\n",
       "        [[10., 11., 12., 13., 14.],\n",
       "         [15., 16., 17., 18., 19.]],\n",
       "\n",
       "        [[20., 21., 22., 23., 24.],\n",
       "         [25., 26., 27., 28., 29.]]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.,  1.],\n",
       "          [ 5.,  6.]],\n",
       " \n",
       "         [[10., 11.],\n",
       "          [15., 16.]],\n",
       " \n",
       "         [[20., 21.],\n",
       "          [25., 26.]]]),\n",
       " tensor([[[ 2.,  3.,  4.],\n",
       "          [ 7.,  8.,  9.]],\n",
       " \n",
       "         [[12., 13., 14.],\n",
       "          [17., 18., 19.]],\n",
       " \n",
       "         [[22., 23., 24.],\n",
       "          [27., 28., 29.]]]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_.split([2, 3], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.,  1.,  2.,  3.,  4.]],\n",
       " \n",
       "         [[10., 11., 12., 13., 14.]],\n",
       " \n",
       "         [[20., 21., 22., 23., 24.]]]),\n",
       " tensor([[[ 5.,  6.,  7.,  8.,  9.]],\n",
       " \n",
       "         [[15., 16., 17., 18., 19.]],\n",
       " \n",
       "         [[25., 26., 27., 28., 29.]]]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_.chunk(2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 自动求导`autograd`\n",
    "\n",
    "torch.autograd提供了类和函数用来对任意标量函数进行求导。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例:\n",
    "$$\n",
    "f(\\mathbf{x})=2\\mathbf{x}+1, g(y)=\\mathbf{y^2}+5, z=mean(\\mathbf{g(y)})\n",
    "$$\n",
    "求$\\frac{dz}{dx}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x:tensor):\n",
    "    return 2*x + 1\n",
    "\n",
    "def g(x:tensor):\n",
    "    return x**2 + 5\n",
    "\n",
    "def mean(x:tensor):\n",
    "    return torch.mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dz_dx(x:tensor):  # 实际上的导数\n",
    "    return (8*x + 4) / x.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(1, 10, size=(2, 5), dtype=torch.float32, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad\n",
    "# x.requires_grad_(True)  # 如果为False, 可以追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch自动求导结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000, 2.8000, 4.4000, 4.4000, 2.0000],\n",
       "        [6.0000, 2.0000, 6.0000, 6.0000, 6.0000]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = mean(g(f(x)))\n",
    "z.backward()  # 反向传播，自动求微分\n",
    "x.grad  # dz/dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解析求导结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000, 2.8000, 4.4000, 4.4000, 2.0000],\n",
       "        [6.0000, 2.0000, 6.0000, 6.0000, 6.0000]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dz_dx(x)  # dz/dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([a, a[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 案例: Bass模型拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_desc(lossfunc, x0, x1, max_iters=200, learn_rate=0.05):\n",
    "    \"\"\"\n",
    "    f: 待优化目标函数, grad_f: f的梯度, x0: 参数初值, x1:固定参数值, learn_rate: 学习率\n",
    "    \"\"\"\n",
    "    trace_x = x0.clone().data.reshape(1, -1)\n",
    "    x = x0\n",
    "    i = 0\n",
    "    for i in range(max_iters):\n",
    "        l = lossfunc(x, x1)\n",
    "        l.backward()\n",
    "        x.data.sub_(learn_rate * x.grad.data)\n",
    "        with torch.no_grad():\n",
    "            trace_x = torch.cat([trace_x, x.detach().data.reshape(1, -1)], 0)\n",
    "            if i % 10 == 0:\n",
    "                loss = lossfunc(x, x1).data.numpy()\n",
    "                print(f\"迭代次数: {i}, 损失函数值: {loss:.4f}\")\n",
    "                \n",
    "            if torch.sum(torch.abs(trace_x[-1] - trace_x[-2])) < 1e-3:  # 停止条件\n",
    "                break\n",
    "\n",
    "        x.grad.zero_()\n",
    "\n",
    "    print(f\"共迭代{i}次, 损失函数值: {lossfunc(x, x1).data.numpy():.4f}, 最优参数值: {x.tolist()}\")\n",
    "    return trace_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_momentum(lossfunc, x0, x1, beta1=0.5, beta2=0.5, learn_rate=0.05):\n",
    "    trace_x = x0.clone().data.reshape(1, -1)\n",
    "    x = x0\n",
    "    m_0, v_0 = 0, 0\n",
    "    i = 0\n",
    "    while True:\n",
    "        l = lossfunc(x0, x1)\n",
    "        l.backward()\n",
    "        m_1 = beta1 * m_0 + (1 - beta1) * x.grad.data\n",
    "        v_1 = beta2 * v_0 + (1 - beta2) * x.grad.data ** 2\n",
    "        x = x - learn_rate * m_1 / np.sqrt(v_1)\n",
    "        trace_x = np.concatenate([trace_x, x.reshape(1, -1)])\n",
    "        if i % 5 == 0:\n",
    "            print(f\"迭代次数: {i}, 目标函数值f: {f(x):.6f}\")\n",
    "\n",
    "        if np.sum(np.abs(trace_x[-1] - trace_x[-2])) < 1e-3:  # 停止条件\n",
    "            break\n",
    "\n",
    "        m_0, v_0 = m_1, v_1\n",
    "        i += 1\n",
    "\n",
    "    print(f\"共迭代{len(trace_x)}次, 目标函数: {f(x)}, 最优参数值: {x.tolist()}\")\n",
    "    return trace_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bass(params, T:int): # 如果要使用其它模型，可以重新定义\n",
    "    p, q, m = params\n",
    "    t_tensor = torch.arange(1, T + 1, dtype=torch.float32)\n",
    "    a = 1 - torch.exp(- (p + q) * t_tensor)\n",
    "    b = 1 + q / p * torch.exp(- (p + q) * t_tensor)\n",
    "    diffu_cont = m * a / b\n",
    "\n",
    "    adopt_cont = torch.zeros_like(diffu_cont)\n",
    "    adopt_cont[0] = diffu_cont[0]\n",
    "    for t in range(1, T):\n",
    "        adopt_cont[t] = diffu_cont[t] - diffu_cont[t - 1]\n",
    "        \n",
    "    return adopt_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanSquaredLoss(params, y):  # 平均平方误差\n",
    "    T = y.numel()\n",
    "    hat_y = bass(params, T)\n",
    "    return torch.mean((hat_y - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_2(params, y):  # R2\n",
    "    T = y.numel()\n",
    "    hat_y = bass(params, T)\n",
    "    tse = torch.sum((y - hat_y)**2)\n",
    "    ssl = torch.sum((y - torch.mean(y))**2)\n",
    "    R_2 = (ssl - tse)/ssl\n",
    "    return R_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tensor([96, 195, 238, 380, 1045, 1230, 1267, 1828, 1586, 1673, 1800, 1580, 1500], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.FloatTensor([0.001, 0.2, 20000])\n",
    "params.requires_grad_(True)\n",
    "res = grad_desc(meanSquaredLoss, params, y, learn_rate=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.FloatTensor([0.001, 0.2, 20000])\n",
    "params.requires_grad_(True)\n",
    "lr = 7e-10\n",
    "for i in range(200):\n",
    "    r = meanSquaredLoss(params, y)\n",
    "    r.backward()\n",
    "    params.data.sub_(lr * params.grad.data)\n",
    "    with torch.no_grad():\n",
    "        r2 = r_2(params, y)\n",
    "        p, q, m = params.detach().numpy()\n",
    "        if i % 10 == 0:\n",
    "#             print(params.grad.data)\n",
    "            print(f\"第{i+1}轮, r2={r2.detach().numpy():.4f}\\n    p:{p:.4f}, q:{q:.4f}, m:{m:.4f}\")\n",
    "    params.grad.data.zero_()  # 清空梯度，否则会累加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 构建网络的流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 定义计算架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, dim_feature, dim_hidden, dim_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = nn.Linear(dim_feature, dim_hidden)\n",
    "        self.output = nn.Linear(dim_hidden, dim_output)\n",
    "    \n",
    "    def forward(self, X):  # 层之间的计算次序\n",
    "        f1 = torch.relu(self.hidden(X))  # 0-1\n",
    "        f2 = self.output(f1)  # 1->2\n",
    "        f3 = nn.functional.softmax(f2, dim=1)  # 2->3\n",
    "        return f3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(dim_feature=2, dim_hidden=10, dim_output=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.05)  # 指定需优化的参数\n",
    "loss_func = nn.CrossEntropyLoss()  # 确定训练准则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 生成测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = torch.randn(100, 2) + 2  # 均值为 2\n",
    "y0 = torch.zeros(100)\n",
    "x1 = torch.randn(100, 2) - 2  # 均值为 -2\n",
    "y1 = torch.ones(100)\n",
    "\n",
    "x = torch.cat((x0, x1)).type(torch.FloatTensor)\n",
    "y = torch.cat((y0, y1)).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(len(x))\n",
    "np.random.shuffle(idx)\n",
    "train_x, train_y = x[idx[:50]], y[idx[:50]]  # 随机选取50个\n",
    "test_x, test_y = x[idx[50:]], y[idx[50:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    out = net.forward(train_x)\n",
    "    loss = loss_func(out, train_y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 20 == 0:\n",
    "        with torch.no_grad():\n",
    "            loss_train = loss_func(out, train_y)\n",
    "            out_test = net.forward(test_x)\n",
    "            loss_test = loss_func(out_test, test_y)\n",
    "            print(f\"loss_train: {loss_train}, loss_test: {loss_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = net(train_x)\n",
    "predict_train_y = torch.max(train_result, 1)[1]\n",
    "\n",
    "test_result = net(test_x)\n",
    "predict_test_y = torch.max(test_result, 1)[1]\n",
    "\n",
    "x_list = [train_x, test_x]\n",
    "y_list = [predict_train_y, predict_test_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "for i in range(2):\n",
    "    px = x_list[i]\n",
    "    py = y_list[i]\n",
    "    ax = fig.add_subplot(1, 2, i+1)\n",
    "    ax.set_xlabel('$x_0$')\n",
    "    ax.set_ylabel('$x_1$')\n",
    "    ax.scatter(px.data.numpy()[:,0], px.data.numpy()[:,1], c=py.data.numpy(), s=60, lw=0, cmap='RdYlGn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 案例： 垃圾邮件分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用`nn.Module`实现`Logit`回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogitNet(nn.Module):\n",
    "    def __init__(self, dim_feature, dim_output):\n",
    "        super(LogitNet, self).__init__()\n",
    "        self.output = nn.Linear(dim_feature, dim_output)\n",
    "    \n",
    "    def forward(self, X):  # 层之间的计算次序\n",
    "        h = self.output(X)  # 1 -> 2\n",
    "        o = nn.functional.softmax(h, dim=1)  # 2 -> 3\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dataset/smsspamcollection/SMSSpamCollection', delimiter='\\t', header=None, names=['category', 'message'])\n",
    "df['label'] = (df.category == 'ham').astype('int')\n",
    "print('垃圾邮件数量: %d ' % np.sum(df.label == 0))\n",
    "print('正常邮件数量: %d ' % np.sum(df.label == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.message.values\n",
    "y = df.label.values\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=200)\n",
    "# y转换为tensor\n",
    "y_train = torch.tensor(y_train).type(torch.LongTensor)  # 注意label的形式为1维，即类别的标签，无需reshape(-1, 1)\n",
    "y_test = torch.tensor(y_test).type(torch.LongTensor)\n",
    "# 获取词的tf-idf\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train_raw)\n",
    "X_test = vectorizer.transform(X_test_raw)\n",
    "# X转换为tensor\n",
    "X_train = torch.tensor(X_train.toarray(), dtype=torch.float)\n",
    "X_test = torch.tensor(X_test.toarray(), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 300  # 构建每批次100个样本的训练集\n",
    "dataset = TensorDataset(X_train, y_train)\n",
    "data_iter = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snet = LogitNet(dim_feature=X_train.shape[1], dim_output=2)\n",
    "optimizer = torch.optim.SGD(snet.parameters(), lr=0.03)  # 指定需优化的参数\n",
    "# loss_func = nn.CrossEntropyLoss()  # 确定训练准则\n",
    "loss_func = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    for X, y in data_iter:        \n",
    "        loss = loss_func(snet.forward(X), y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if i % 20 == 0:\n",
    "        with torch.no_grad():\n",
    "            loss_train = loss_func(snet.forward(X_train), y_train)\n",
    "            out_test = snet.forward(X_test)\n",
    "            loss_test = loss_func(out_test, y_test)\n",
    "            print(f\"loss_train: {loss_train:.5f}, loss_test: {loss_test:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = snet(X_train)\n",
    "predict_y_train = torch.max(train_result, 1)[1]\n",
    "print(torch.sum(predict_y_train != y_train), torch.sum(predict_y_train == y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.std(a, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
