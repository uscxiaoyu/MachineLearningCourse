{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三. 线性模型\n",
    "\n",
    "线性回归输出是一个连续值，因此适用于回归问题。回归问题在实际中很常见，如预测房屋价格、气温、销售额等连续值的问题。与回归问题不同，分类问题中模型的最终输出是一个离散值。我们所说的图像分类、垃圾邮件识别、疾病检测等输出为离散值的问题都属于分类问题的范畴。Logit和softmax回归则适用于分类问题。\n",
    "\n",
    "由于线性回归和softmax回归都是单层神经网络，它们涉及的概念和技术同样适用于大多数的深度学习模型。我们首先以线性回归为例，介绍大多数深度学习模型的基本要素和表示方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = ['svg']\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. 基本形式\n",
    "给定由d个属性描述的示例$\\mathbf{x}=(x_1;x_2;...;x_d)$，其中$x_i$是$x$在第i个属性上的取值，线性模型试图学得通过属性的线性组合来进行预测的函数，即\n",
    "$$f(\\mathbf{x})=\\omega_1x_1 + \\omega_2x_2+...+\\omega_dx_d+b,$$\n",
    "一般用向量形式写出\n",
    "$$f(\\mathbf{x})=\\mathbf{x\\omega}+b,$$\n",
    "其中$\\mathbf{\\omega}=(\\omega_1;\\omega_2;...;\\omega_d)$. $\\mathbf{\\omega}$和b学得后，模型就确定了."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "`Galton(1886)`发现了父亲身高与儿子身高存在着某种给定的关系：子辈的平均身高是其父辈平均身高以及他们所处族群平均身高的加权平均和。这种“回归”现象称为：均值回归或者平庸回归（`reversion to the mean/reversion to mediocrity`）。因此，哪怕单看一组父亲和孩子的身高，两个人的身高可能差异很大，但是从整个人群上来看，父亲和孩子的身高分布应该是很相近的。\n",
    "\n",
    "此外，这里所说的线性回归是指$f(x)$相对于系数$\\omega, b$为线性，变量$X_j$ 可以是\n",
    "- 数值变量\n",
    "- 数值变量的转换，如log\n",
    "- 数值变量的基扩展，例如$X_2=X_1^2,X_3=\\sin(X_1)$\n",
    "- 分组变量，用one-hot向量表示分组"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 线性回归\n",
    "给定数据集$D=\\{(\\mathbf{x_1},y_1),(\\mathbf{x_2},y_2),...,(\\mathbf{x_m},y_m)\\}$，其中$\\mathbf{x_i}=(x_{i1};x_{i2};...;x_{id}),y_i\\in R$。 线性回归试图学得一个线性模型以尽可能准确地预测实数值输出标记. 我们试图学得\n",
    "$$f(\\mathbf{x_i})=\\mathbf{x_i}\\omega+b,使得f(x_i)\\simeq y_i,$$\n",
    "这称为“多元线性回归”(multivariate linear regression).\n",
    "可以利用**最小二乘法**对$\\mathbf{\\omega}$和b进行估计。假定$\\mathbf{\\hat \\omega}=(\\mathbf{\\omega};b)$，相应地\n",
    "$$\n",
    "\\mathbf{X} =\n",
    "\\begin{pmatrix}\n",
    "    x_{11} & x_{12} & ... & x_{1d} & 1 \\\\\n",
    "    x_{21} & x_{22} & ... & x_{2d} & 1 \\\\\n",
    "    ... \\\\\n",
    "    x_{m1} & x_{m2} & ... & x_{md} & 1\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "    \\mathbf{x_1^T} & 1 \\\\\n",
    "    \\mathbf{x_2^T} & 1 \\\\\n",
    "    ... \\\\\n",
    "    \\mathbf{x_m^T} & 1\n",
    "\\end{pmatrix}, \\mathbf{y}=\n",
    "\\begin{pmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "... \\\\\n",
    "y_m\n",
    "\\end{pmatrix}=(y_1;y_2;...;y_m)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearModel(X: torch.tensor, w: torch.tensor)-> torch.tensor:\n",
    "    \"\"\"\n",
    "    X: N*a, N为样本数量，a为（增广）特征维度\n",
    "    w: a*1\n",
    "    \"\"\"\n",
    "    return X@w.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_data(w, num_examples):\n",
    "    \"\"\"Generate y = Xw + noise.\"\"\"\n",
    "    X = torch.normal(0, 1, (num_examples, len(w) - 1))\n",
    "    hat_X = torch.cat([X, torch.ones(num_examples, 1)], 1)\n",
    "    y = linearModel(hat_X, w)\n",
    "    y += torch.normal(0, 0.01, y.shape)\n",
    "    return hat_X, y.reshape((-1, 1))\n",
    "\n",
    "true_w = torch.tensor([2, -3.4, 1])  # 增广参数（包含截距项）\n",
    "features, labels = synthetic_data(true_w, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何确定$\\mathbf{\\hat\\omega}$呢？关键在于衡量$f(\\mathbf{x})$和$y$的差别。均方误是回归任务中最常用的性能衡量指标，因此我们可以试图让均方误差最小化.\n",
    "$$\n",
    "\\mathbf{\\hat\\omega^*}=\\operatorname*{argmin}_{\\mathbf{\\hat\\omega}}\\mathbf{\\frac{1}{2m}(y-X\\hat\\omega)^T(y-X\\hat\\omega)}.\n",
    "$$\n",
    "其中m为样本数量。令$E_{\\mathbf{\\hat\\omega}}=\\mathbf{\\frac{1}{2m}(y-X\\hat\\omega)^T(y-X\\hat\\omega)}$，对$\\mathbf{\\hat\\omega}$求导可得\n",
    "$$\n",
    "\\cfrac{\\partial E_{\\hat{w}}}{\\partial \\hat{w}}=\\frac{1}{m}\\mathbf{X}^T(\\mathbf{X}\\hat{w}-\\mathbf{y}).\n",
    "$$\n",
    "令上式为0可得$\\mathbf{\\hat\\omega}$最优解的封闭解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**最大似然估计角度**\n",
    ">\n",
    "> 假定残差服从正态分布，$\\epsilon\\sim N(0,\\sigma^2)$，即有$(y-xw^T)\\sim N(0,\\sigma^2)$。\n",
    ">\n",
    "> 根据最大似然估计原则，建立优化目标\n",
    "> $$\n",
    "\\max \\prod_{i=0}^N \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left({-\\frac{(y_i-x_i\\hat{w}^T)^2}{2\\sigma^2}}\\right)\n",
    "> $$\n",
    "> 化简后可\n",
    "> $$\n",
    "\\max \\frac{1}{(\\sqrt{2\\pi}\\sigma)^N}\\exp{\\left(-\\frac{\\sum_{i=1}^N(y_i-x_i\\hat{w}^T)^2}{2\\sigma^2}\\right)}\n",
    "> $$\n",
    "\n",
    "> 等价于最小二乘优化目标\n",
    ">$$\n",
    "\\min \\mathbf{(y-X\\hat{w})(y-X\\hat{w})^T}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanSquaredLoss(X: torch.tensor, y: torch.tensor, w: torch.tensor) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    X: N*a, N为样本数量，a为（增广）特征维度\n",
    "    y: N, 标签\n",
    "    w: a*1\n",
    "    \"\"\"\n",
    "    hat_y = X @ w.reshape(-1, 1)\n",
    "    sLoss = torch.dot(hat_y.reshape(-1) - y.reshape(-1), hat_y.reshape(-1) - y.reshape(-1))\n",
    "    return sLoss / y.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.2395)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.tensor([2, -3.4, 1])\n",
    "meanSquaredLoss(features, labels, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**正则化：在实际回归任务中，往往会遇到大量特征的数据集。为了防止过拟合，以上损失函数也可以加入对参数的惩罚，常用的有L1和L2惩罚。**\n",
    "- `L1: Lasso regression`\n",
    "$$\n",
    "\\min \\frac{1}{2}(y-X\\hat{w})(y-X\\hat{w})^T+\\frac{1}{2}\\lambda \\sum_{i=1}^d|w_i|\n",
    "$$\n",
    "\n",
    "等价于\n",
    "\n",
    "$$\n",
    "\\min \\frac{1}{2}(y-X\\hat{w})(y-X\\hat{w})^T \\\\\n",
    "\\text{s.t.  } \\sum_{i=1}^d|w_i|\\leq t \n",
    "$$\n",
    "\n",
    "- `L2: Ridge regression`\n",
    "$$\n",
    "\\min \\frac{1}{2}(y-X\\hat{w})(y-X\\hat{w})^T+\\frac{1}{2}\\lambda ww^T\n",
    "$$\n",
    "\n",
    "等价于\n",
    "\n",
    "$$\n",
    "\\min \\frac{1}{2}(y-X\\hat{w})(y-X\\hat{w})^T \\\\\n",
    "\\text{s.t.  } ww^T \\leq t \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanSquaredLoss_Penalty(X, y, w, delta=0.5, l=2):\n",
    "    \"\"\"\n",
    "    X: N*a, N为样本数量，a为（增广）特征维度\n",
    "    y: N, 标签\n",
    "    w: a*1|\n",
    "    delta: 惩罚系数, 超参数\n",
    "    l: 取0, 1, 2, 分别表示不惩罚，l1惩罚和l2惩罚\n",
    "    \"\"\"\n",
    "    hat_y = X @ w.reshape(-1, 1)\n",
    "    sLoss = torch.dot(hat_y.reshape(-1) - y.reshape(-1), hat_y.reshape(-1) - y.reshape(-1))\n",
    "    if l == 0:\n",
    "        penalty = 0\n",
    "    elif l == 1:\n",
    "        penalty = torch.sum(torch.abs(w.reshape(-1)[:-1]))\n",
    "    else:\n",
    "        penalty = torch.sqrt(torch.dot(w.reshape(-1)[:-1], w.reshape(-1)[:-1]))\n",
    "    return (sLoss + delta*penalty) / y.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.2435)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meanSquaredLoss_Penalty(features, labels, w, delta=1, l=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当$\\mathbf{X^t X}$为满秩矩阵或正定矩阵时，令$\\cfrac{\\partial E_{\\hat{w}}}{\\partial \\hat{w}}=0$可得\n",
    "$$\n",
    "\\mathbf{\\hat\\omega^*=(X^TX)^{-1}X^Ty},\n",
    "$$\n",
    "其中$\\mathbf{(X^TX)^{-1}}$是$(X^TX)$的逆矩阵. 令$\\mathbf{\\hat x_i} = (\\mathbf{x_i}; 1)$，则最终学得的线性回归模型为\n",
    "$$\n",
    "f(\\mathrm{\\hat x_i})=\\mathbf{\\hat x_i^T(X^TX)^{-1}X^Ty}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然而，现实任务中$\\mathbf{X^t X}$往往不是满秩矩阵，而且随着数据量的增加，计算量呈现大幅增长。因此，往往求助于数值优化算法迭代求解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_mse_w(X, y, w):\n",
    "    '''\n",
    "    X: N*a, N为样本数量，a为（增广）特征维度\n",
    "    y: N\n",
    "    w: a*1\n",
    "    '''\n",
    "    return 2*(X.t()@(X@w.reshape(-1, 1) - y.reshape(-1, 1))) / y.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_mse_w(features, labels, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 评价指标\n",
    "    - 回归平方和：$\\mathrm{SS_{res}}=\\sum_{i=1}^{n}(y_i - f(x_i))^2$\n",
    "    - 总平方和：$\\mathrm{SS_{tot}=\\sum_{i=1}^n(y_i-\\bar{y})^2}$\n",
    "    - 决定系数：$R^2 = \\mathrm{\\frac{SS_{tot}-SS_{res}}{SS_{tot}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 利用`sciki-learn`实现线性回归模型\n",
    "可以使用`scikit-learn`的`LinearRegression`模块实现线性回归模型，其中有两个关键属性，`fit_intercept`指定是否保留截距，`normalize`指定是否使用标准化数据进行模型拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 根据披萨的尺寸预测价格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[6], [8], [10], [14], [18]]) # X为训练数据的特征，即直径\n",
    "y = np.array([7, 9, 13, 17.5, 18])  # 披萨价格\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_title(\"Pizza diameter v.s. price\")\n",
    "ax.set_xlabel(\"Diameter in inches\")\n",
    "ax.set_ylabel(\"Price in dollars\")\n",
    "ax.scatter(X, y, s=30, alpha=0.5, c=\"red\")\n",
    "ax.axis([0, 25, 0, 25])\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=True, normalize=True)  # 创建一个scikit-learn估计器实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)  # 利用数据拟合模型\n",
    "model.get_params()  # 模型设置的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"拟合参数估计值: {model.coef_}\\n截距项值:{model.intercept_}\")\n",
    "r2 = model.score(X, y)  # 模型的r2\n",
    "print(f\"R2: {r2: .4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 预测值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pizza = np.array([[12], [18], [20], [21]])\n",
    "predicted_price = model.predict(test_pizza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_title(\"Pizza diameter v.s. price\")\n",
    "ax.set_xlabel(\"Diameter in inches\")\n",
    "ax.set_ylabel(\"Price in dollars\")\n",
    "ax.plot(np.linspace(0, 25, num=100), model.predict(np.linspace(0, 25, num=100).reshape(-1, 1)), \n",
    "        'r-', alpha=0.5)\n",
    "ax.scatter(X, y, s=50, alpha=0.5, c=\"grey\", label=\"actual\")\n",
    "ax.scatter(test_pizza, predicted_price, s=50, alpha=0.5, c=\"blue\", label=\"predicted\")\n",
    "ax.axis([0, 25, 0, 25])\n",
    "ax.text(15, 15, '$y=0.9763x+1.9655$', color='red')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.2)\n",
    "plt.savefig(\"../pictures/4.1.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = np.mean((model.predict(X) - y)**2)\n",
    "print(f\"Residual of MSE: {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 基于梯度下降实现多元线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 生成模拟数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_data(w, N, bias=True):\n",
    "    '''\n",
    "    y = Xw + noise\n",
    "    w: a*1, (增广)权重\n",
    "    N: 样本数量\n",
    "    bias: w中是否包含偏置\n",
    "    '''\n",
    "    if bias:\n",
    "        X = torch.cat([torch.randn(N, w.numel()-1), torch.ones(N, 1)], axis=1)\n",
    "    else:\n",
    "        X = torch.randn(N, w.numel())\n",
    "\n",
    "    y = (X @ w.reshape(-1, 1)).reshape(-1, 1)  # torch.matmul(X, w).reshape(-1, 1)\n",
    "    y += torch.randn(size=y.size()) * 0.01\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w = torch.Tensor([2, -3.4, 4.2])  # 特征权重为2, -3.4, 偏置权重为4.2\n",
    "features, labels = synthetic_data(true_w, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "# y随第1个特征x_0的散点图\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.set_xlabel(\"$x_0$\")\n",
    "ax1.set_ylabel(\"y\")\n",
    "ax1.scatter(features[:, 0].numpy(), labels.numpy(), s=1)\n",
    "# y随第2个特征x_1的散点图\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.set_xlabel(\"$x_1$\")\n",
    "ax2.set_ylabel(\"y\")\n",
    "ax2.scatter(features[:, 1].numpy(), labels.numpy(), s=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanSquaredLoss(features, labels, true_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**线性回归的梯度下降参数学习算法**\n",
    "- 输入: 特征`X`, 标签`y`, 样本数量`m`, 训练回合数`epochs`, 学习率$\\eta$\n",
    "- 输出: 最优参数$w^*$，训练模型$f(x, w^*)$\n",
    "- 算法过程:\n",
    "    - 初始化参数值w, 当前回合数`epoch=1`\n",
    "    - 如果`epoch > n`, 转至下一步, 否则执行以下循环\n",
    "        - 更新参数值$w := w - \\frac{\\eta}{m}\\mathbf{X}^T(\\mathbf{X}\\hat{w}-\\mathbf{y})$\n",
    "        - 更新训练次数`epoch := epoch + 1`\n",
    "    - 令$w^*=w, f(x, w^*)=x^Tw^*$, 算法结束"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_grad_desc(features, labels, epochs=100, learn_rate=0.05):\n",
    "    w = torch.randn(features.shape[1], 1)\n",
    "    for epoch in range(epochs):\n",
    "        w -= learn_rate * grad_mse_w(features, labels, w)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            mse = meanSquaredLoss(features, labels, w)\n",
    "            print(f\"{epoch + 1}: {mse:.4f}\")\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = lm_grad_desc(features, labels)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 基于`torch`自动求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_gd_auto(features, labels, epochs=100, learn_rate=0.05):\n",
    "    w = torch.randn(features.shape[1], 1)\n",
    "    w.requires_grad_(True)\n",
    "    # w = torch.randn(size=(num_inputs, 1), requires_grad=True)\n",
    "    for epoch in range(epochs):\n",
    "        loss = meanSquaredLoss(features, labels, w)\n",
    "        # 计算损失函数在 w 上的梯度\n",
    "        loss.backward()\n",
    "        # 原地操作，结果等价于 param -= lr * param.grad\n",
    "        w.data.sub_(learn_rate * w.grad.data)\n",
    "        # 需将梯度重置为0，否则会被累加，影响迭代结果\n",
    "        w.grad.data.zero_() \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            with torch.no_grad():  # 不计算梯度，加速损失函数的运算\n",
    "                # 最近一次的损失函数值\n",
    "                train_l = meanSquaredLoss(features, labels, w)  \n",
    "                # detach得到一个有着和原tensor相同数据的tensor\n",
    "                est_w = w.detach().data.reshape(-1).numpy()\n",
    "                print(f'epoch {epoch + 1}, mean of squared loss: {train_l.numpy():.4f}')\n",
    "\n",
    "    return w.detach().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_gd_auto(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 基于小批量随机梯度下降实现多元线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在求数值解的优化算法中，小批量随机梯度下降(`mini-batch stochastic gradient descent`)在深度学习中被广泛使用。\n",
    "\n",
    "**线性回归的小批量随机梯度下降学习方法**\n",
    "- 输入: 特征`X`, 标签`y`, 批次样本量`batch_size`, 训练回合总数`epochs`, 学习率$\\eta$\n",
    "- 输出: 最优参数$w^*$，训练模型$f(x, w^*)$\n",
    "- 算法过程:\n",
    "    - 初始化模型参数值`w`，如随机选取，初始化当前回合数`epoch=1`;\n",
    "    - 构建小样本生成器: 随机打乱样本次序，然后依次取出`batch_size`的样本作为用于一次参数更新的样本\n",
    "    - 如果`epoch > epochs`，则跳至下一步，否则进行以下循环\n",
    "        - 由小样本生成器依次取出`batch_size`的小样本$\\{X_s, y_s\\}$，按照$w:=w-\\frac{1}{\\mathrm{batch\\_size}}\\eta \\mathbf{X_s}^T(\\mathbf{X_s}\\hat{w}-\\mathbf{y_s})$，直到取完，\n",
    "        - 则更新`epoch:= epoch+1`；\n",
    "    - 令$w^*=w, f(x, w^*)=x^Tw^*$, 算法结束"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 每一次小批量更新为一次迭代，所有训练集的样本更新一遍为一个回合\n",
    "> - **批量梯度下降法**和**随机梯度下降**可以看作是**小批量随机梯度下降法**的特殊形式，批量梯度下降法使用所有的样本更新参数，随机梯度下降使用1个样本更新参数，小批量随机梯度下降法选择1个小样本更新参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机选取batch_size数量的数据，生成器\n",
    "def sampleData(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    随机打乱样本次序，然后依次取出batch_size的(features, labels)作为训练集\n",
    "    \"\"\"\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # 随机打乱索引次序\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        j = torch.tensor(indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features[j], labels[j]  # 没调用一次函数，生成一个批次样本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 方法1: 自己写导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_mb_grad_desc(features, labels, batch_size=20, epochs=100, learn_rate=0.05):\n",
    "    w = torch.randn(features.shape[1], 1)\n",
    "    for epoch in range(epochs):\n",
    "        for X, y in sampleData(batch_size, features, labels):\n",
    "            w -= learn_rate * grad_mse_w(X, y, w)\n",
    "\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            mse = meanSquaredLoss(features, labels, w)\n",
    "            print(f\"{epoch + 1}: {mse:.4f}\")\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = lm_mb_grad_desc(features, labels, epochs=10)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 当`batch_size=1`时，即为随机梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化参数\n",
    "num_inputs= 3\n",
    "w = torch.randn(num_inputs, 1) * 0.1\n",
    "lr = 0.05  # Learning rate\n",
    "num_epochs = 10  # 一共训练次数\n",
    "batch_size = 1  # 一批包含的样本数量\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in sampleData(batch_size, features, labels):\n",
    "        w -= lr * grad_mse_w(X, y, w) / batch_size\n",
    "    \n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        mse = meanSquaredLoss(features, labels, w)\n",
    "        print(f\"{epoch + 1}: {mse:.4f}\")\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 方法2: 利用`torch`的`auto_grad`求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_mbgd_auto(features, labels, batch_size=20, epochs=10, learn_rate=0.05):\n",
    "    w = torch.randn(features.shape[1], 1)\n",
    "    w.requires_grad_(True)\n",
    "    # w = torch.randn(size=(num_inputs, 1), requires_grad=True)\n",
    "    for epoch in range(epochs):\n",
    "        for X, y in sampleData(batch_size, features, labels):\n",
    "            loss = meanSquaredLoss(X, y, w)\n",
    "            # 计算损失函数在 w 上的梯度\n",
    "            loss.backward()\n",
    "            # 原地操作，结果等价于 param -= lr * param.grad\n",
    "            w.data.sub_(learn_rate * w.grad.data)\n",
    "            # 需将梯度重置为0，否则会被累加，影响迭代结果\n",
    "            w.grad.data.zero_() \n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            with torch.no_grad():  # 不计算梯度，加速损失函数的运算\n",
    "                # 最近一次的损失函数值\n",
    "                train_l = meanSquaredLoss(features, labels, w)  \n",
    "                # detach得到一个有着和原tensor相同数据的tensor\n",
    "                est_w = w.detach().data.reshape(-1).numpy()\n",
    "                print(f'epoch {epoch + 1}, mean of squared loss: {train_l.numpy():.4f}')\n",
    "\n",
    "    return w.detach().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_mbgd_auto(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 利用`torch.nn.Module`实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用`TensorDataset`和`DataLoader`构建小批量数据加载器， 等价于`sampleData`过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10  # 构建10个批次的训练集\n",
    "p_features = features[:, :-1]  # 去掉截距项\n",
    "dataset = TensorDataset(p_features, labels)\n",
    "data_iter = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 创建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class LinearModel(torch.nn.Module):\n",
    "    def __init__(self, ndim):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.ndim = ndim\n",
    "        self.w = torch.nn.Parameter(torch.randn(self.ndim, 1))  # params\n",
    "        self.b = torch.nn.Parameter(torch.tensor(0, dtype=torch.float16))  # bias\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X@self.w + self.b\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(torch.nn.Module): \n",
    "    def __init__(self, num_input, num_output):\n",
    "        # 首先找到LinearModel的父类torch.nn.Module，然后把类LinearModel的对象转换为类torch.nn.Module的对象, \n",
    "        # 即执行父类torch.nn.Module的初始化__init__()\n",
    "        super(LinearRegressionModel, self).__init__() \n",
    "        self.linear_layer1 = torch.nn.Linear(num_input, num_output, bias=True)  # 线性层映射\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear_layer1(x)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化实例，2维特征，1维输出\n",
    "num_input, num_output = 2, 1\n",
    "net = LinearRegressionModel(num_input, num_output)\n",
    "# 初始化线性层的权重和偏差\n",
    "net.linear_layer1.weight.data = torch.randn(num_output, num_input)\n",
    "net.linear_layer1.bias.data = torch.randn(1)\n",
    "# 定义损失函数\n",
    "# reduction指定了应用于output的方法：'none' | 'mean' | 'sum'\n",
    "loss = torch.nn.MSELoss(reduction = \"sum\") \n",
    "# 定义训练方法\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.005)  # 随机梯度下降算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs): \n",
    "    for X, y in data_iter:\n",
    "        l = loss(net.forward(X), y)  # 计算损失函数值\n",
    "        trainer.zero_grad()  # 参数梯度归0\n",
    "        l.backward()  # 计算参数梯度\n",
    "        trainer.step()  # 执行一步随机梯度下降算法\n",
    "    \n",
    "    with torch.no_grad():  # 不计算梯度，加速损失函数的运算\n",
    "        l_epoch = loss(net(p_features), labels) \n",
    "        print('epoch {}, loss {}'.format(epoch+1, l_epoch)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = list(net.parameters())[0][0]\n",
    "print('estimates of w:', w.detach().data)\n",
    "b = list(net.parameters())[1][0]\n",
    "print('estimate of b:', b.detach().data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 练习\n",
    "1. 请基于`torch`实现多项式回归。\n",
    "$$\n",
    "y=\\alpha + \\sum_{i=1}^pX^iw_i\n",
    "$$\n",
    "\n",
    "2. 请基于`torch`实现`Ridge regression`的参数学习。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_gd_auto(features, labels, epochs=100, learn_rate=0.05):\n",
    "    '''\n",
    "    自动求导求极值\n",
    "    '''\n",
    "    w = torch.randn(size=(features.shape[1], 1), requires_grad=True)\n",
    "    for epoch in range(epochs):\n",
    "        loss = meanSquaredLoss(features, labels, w)\n",
    "        loss.backward()\n",
    "        w.data.sub_(learn_rate * w.grad.data)\n",
    "        w.grad.data.zero_() \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            with torch.no_grad():  \n",
    "                train_l = meanSquaredLoss(features, labels, w)  \n",
    "                est_w = w.detach().data.reshape(-1).numpy()\n",
    "                print(f'epoch {epoch + 1}, mean of squared loss: {train_l.numpy():.4f}')\n",
    "\n",
    "    return w.detach().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyRegres(features, labels, p=2, bias=True):\n",
    "    '''\n",
    "    多项式回归\n",
    "    features: 特征\n",
    "    labels: 标签\n",
    "    bias: 是否包含偏置\n",
    "    '''\n",
    "    c_features = torch.cat([features**i for i in range(1, p+1)], axis=1)\n",
    "    if bias:\n",
    "        c_features = torch.cat([c_features, torch.ones(features.shape[0], 1)], axis=1)\n",
    "        \n",
    "    w = lm_gd_auto(c_features, labels)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyfunc(features, w, p=2, bias=True):\n",
    "    '''\n",
    "    features: 特征\n",
    "    w: 学习到的权重\n",
    "    bias: 是否包含偏置\n",
    "    '''\n",
    "    c_features = torch.cat([features**i for i in range(1, p+1)], axis=1)\n",
    "    if bias:\n",
    "        c_features = torch.cat([c_features, torch.ones(features.shape[0], 1)], axis=1)\n",
    "    \n",
    "    return c_features @ w.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, mean of squared loss: 1.2054\n",
      "epoch 20, mean of squared loss: 0.4470\n",
      "epoch 30, mean of squared loss: 0.2091\n",
      "epoch 40, mean of squared loss: 0.1029\n",
      "epoch 50, mean of squared loss: 0.0511\n",
      "epoch 60, mean of squared loss: 0.0255\n",
      "epoch 70, mean of squared loss: 0.0127\n",
      "epoch 80, mean of squared loss: 0.0064\n",
      "epoch 90, mean of squared loss: 0.0032\n",
      "epoch 100, mean of squared loss: 0.0017\n"
     ]
    }
   ],
   "source": [
    "l_w = polyRegres(features, labels, p=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9709e+00],\n",
       "        [-3.3409e+00],\n",
       "        [ 2.1693e+00],\n",
       "        [ 2.7844e-04],\n",
       "        [ 1.9104e-03],\n",
       "        [ 7.9475e-01],\n",
       "        [ 5.8731e-03],\n",
       "        [-1.3024e-02],\n",
       "        [ 3.2068e-01],\n",
       "        [ 9.1309e-01]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_features = features[:10]\n",
    "t_labels = labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0091],\n",
       "        [ 0.0085],\n",
       "        [-0.0079],\n",
       "        [-0.0038],\n",
       "        [ 0.0086],\n",
       "        [-0.0023],\n",
       "        [ 0.0074],\n",
       "        [ 0.0070],\n",
       "        [-0.0046],\n",
       "        [-0.0049]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polyfunc(t_features, l_w, bias=True) - t_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `scikit-learn`实现单变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 0.3 * np.arange(50).reshape(-1, 1)\n",
    "y = np.sin(X) + np.random.randn(len(X), 1).reshape(-1, 1) * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyRegres(X, y, p=2):\n",
    "    c_X = np.concatenate([X**i for i in range(1, p+1)], axis=1)\n",
    "    plm = LinearRegression(fit_intercept=True, normalize=True)  # 创建一个估计器实例\n",
    "    plm.fit(c_X, y)\n",
    "    return plm, c_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plm, c_X = polyRegres(X, y, p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plm.get_params()  # 模型设置的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"拟合参数估计值: {plm.coef_}\\n截距项值:{plm.intercept_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = plm.score(c_X, y)  # 模型的r2\n",
    "print(f\"R2: {r2: .4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "for i in range(1, 10):\n",
    "    p = i\n",
    "    plm, c_X = polyRegres(X, y, p=p)\n",
    "    ax = fig.add_subplot(3, 3, i)\n",
    "    if i >= 7:\n",
    "        ax.set_xlabel('$x$', size=12)\n",
    "    if i == 1 or i == 4 or i == 7:\n",
    "        ax.set_ylabel('$y$', size=12)\n",
    "        \n",
    "    ax.set_title(f'p={p}, r2={plm.score(c_X, y):.4f}', size=9)\n",
    "    ax.scatter(X, y, c='blue', s=10, alpha=0.5)\n",
    "    x_array = np.linspace(np.min(X), np.max(X), num=100).reshape(-1, 1)\n",
    "    c_x_array = np.concatenate([x_array**j for j in range(1, p+1)], axis=1)\n",
    "    ax.plot(x_array, plm.predict(c_x_array), 'r-', lw=1.5, alpha=0.8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pizza = np.array([[12], [18], [20], [21]])\n",
    "test_pizza = np.concatenate([test_pizza**i for i in range(1, p+1)], axis=1)\n",
    "predicted_price = plm.predict(test_pizza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考资料\n",
    "1. 周志华. 机器学习. 2019.\n",
    "2. 邱锡鹏. 神经网络与机器学习. 2020.\n",
    "3. [阿斯顿·张、李沐、扎卡里 C. 立顿、亚历山大 J. 斯莫拉等. 动手学深度学习. 2020.](https://github.com/d2l-ai/d2l-zh)\n",
    "4. Bishop C. M.. Pattern recognition and machine learning. 2006.\n",
    "5. Hastie T., Tibshirani R.. Friedman J. The elements of statistical learning (second edition), 2009."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 附录\n",
    "\n",
    "### [优化方法](./A2.Optimization_methods.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 变量子集选择\n",
    "\n",
    "变量选择的原因在于：\n",
    "\n",
    "- 预测精度：最小而成估计通常有低偏差但有高方差\n",
    "- 可解释性：为了得到更大的轮廓，需要牺牲掉部分细节\n",
    "\n",
    "有以下方法：\n",
    "- forward-stepwise selection\n",
    "- backward-stepwise selection\n",
    "- forward-stagewise regression\n",
    "- best-subset selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 收缩方法\n",
    "\n",
    "变量选择过程是离散的，通常效果并不好，以下回归方式是连续的，而且对变量的系数施加了限制。\n",
    "\n",
    "- ridge regression\n",
    "- lasso regression\n",
    "- least angle regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
