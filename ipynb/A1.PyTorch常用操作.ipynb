{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = ['svg']\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import tensor, nn\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.1+cu121'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 数据结构: 张量`Tensor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.Tensor`是一种包含单一数据类型元素的多维矩阵。`pytorch`包含七种`CPU tensor`类型和八种`GPU tensor`类型：\n",
    "\n",
    "|Data type|\t CPU tensor | GPU tensor|\n",
    "|:---|:---|:---|\n",
    "|32-bit floating point|\ttorch.FloatTensor|\ttorch.cuda.FloatTensor|\n",
    "|64-bit floating point|\ttorch.DoubleTensor|\ttorch.cuda.DoubleTensor|\n",
    "|16-bit floating point|\tN/A|\ttorch.cuda.HalfTensor|\n",
    "|8-bit integer (unsigned)|\ttorch.ByteTensor|\ttorch.cuda.ByteTensor|\n",
    "|8-bit integer (signed)|\ttorch.CharTensor|\ttorch.cuda.CharTensor|\n",
    "|16-bit integer (signed)|\ttorch.ShortTensor|\ttorch.cuda.ShortTensor|\n",
    "|32-bit integer (signed)|\ttorch.IntTensor|\ttorch.cuda.IntTensor|\n",
    "|64-bit integer (signed)|\ttorch.LongTensor|\ttorch.cuda.LongTensor|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones_like(torch.arange(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(10) @ torch.arange(10)  # 内积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ones_like(np.arange(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.tensor`和`torch.Tensor`的区别\n",
    "\n",
    "在概念上，**torch.tensor** 和 **torch.Tensor** 函数之间有以下区别：\n",
    "\n",
    "\n",
    "编号 | 概念区别 | torch.tensor | torch.Tensor\n",
    "---|---:|---:|---:\n",
    "1 | 抽象与具体 | 具体函数 | 抽象张量类型\n",
    "2 | 构造与转换 | 构造新张量 | 转换现有数据\n",
    "3 | 通用与特定 | 通用函数 | 特定张量类型\n",
    "4 | 创建与引用 | 总是创建新张量 | 创建新张量或引用现有张量\n",
    "5 | 灵活与控制 | 简单方法 | 细粒度控制\n",
    "\n",
    "**总结**\n",
    "\n",
    "* **torch.tensor** 函数是创建张量的更简单方法。\n",
    "* **torch.Tensor** 提供了更细粒度的控制，可以用于创建具有特定属性和方法的张量。\n",
    "\n",
    "选择使用哪个函数取决于具体需求，以下是一些具体的例子：\n",
    "\n",
    "* 如果您需要创建一个具有特定数据类型的简单张量，请使用 **torch.tensor** 函数。\n",
    "* 如果您需要创建一个具有特定属性和方法的复杂张量，请使用 **torch.Tensor** 类。\n",
    "* 如果您需要将现有数据转换为张量，请使用 **torch.Tensor** 类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor([5, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor([5, 4]).type()  # 转换为默认的FLoatTensor类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([5, 4])  # tensor接受已经存在的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([5, 4]).type()  # tensor接受已经存在的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor(5, 4)  #  Tensor创建一个多维矩阵，注意和torch.Tensor([5, 4])之间的区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(torch.Tensor(5), torch.Tensor([5]), torch.tensor(5), sep='\\n')  # 三者之间的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 不同类别的`torch.Tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.IntTensor(2, 4), torch.IntTensor([2, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.IntTensor([[1, 2, 3], [4, 5, 6]])  # 一个张量tensor可以从Python的list或序列构建："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ShortTensor(2, 4).zero_()  # 一个空张量tensor可以通过规定其大小来构建："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ByteTensor(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ShortTensor(np.arange(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.from_numpy()`有numpy.ndarray对象创建Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.random.randint(10, size=(2, 10))\n",
    "u = torch.from_numpy(u)  # 默认转为LongTensor\n",
    "u.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.requires_grad  # 是否计算梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.type()  # tensor的类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = u.type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.requires_grad = True  # 附加梯度，反向传播时计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. `PyTorch`常用函数\n",
    "\n",
    "PyTorch 提供了丰富的函数来创建、操作和管理张量。以下是一些常用的函数，按类别分类：\n",
    "\n",
    "**1.1. 创建张量**\n",
    "\n",
    "* `torch.tensor(data, dtype=None, device=None)`：根据输入数据创建张量。\n",
    "* `torch.zeros(shape, dtype=None, device=None)`：创建指定形状和数据类型的零张量。\n",
    "* `torch.ones(shape, dtype=None, device=None)`：创建指定形状和数据类型的全一(one)张量。\n",
    "* `torch.eye(n, m=None, dtype=None, device=None)`：创建单位矩阵。\n",
    "* `torch.arange(start, end, step=1, dtype=None, device=None)`：创建等差数列张量。\n",
    "* `torch.linspace(start, end, steps=100, dtype=None, device=None)`：创建线性空间张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个包含三个元素的张量\n",
    "x = torch.tensor([1, 2, 3])\n",
    "print(x)\n",
    "\n",
    "# 创建一个形状为 (2, 3) 的零张量\n",
    "x = torch.zeros((2, 3))\n",
    "print(x)\n",
    "\n",
    "# 创建一个形状为 (2, 3) 的全一(one)张量\n",
    "x = torch.ones((2, 3))\n",
    "print(x)\n",
    "\n",
    "# 创建一个单位矩阵\n",
    "x = torch.eye(3)\n",
    "print(x)\n",
    "\n",
    "# 创建一个等差数列张量\n",
    "x = torch.arange(1, 6)\n",
    "print(x)\n",
    "\n",
    "# 创建一个线性空间张量\n",
    "x = torch.linspace(1, 5, steps=10)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2. 张量操作**\n",
    "\n",
    "* `x.view(shape)`：改变张量的形状。\n",
    "* `x.reshape(shape)`：改变张量的形状。\n",
    "* `x.t()`：转置张量。\n",
    "* `x.transpose(dim0, dim1)`：交换张量的两个维度。\n",
    "* `x.unsqueeze(dim)`：增加张量的一个维度。\n",
    "* `x.squeeze(dim)`：删除张量的一个维度。\n",
    "* `x.repeat(n)`：重复张量 n 次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个张量\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# 改变张量形状\n",
    "y = x.view(3, 2)\n",
    "print(y)\n",
    "\n",
    "# 转置张量\n",
    "y = x.t()\n",
    "print(y)\n",
    "\n",
    "# 交换张量两个维度\n",
    "y = x.transpose(0, 1)\n",
    "print(y)\n",
    "\n",
    "# 增加张量的一个维度\n",
    "y = x.unsqueeze(0)\n",
    "print(y)\n",
    "\n",
    "# 删除张量的一个维度\n",
    "y = x.squeeze(0)\n",
    "print(y)\n",
    "\n",
    "# 重复张量 n 次\n",
    "y = x.repeat(2, 1)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3. 数学运算**\n",
    "\n",
    "* `x + y`：张量加法。\n",
    "* `x - y`：张量减法。\n",
    "* `x * y`：张量乘法。\n",
    "* `x / y`：张量除法。\n",
    "* `x.dot(y)`：张量点积。\n",
    "* `x.mm(y)`：张量矩阵乘法。\n",
    "* `x.pow(n)`：张量幂运算。\n",
    "* `x.exp()`：张量指数运算。\n",
    "* `x.log()`：张量对数运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建两个张量\n",
    "x = torch.tensor([1, 2, 3])\n",
    "y = torch.tensor([4, 5, 6])\n",
    "\n",
    "# 加法\n",
    "z = x + y\n",
    "print(z)\n",
    "\n",
    "# 减法\n",
    "z = x - y\n",
    "print(z)\n",
    "\n",
    "# 乘法\n",
    "z = x * y\n",
    "print(z)\n",
    "\n",
    "# 除法\n",
    "z = x / y\n",
    "print(z)\n",
    "\n",
    "# 点积\n",
    "z = torch.dot(x, y)\n",
    "print(z)\n",
    "\n",
    "# 矩阵乘法\n",
    "x = torch.tensor([[1, 2], [3, 4]])\n",
    "y = torch.tensor([[0, 1], [1, 0]])\n",
    "\n",
    "z = torch.mm(x, y)\n",
    "print(z)\n",
    "\n",
    "# 矩阵乘法，x 矩阵乘 y的转置\n",
    "torch.matmul(x, y)  \n",
    "print(z)\n",
    "\n",
    "# 幂运算\n",
    "z = x.pow(2)\n",
    "print(z)\n",
    "\n",
    "# 指数运算\n",
    "z = x.exp()\n",
    "print(z)\n",
    "\n",
    "# 对数运算\n",
    "z = x.log()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4. 统计运算**\n",
    "\n",
    "* `torch.sum(x)`：计算张量元素的总和。\n",
    "* `torch.mean(x)`：计算张量元素的平均值。\n",
    "* `torch.max(x)`：计算张量元素的最大值。\n",
    "* `torch.min(x)`：计算张量元素的最小值。\n",
    "* `torch.var(x)`：计算张量元素的方差。\n",
    "* `torch.std(x)`：计算张量元素的标准差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个张量\n",
    "x = torch.FloatTensor([1, 2, 3, 4, 5])\n",
    "\n",
    "# 求和\n",
    "print(\"求和:\", torch.sum(x))\n",
    "\n",
    "# 求平均值\n",
    "print(\"求平均值:\", torch.mean(x))\n",
    "\n",
    "# 求方差\n",
    "print(\"求方差:\", torch.var(x))\n",
    "\n",
    "# 求标准差\n",
    "print(\"求标准差:\", torch.std(x))\n",
    "\n",
    "# 求最大值\n",
    "print(\"求最大值:\", torch.max(x))\n",
    "\n",
    "# 求最小值\n",
    "print(\"求最小值:\", torch.min(x))\n",
    "\n",
    "# 统计元素个数\n",
    "print(\"统计元素个数:\", torch.numel(x))\n",
    "\n",
    "# 统计非零元素个数\n",
    "print(\"统计非零元素个数:\", torch.count_nonzero(x))\n",
    "\n",
    "# 求中位数\n",
    "print(\"求中位数:\", torch.median(x))\n",
    "\n",
    "# 求众数\n",
    "print(\"求众数:\", torch.mode(x))\n",
    "\n",
    "# 按指定维度求和\n",
    "x = torch.arange(20).reshape(2, 10)\n",
    "print(\"按指定维度求和:\", torch.sum(x, dim=0))\n",
    "\n",
    "# 按指定维度求平均值\n",
    "x = torch.arange(20).reshape(2, 10).float()  # 注意类型转换\n",
    "print(\"按指定维度求平均值:\", torch.mean(x, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.5. 比较运算**\n",
    "\n",
    "* `x == y`：张量元素相等比较。\n",
    "* `x != y`：张量元素不等比较。\n",
    "* `x > y`：张量元素大于比较。\n",
    "* `x < y`：张量元素小于比较。\n",
    "* `x >= y`：张量元素大于等于比较。\n",
    "* `x <= y`：张量元素小于等于比较。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(1, 5)\n",
    "y = torch.arange(3, 7)\n",
    "\n",
    "x == y\n",
    "x != y\n",
    "x > y\n",
    "x < y\n",
    "x >= y\n",
    "x <= y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.6. 逻辑运算**\n",
    "\n",
    "* `torch.logical_and(x, y)`：张量逻辑与运算。\n",
    "* `torch.logical_or(x, y)`：张量逻辑或运算。\n",
    "* `torch.logical_not(x)`：张量逻辑非运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建两个张量\n",
    "x = torch.tensor([True, False, True])\n",
    "y = torch.tensor([False, True, False])\n",
    "\n",
    "# 逻辑与运算\n",
    "z = x & y\n",
    "print(\"逻辑与运算:\", z)\n",
    "\n",
    "# 逻辑或运算\n",
    "z = x | y\n",
    "print(\"逻辑或运算:\", z)\n",
    "\n",
    "# 逻辑非运算\n",
    "z = ~x\n",
    "print(\"逻辑非运算:\", z)\n",
    "\n",
    "# 逻辑异或运算\n",
    "z = x ^ y\n",
    "print(\"逻辑异或运算:\", z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 逻辑与运算\n",
    "z = torch.logical_and(x, y)\n",
    "print(\"逻辑与运算:\", z)\n",
    "\n",
    "# 逻辑或运算\n",
    "z = torch.logical_or(x, y)\n",
    "print(\"逻辑或运算:\", z)\n",
    "\n",
    "# 逻辑非运算\n",
    "z = torch.logical_not(x)\n",
    "print(\"逻辑非运算:\", z)\n",
    "\n",
    "# 逻辑异或运算\n",
    "z = torch.logical_xor(x, y)\n",
    "print(\"逻辑异或运算:\", z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.7. 随机数生成**\n",
    "\n",
    "* `torch.rand(shape, dtype=None, device=None)`：生成随机均匀分布的张量。\n",
    "* `torch.randn(shape, dtype=None, device=None)`：生成随机正态分布的张量。\n",
    "* `torch.randint(shape, dtype=None, device=None)`：生成指定范围内的随机整数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成随机均匀分布的张量\n",
    "x = torch.rand(3, 2)\n",
    "print(\"随机均匀分布:\", x)\n",
    "\n",
    "# 生成随机正态分布的张量\n",
    "y = torch.randn(3, 2)\n",
    "print(\"随机正态分布:\", y)\n",
    "\n",
    "# 生成指定范围内的随机整数\n",
    "z = torch.randint(0, 10, (3, 2))\n",
    "print(\"指定范围内的随机整数:\", z)\n",
    "\n",
    "# 生成随机排列\n",
    "w = torch.randperm(5)\n",
    "print(\"随机排列:\", w)\n",
    "\n",
    "# 生成随机采样\n",
    "v = torch.multinomial(torch.ones(5), 3, replacement=True)\n",
    "print(\"随机采样:\", v)\n",
    "\n",
    "# 生成伯努利分布的随机数\n",
    "u = torch.bernoulli(torch.rand(3, 2))\n",
    "print(\"伯努利分布的随机数:\", u)\n",
    "\n",
    "# 生成泊松分布的随机数\n",
    "p = torch.poisson(torch.rand(3, 2))\n",
    "print(\"泊松分布的随机数:\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.8. 其他常用函数**\n",
    "\n",
    "* `torch.argmax(x)`：返回张量中最大元素的索引。\n",
    "* `torch.argmin(x)`：返回张量中最小元素的索引。\n",
    "* `torch.sort(x, dim=0, descending=False)`：对张量进行排序。\n",
    "* `torch.clamp(x, min, max)`：将张量元素限制在指定范围内。可以用于防止梯度爆炸、归一化数据、防止溢出和动态调整数据范围。\n",
    "* `torch.round(x)`：对张量元素进行四舍五入。\n",
    "* `torch.ceil(x)`：对张量元素向上取整。\n",
    "* `torch.floor(x)`：对张量元素向下取整。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个张量\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# 求最大元素的索引\n",
    "max_index = torch.argmax(x)\n",
    "print(\"最大元素的索引:\", max_index)\n",
    "\n",
    "# 求最小元素的索引\n",
    "min_index = torch.argmin(x)\n",
    "print(\"最小元素的索引:\", min_index)\n",
    "\n",
    "# 对张量进行排序\n",
    "sorted_x, indices = torch.sort(x, dim=0, descending=False)\n",
    "print(\"排序后的张量:\", sorted_x)\n",
    "print(\"排序索引:\", indices)\n",
    "\n",
    "# 将张量元素限制在指定范围内\n",
    "clamped_x = torch.clamp(x, min=2, max=8)\n",
    "print(\"限制范围后的张量:\", clamped_x)\n",
    "\n",
    "# 对张量元素进行四舍五入\n",
    "rounded_x = torch.round(x)\n",
    "print(\"四舍五入后的张量:\", rounded_x)\n",
    "\n",
    "# 对张量元素向上取整\n",
    "ceiled_x = torch.ceil(x)\n",
    "print(\"向上取整后的张量:\", ceiled_x)\n",
    "\n",
    "# 对张量元素向下取整\n",
    "floored_x = torch.floor(x)\n",
    "print(\"向下取整后的张量:\", floored_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.9. 形状**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(1, 50, size=(10, 5))  # 1~50之间的随机整数10行5列\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.size()  # 形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape  # 各维度上的数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.reshape(5, 10)  # 重塑形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.reshape(-1, 10)  # 如果某一维度为-1，则根据总元素个数自动计算该轴长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.reshape(10, -1)  # 如果某一维度为-1，则根据总元素个数自动计算该轴长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.numel()  # 元素个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.reshape(50)  # 变为1维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.reshape(2, 5, 5)  # 变为3维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(1, 10, 2)  # torch.range()也可用，建议用前者"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linspace(1, 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones_like(x)  #  注意类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones_like(x).type(), x.type()  #  注意类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones(size=x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros_like(x)  # 注意类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(10, 2) * 0.5 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.randn(10, 2, 2)  # 10*2*2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.squeeze(y, dim=1)  # 10\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(1, 1, 5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.arange(10).reshape(2, 5)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 转置和变换形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.t()  # 输入一个矩阵（2维张量），并转置0, 1维。 可以被视为函数transpose(input, 0, 1)的简写函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.reshape(z.shape[1], -1)  # 注意和转置之间的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 转置: 行变为列, 变换形状: 先转变为1纬，然后再重新塑形"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.sign() 得到变量的符号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.squeeze_().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sign(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sigmoid(y)  # 返回一个新张量，包含输入input张量每个元素的sigmoid值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.norm(y, p=2)  # 返回输入张量input 的p 范数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.median(y)  # 返回输入张量给定维度每行的中位数，同时返回一个包含中位数的索引的LongTensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.prod(y)  # 返回输入张量input 所有元素的积。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.relu(torch.randn(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.10. 乘法**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 向量乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(10)\n",
    "b = torch.arange(10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a * b  # 按元素乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mul(a, b)  # 按元素乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.matmul(a, b)  # 矩阵乘法，前者 矩阵乘 后者的转置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.reshape(-1, 1) @ b.reshape(1, -1)  # 10*1 @ 1*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.trace(a.reshape(-1, 1) @ b.reshape(1, -1))  # 迹运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mm(a.reshape(1, -1), b.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.dot(a, b)  # 內积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a @ b  # 矩阵乘， 前者 矩阵乘 后者的转置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 矩阵乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(4, 3)\n",
    "y = torch.rand(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mm(x, y)  # 矩阵乘法: 对矩阵mat1和mat2进行相乘。 如果mat1 是一个n×m张量，mat2 是一个 m×p张量，将会输出一个 n×p张量out。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.matmul(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x @ y  # 矩阵乘法运算符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.rand(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mv(x, z)  # 矩阵向量乘： 对矩阵mat和向量vec进行相乘。 如果mat 是一个n×m张量，vec 是一个m元 1维张量，将会输出一个n元 1维张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x @ z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 迹运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.arange(25).reshape(5, 5)\n",
    "B = torch.arange(25, 50).reshape(5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$tr(A)=tr(A^T)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.trace(A), torch.trace(A.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.trace(B), torch.trace(B.t())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$tr(AB)=tr(BA)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mm(A, B), torch.mm(B, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.trace(torch.mm(A, B)), torch.trace(torch.mm(B, A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$tr(A^TB)=\\sum_i\\sum_jA_{i,j}B_{i,j}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_t_B = torch.mm(A.t(), B)\n",
    "A_t_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.trace(A_t_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(torch.IntTensor([A_t_B[i, i] for i in range(5)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.11. 张量拼接**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tensor(range(10), dtype=torch.float32).reshape(2, 5)\n",
    "b = tensor(range(10, 20), dtype=torch.float32).reshape(2, 5)\n",
    "c = tensor(range(20, 30), dtype=torch.float32).reshape(2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_ = torch.cat([a, b, c], 0)  # 在给定维度上对输入的张量序列进行连接操作，和extend类似\n",
    "cat_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([a, b, c], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_ = torch.stack([a, b, c], 0)  # 沿着一个新维度对输入张量序列进行连接。 序列中所有的张量都应该为相同形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_.split([2, 3], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_.chunk(2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 自动求导`autograd`\n",
    "\n",
    "torch.autograd提供了类和函数用来对任意标量函数进行求导。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例:\n",
    "$$\n",
    "f(\\mathbf{x})=2\\mathbf{x}+1, g(y)=\\mathbf{y^2}+5, z=mean(\\mathbf{g(y)})\n",
    "$$\n",
    "求$\\frac{dz}{dx}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x:tensor):\n",
    "    return 2*x + 1\n",
    "\n",
    "def g(x:tensor):\n",
    "    return x**2 + 5\n",
    "\n",
    "def mean(x:tensor):\n",
    "    return torch.mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dz_dx(x:tensor):  # 实际上的导数\n",
    "    return (8*x + 4) / x.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(1, 10, size=(2, 5), dtype=torch.float32, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad\n",
    "# x.requires_grad_(True)  # 如果为False, 可以追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch自动求导结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = mean(g(f(x)))\n",
    "z.backward()  # 反向传播，自动求微分\n",
    "x.grad  # dz/dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 解析求导结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dz_dx(x)  # dz/dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([a, a[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 案例: Bass模型拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bass扩散模型针对创新产品、服务和扩散进行建模，常被用作市场分析工具，对新产品、新技术需求进行预测。\n",
    "\n",
    "新产品创新扩散是指新产品从创造研制到进入市场推广、最终使用的过程，表现为广大消费者从知晓、兴趣、评估、试用到最终采用新产品的行为。\n",
    "\n",
    "Bass扩散模型的许多变形也已被开发出来，用以满足某些特殊情形的精确需求。\n",
    "\n",
    "$$\n",
    "N(t)=m\\frac{1-e^{-(p+1)t}}{1+\\frac{q}{p}e^{-(p+q)t}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_momentum(lossfunc, w, x_dict, beta1=0.5, beta2=0.9, learn_rate=0.999, max_iter=1000, epsilon=1e-8):\n",
    "    trace_w = w.clone().data.reshape(1, -1)\n",
    "    v_0, s_0 = 0, 0\n",
    "    i = 1\n",
    "    while i <= max_iter:\n",
    "        l = lossfunc(w, **x_dict)\n",
    "        l.backward()\n",
    "        v_1 = (beta1 * v_0 + (1 - beta1) * w.grad.data) / (1 - beta1 ** i)\n",
    "        s_1 = (beta2 * s_0 + (1 - beta2) * w.grad.data ** 2) / (1 - beta2 ** i)\n",
    "        w.data.sub_(learn_rate * v_1 / (torch.sqrt(s_1) + epsilon))\n",
    "        with torch.no_grad():\n",
    "            trace_w = torch.cat([trace_w, w.detach().data.reshape(1, -1)], 0)\n",
    "            if i % 50 == 0:\n",
    "                loss = lossfunc(w, **x_dict).data.numpy()\n",
    "                print(f\"迭代次数: {i}, 损失函数值: {loss:.4f}\")\n",
    "\n",
    "            if torch.sum(torch.abs(trace_w[-1] - trace_w[-2])) < 1e-3:  # 停止条件\n",
    "                break\n",
    "\n",
    "        w.grad.zero_()\n",
    "        v_0, s_0 = v_1, s_1\n",
    "        i += 1\n",
    "\n",
    "    print(f\"共迭代{i - 1}次, 损失函数值: {lossfunc(w, **x_dict).data.numpy():.4f}, 最优参数值: {w.tolist()}\")\n",
    "    return trace_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bass(params, T:int): # 如果要使用其它模型，可以重新定义\n",
    "    p, q, m = params\n",
    "    t_tensor = torch.arange(1, T + 1, dtype=torch.float32)\n",
    "    a = 1 - torch.exp(- (p + q) * t_tensor)\n",
    "    b = 1 + q / p * torch.exp(- (p + q) * t_tensor)\n",
    "    diffu_cont = m * a / b\n",
    "\n",
    "    adopt_cont = torch.zeros_like(diffu_cont)\n",
    "    adopt_cont[0] = diffu_cont[0]\n",
    "    for t in range(1, T):\n",
    "        adopt_cont[t] = diffu_cont[t] - diffu_cont[t - 1]\n",
    "        \n",
    "    return adopt_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanSquaredLoss(params, y):  # 平均平方误差\n",
    "    T = y.numel()\n",
    "    hat_y = bass(params, T)\n",
    "    return torch.mean((hat_y - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_2(params, y):  # R2\n",
    "    T = y.numel()\n",
    "    hat_y = bass(params, T)\n",
    "    tse = torch.sum((y - hat_y)**2)\n",
    "    ssl = torch.sum((y - torch.mean(y))**2)\n",
    "    R_2 = (ssl - tse)/ssl\n",
    "    return R_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tensor([96, 195, 238, 380, 1045, 1230, 1267, 1828, 1586, 1673, 1800, 1580, 1500], dtype=torch.float32) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.FloatTensor([0.001, 0.3, 20])\n",
    "params.requires_grad_(True)\n",
    "# res = grad_desc(meanSquaredLoss, params, y, learn_rate=1e-9)\n",
    "res = adaptive_momentum(meanSquaredLoss, params, x_dict={\"y\": y}, beta1=0.6, beta2=0.5, learn_rate=0.003, max_iter=1000)\n",
    "r2 = r_2(res[-1], y).numpy()\n",
    "print(\"r2:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = y.numel()\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Sales volumn\")\n",
    "plt.plot(np.arange(T), bass(res[-1], T).numpy() * 1000)\n",
    "plt.scatter(np.arange(T), y.numpy() * 1000, marker='o', color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 构建神经网络的流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. 定义计算架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, dim_feature, dim_hidden, dim_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = nn.Linear(dim_feature, dim_hidden)\n",
    "        self.output = nn.Linear(dim_hidden, dim_output)\n",
    "    \n",
    "    def forward(self, X):  # 层之间的计算次序\n",
    "        f1 = torch.relu(self.hidden(X))  # 0-1\n",
    "        f2 = self.output(f1)  # 1->2\n",
    "        f3 = nn.functional.softmax(f2, dim=1)  # 2->3\n",
    "        return f3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(dim_feature=2, dim_hidden=10, dim_output=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.05)  # 指定需优化的参数\n",
    "loss_func = nn.CrossEntropyLoss()  # 确定训练准则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. 生成测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = torch.randn(100, 2) + 2  # 均值为 2\n",
    "y0 = torch.zeros(100)\n",
    "x1 = torch.randn(100, 2) - 2  # 均值为 -2\n",
    "y1 = torch.ones(100)\n",
    "\n",
    "x = torch.cat((x0, x1)).type(torch.FloatTensor)\n",
    "y = torch.cat((y0, y1)).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(len(x))\n",
    "np.random.shuffle(idx)\n",
    "train_x, train_y = x[idx[:50]], y[idx[:50]]  # 随机选取50个\n",
    "test_x, test_y = x[idx[50:]], y[idx[50:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(400):\n",
    "    out = net(train_x)\n",
    "    loss = loss_func(out, train_y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 40 == 0:\n",
    "        with torch.no_grad():\n",
    "            loss_train = loss_func(out, train_y)\n",
    "            out_test = net.forward(test_x)\n",
    "            loss_test = loss_func(out_test, test_y)\n",
    "            print(f\"loss_train: {loss_train}, loss_test: {loss_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = net(train_x)\n",
    "predict_train_y = torch.max(train_result, 1)[1]\n",
    "\n",
    "test_result = net(test_x)\n",
    "predict_test_y = torch.max(test_result, 1)[1]\n",
    "\n",
    "x_list = [train_x, test_x]\n",
    "y_list = [predict_train_y, predict_test_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "for i in range(2):\n",
    "    px = x_list[i]\n",
    "    py = y_list[i]\n",
    "    ax = fig.add_subplot(1, 2, i+1)\n",
    "    ax.set_xlabel('$x_0$')\n",
    "    ax.set_ylabel('$x_1$')\n",
    "    ax.scatter(px.data.numpy()[:,0], px.data.numpy()[:,1], c=py.data.numpy(), s=60, lw=0, cmap='RdYlGn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 案例： 垃圾邮件分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用`nn.Module`实现`Logit`回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogitNet(nn.Module):\n",
    "    def __init__(self, dim_feature, dim_output):\n",
    "        super(LogitNet, self).__init__()\n",
    "        self.output = nn.Linear(dim_feature, dim_output)\n",
    "    \n",
    "    def forward(self, X):  # 层之间的计算次序\n",
    "        h = self.output(X)  # 1 -> 2\n",
    "        o = nn.functional.softmax(h, dim=1)  # 2 -> 3\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dataset/smsspamcollection/SMSSpamCollection', delimiter='\\t', header=None, names=['category', 'message'])\n",
    "df['label'] = (df.category == 'ham').astype('int')\n",
    "print('垃圾邮件数量: %d ' % np.sum(df.label == 0))\n",
    "print('正常邮件数量: %d ' % np.sum(df.label == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.message.values\n",
    "y = df.label.values\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=200)\n",
    "# y转换为tensor\n",
    "y_train = torch.tensor(y_train).type(torch.LongTensor)  # 注意label的形式为1维，即类别的标签，无需reshape(-1, 1)\n",
    "y_test = torch.tensor(y_test).type(torch.LongTensor)\n",
    "# 获取词的tf-idf\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train_raw)\n",
    "X_test = vectorizer.transform(X_test_raw)\n",
    "# X转换为tensor\n",
    "X_train = torch.tensor(X_train.toarray(), dtype=torch.float)\n",
    "X_test = torch.tensor(X_test.toarray(), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 300  # 构建每批次100个样本的训练集\n",
    "dataset = TensorDataset(X_train, y_train)\n",
    "data_iter = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snet = LogitNet(dim_feature=X_train.shape[1], dim_output=2)\n",
    "optimizer = torch.optim.SGD(snet.parameters(), lr=0.03)  # 指定需优化的参数\n",
    "# loss_func = nn.CrossEntropyLoss()  # 确定训练准则\n",
    "loss_func = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    for X, y in data_iter:        \n",
    "        loss = loss_func(snet.forward(X), y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if i % 20 == 0:\n",
    "        with torch.no_grad():\n",
    "            loss_train = loss_func(snet.forward(X_train), y_train)\n",
    "            out_test = snet.forward(X_test)\n",
    "            loss_test = loss_func(out_test, y_test)\n",
    "            print(f\"loss_train: {loss_train:.5f}, loss_test: {loss_test:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = snet(X_train)\n",
    "predict_y_train = torch.max(train_result, 1)[1]\n",
    "print(torch.sum(predict_y_train != y_train), torch.sum(predict_y_train == y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.std(a, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考资料\n",
    "\n",
    "* PyTorch 官方文档: [https://pytorch.org/docs/](https://pytorch.org/docs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
