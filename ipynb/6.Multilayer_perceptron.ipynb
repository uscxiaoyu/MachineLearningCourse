{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 六. 前馈神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from IPython import display\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import gzip\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "display.set_matplotlib_formats(\"svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 导言\n",
    "\n",
    "神经网络是一门重要的机器学习技术，是深度学习的基础。学习神经网络不仅可以让你掌握一门强大的机器学习方法，也可以更好地帮助你理解深度学习技术。神经网络是一种模拟人脑的神经网络结构以期能够实现类人工智能的机器学习技术。人脑中的神经网络是一个非常复杂的组织，成人的大脑中估计有1000亿个神经元。\n",
    "\n",
    "神经网络（neural network）方面的研究很早就已出现，今天的“神经网络”已经是一个相当大的、多学科交叉的学科领域。各相关学科对神经网络的定义多种多样，此处采用目前使用最广泛的一种：神经网络是由具有适应性的简单单元组成的广泛并行互联的网络，它的祖师能够模拟生物神经系统对真实世界物体所作出的交互反应(`Kohonen, 1988`)。\n",
    "\n",
    "神经网络中最基本的成分是神经元模型，即上述定义的“简单单元”。在生物神经网络中，每个神经元与其他神经元相连，当它“兴奋”时，就会向相连的神经元发送化学物质，从而改变这些神经元内的电位；如果某个神经元的电位超过了一个“阈值”，那么它被激活，向其它神经元发送化学物质。把许多个这样的神经元按一定的层次结构连接起来，就得到了神经网络。\n",
    "\n",
    "从计算机科学的角度看，我们暂时不考虑神经网络是否真的模拟和生物神经网络，只需将一个神经网络视为包含了许多参数的数学模型，这个模型是若干个函数相互嵌套代入而得。感知机只有输出层神经元进行激活函数处理，即只拥有一层功能神经元，学习能力非常有限，只能解决线性可分问题，例如甚至不能学习“异或”问题。要解决更加复杂的如非线性可分问题，需考虑使用多层功能神经元。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 神经网络的发展过程\n",
    "\n",
    "- 1940年，生物学家知晓了神经元的组织结构\n",
    "\n",
    "> 一个神经元通常具有多个树突，主要接收传入信息。轴突只有一条，轴突尾端有许多轴突末梢可以给其它多个神经元传递信息，跟其它神经元的树突产生连接，连接的问题称为“突触”。\n",
    "\n",
    "- 1943年，心理学家McCulloch和Pitts参考了生物神经元的结构，发明了抽象的神经元模型MP\n",
    "\n",
    "- 1949年，心理学家Hebb提出了Hebb学习率，任务人脑神经细胞的突触上的强度可以变化。因此，计算机科学家开始考虑用调整权重的方法来让机器学习，为后来的学习算法奠定了基础。\n",
    "\n",
    "- 1958年，计算科学家Rosenbalt提出了由两层神经元组成的神经网络，取名为“Perceptron”\n",
    "\n",
    "- 1969年，人工智能巨擘Minsky在一本叫《Perceptron》一书中，用详细的数学证明了感知机的弱点（不能解决异或问题），且认为如果将计算层增加到两层，计算量将很大，而且没有有效的学习算法，由此认为研究更深层的网络没有价值。\n",
    "\n",
    "- 1986年，Rumelhar&Hinton等人提出反向传播算法，解决了两层神经网络（MLP）所需要的复杂计算量问题，从而带动了业界使用两层神经网络研究的热潮。10多年前困扰神经网络界的异或问题被轻松解决，神经网络在这个时候，已经可以用于语音识别、图像识别和自动驾驶等多个领域。\n",
    "\n",
    "- 2006年，Hinton在《Science》和相关期刊上发表论文，首次提出了“深度信念网络”的概念。深度信念网络有一个“预训练(pre-training)”的过程，这可以方便的让神经网络中的权值找到一个接近最优解的值，之后再使用“微调(fine-tuning)”技术对整个网络进行优化训练。\n",
    "\n",
    "- 2012年，Hinton与他的学生在ImageNet竞赛中，用多层的卷积神经网络成功地对包含1,000多个类别的1,000,000多张图片进行了训练，取得了分类错误率15%的好成绩，比第2名高出11个百分点。\n",
    "\n",
    "- 神经网络在人工智能界占据统治地位：`Hinton, YannLecun, Bengio, Andrew Ng`获得2018年的图灵奖\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"350\" src=\"../pictures/6.neural-network-develop.jpg\"/>\n",
    "</div>\n",
    "\n",
    "<div align=center> \n",
    "    图2 神经网络发展历程\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 神经网络的分类\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"700\" src=\"../pictures/6.category of ann.svg\"/>\n",
    "</div>\n",
    "\n",
    "<div align=center> \n",
    "    图3 神经网络的分类\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 多层感知机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们将以多层感知机（multilayer perceptron，MLP）为例，介绍多层神经网络的概念。\n",
    "\n",
    "多层感知机在单层神经网络的基础上引入了一到多个隐藏层（hidden layer）。隐藏层位于输入层和输出层之间。图3.3展示了一个多层感知机的神经网络图，它含有一个隐藏层，该层中有5个隐藏单元。\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"350\" src=\"../pictures/6.1.svg\"/>\n",
    "</div>\n",
    "<div align=center> \n",
    "    图3 带有隐藏层的多层感知机\n",
    "</div>\n",
    "\n",
    "图3所示的多层感知机中，输入和输出个数分别为4和3，中间的隐藏层中包含了5个隐藏单元（hidden unit）。由于输入层不涉及计算，图3中的多层感知机的层数为2。隐藏层中的神经元和输入层中各个输入完全连接，输出层中的神经元和隐藏层中的各个神经元也完全连接。因此，多层感知机中的隐藏层和输出层都是全连接层。\n",
    "\n",
    "具体来说，给定一个小批量样本$\\boldsymbol{X} \\in \\mathbb{R}^{n \\times d}$，其批量大小为$n$，输入个数为$d$。假设多层感知机只有一个隐藏层，其中隐藏单元个数为$h$。记隐藏层的输出（也称为隐藏层变量或隐藏变量）为$\\boldsymbol{H}$，有$\\boldsymbol{H} \\in \\mathbb{R}^{n \\times h}$。\n",
    "\n",
    "因为隐藏层和输出层均是全连接层，可以设隐藏层的权重参数和偏差参数分别为$\\boldsymbol{W}_h \\in \\mathbb{R}^{d \\times h}$和 $\\boldsymbol{b}_h \\in \\mathbb{R}^{1 \\times h}$，输出层的权重和偏差参数分别为$\\boldsymbol{W}_o \\in \\mathbb{R}^{h \\times q}$和$\\boldsymbol{b}_o \\in \\mathbb{R}^{1 \\times q}$。\n",
    "\n",
    "我们先来看一种含单隐藏层的多层感知机的设计。其输出$\\boldsymbol{O} \\in \\mathbb{R}^{n \\times q}$的计算为\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{H} &= \\boldsymbol{X} \\boldsymbol{W}_h + \\boldsymbol{b}_h,\\\\\n",
    "\\boldsymbol{O} &= \\boldsymbol{H} \\boldsymbol{W}_o + \\boldsymbol{b}_o,\n",
    "\\end{aligned}      \n",
    "$$\n",
    "\n",
    "也就是将隐藏层的输出直接作为输出层的输入。如果将以上两个式子联立起来，可以得到\n",
    "\n",
    "$$\n",
    "\\boldsymbol{O} = (\\boldsymbol{X} \\boldsymbol{W}_h + \\boldsymbol{b}_h)\\boldsymbol{W}_o + \\boldsymbol{b}_o = \\boldsymbol{X} \\boldsymbol{W}_h\\boldsymbol{W}_o + \\boldsymbol{b}_h \\boldsymbol{W}_o + \\boldsymbol{b}_o.\n",
    "$$\n",
    "\n",
    "从联立后的式子可以看出，虽然神经网络引入了隐藏层，却依然等价于一个单层神经网络：其中输出层权重参数为$\\boldsymbol{W}_h\\boldsymbol{W}_o$，偏差参数为$\\boldsymbol{b}_h \\boldsymbol{W}_o + \\boldsymbol{b}_o$。\n",
    "\n",
    "不难发现，即便再添加更多的隐藏层，以上设计依然只能与仅含输出层的单层神经网络等价。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. 激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述问题的根源在于全连接层只是对数据做仿射变换（`affine transformation`），而多个仿射变换的叠加仍然是一个仿射变换。\n",
    "\n",
    "解决问题的一个方法是引入非线性变换，例如对隐藏变量使用按元素运算的非线性函数进行变换，然后再作为下一个全连接层的输入。这个非线性函数被称为激活函数（activation function）。下面我们介绍几个常用的激活函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. `torch.sigmoid`\n",
    "\n",
    "sigmoid函数可以将元素的值变换到0和1之间：\n",
    "\n",
    "$$\\text{sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}.$$\n",
    "\n",
    "sigmoid函数在早期的神经网络中较为普遍，但它目前逐渐被更简单的ReLU函数取代。当输入接近0时，sigmoid函数接近线性变换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-5, 5, 0.02)\n",
    "y = 1 / (1+np.exp(-x))\n",
    "dy_dx = y*(1 - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 4))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(x, y, 'k-', label=\"sigmoid(x)\")\n",
    "ax.annotate('Threshold', xy=(0, 0.5), xytext=(1, 0.5),\n",
    "            arrowprops=dict(facecolor='green', width=1, headwidth=10, alpha=0.2))\n",
    "ax.scatter(0, 0.5, color='tan', marker='o')\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_xlim([-5, 5])\n",
    "ax.set_ylim([0, 1])\n",
    "ax.vlines(0, ymin=0, ymax=1, linestyle='--')\n",
    "ax.legend()\n",
    "plt.savefig(\"../pictures/6.sigmoid.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "依据链式法则，sigmoid函数的导数\n",
    "\n",
    "$$\\text{sigmoid}'(x) = \\text{sigmoid}(x)\\left(1-\\text{sigmoid}(x)\\right).$$\n",
    "\n",
    "下面绘制了sigmoid函数的导数。当输入为0时，sigmoid函数的导数达到最大值0.25；当输入越偏离0时，sigmoid函数的导数越接近0。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 4))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(x, dy_dx, linestyle='-', color='sienna', label=r'$\\frac{\\mathrm{d}sigmoid(x)}{\\mathrm{x}}$')\n",
    "ax.scatter(0, 0.25, color='tan', marker='o')\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_xlim([-5, 5])\n",
    "ax.set_ylim([0, 1])\n",
    "ax.vlines(0, ymin=0, ymax=1, linestyle='--')\n",
    "ax.legend()\n",
    "plt.savefig(\"../pictures/6.sigmoid-derivate.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. `torch.relu`\n",
    "\n",
    "ReLU（rectified linear unit）函数提供了一个很简单的非线性变换。给定元素$x$，该函数定义为\n",
    "\n",
    "$$\\text{ReLU}(x) = \\max(x, 0).$$\n",
    "\n",
    "可以看出，ReLU函数只保留正数元素，并将负数元素清零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-2, 2, 0.02)\n",
    "y = x.copy()\n",
    "y[x < 0] = 0\n",
    "dy_dx = np.ones_like(x)\n",
    "dy_dx[x >= 0] = 1\n",
    "dy_dx[x < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 4))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(x, y, 'k-', label=\"Relu(x)\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_xlim([-2, 2])\n",
    "ax.set_ylim([-1, 2])\n",
    "ax.vlines(0, ymin=-1, ymax=2, linestyle='--', alpha=0.2)\n",
    "ax.hlines(0, xmin=-2, xmax=2, linestyle='--', alpha=0.2)\n",
    "ax.legend()\n",
    "plt.savefig(\"../pictures/6.relu.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显然，当输入为负数时，ReLU函数的导数为0；当输入为正数时，ReLU函数的导数为1。尽管输入为0时ReLU函数不可导，但是我们可以取此处的导数为0。下面绘制ReLU函数的导数。\n",
    "$$\n",
    "\\frac{\\text{d}Relu(x)}{\\text{d}x}=\\begin{cases}\n",
    "1,x\\ge 0\\\\\n",
    "0,x< 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 4))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "# ax.plot(x, y, 'k-', label=\"f(x)\")\n",
    "ax.plot(x[dy_dx==0], dy_dx[dy_dx==0], linestyle='-', color='sienna', label=r'$\\frac{\\mathrm{d}Relu(x)}{\\mathrm{x}}$')\n",
    "ax.plot(x[dy_dx==1], dy_dx[dy_dx==1], linestyle='-', color='sienna')\n",
    "ax.scatter(0, 0, color='white', edgecolor='grey')\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_xlim([-2, 2])\n",
    "ax.set_ylim([-1, 2])\n",
    "ax.vlines(0, ymin=-1, ymax=2, linestyle='--', alpha=0.2)\n",
    "ax.hlines(0, xmin=-2, xmax=2, linestyle='--', alpha=0.2)\n",
    "ax.legend()\n",
    "plt.savefig(\"../pictures/6.relu-derivate.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. `torch.tanh`\n",
    "\n",
    "tanh（双曲正切）函数可以将元素的值变换到-1和1之间：\n",
    "\n",
    "$$\\text{tanh}(x) = \\frac{1 - \\exp(-2x)}{1 + \\exp(-2x)}.$$\n",
    "\n",
    "我们接着绘制tanh函数。当输入接近0时，tanh函数接近线性变换。虽然该函数的形状和sigmoid函数的形状很像，但tanh函数在坐标系的原点上对称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-5, 5, 0.02)\n",
    "y = (1 - np.exp(-2*x)) / (1+np.exp(-2*x))\n",
    "dy_dx = 1 - y**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 4))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(x, y, 'k-', label=\"tanh(x)\")\n",
    "ax.annotate('Threshold', xy=(0, 0), xytext=(1, -0.5),\n",
    "            arrowprops=dict(facecolor='green', width=1, headwidth=10, alpha=0.2))\n",
    "ax.scatter(0, 0, color='tan', marker='o')\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_xlim([-5, 5])\n",
    "ax.set_ylim([-1.2, 1.2])\n",
    "ax.vlines(0, ymin=-1.2, ymax=1.2, linestyle='--')\n",
    "ax.hlines(0, xmin=-5, xmax=5, linestyle='--')\n",
    "ax.legend()\n",
    "\n",
    "plt.savefig(\"../pictures/6.tanh.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "依据链式法则，tanh函数的导数\n",
    "\n",
    "$$\\text{tanh}'(x) = 1 - \\text{tanh}^2(x).$$\n",
    "\n",
    "下面绘制了tanh函数的导数。当输入为0时，tanh函数的导数达到最大值1；当输入越偏离0时，tanh函数的导数越接近0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 4))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(x, dy_dx, linestyle='-', color='sienna', label=r'$\\frac{\\mathrm{d}tanh(x)}{\\mathrm{x}}$')\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_xlim([-5, 5])\n",
    "ax.set_ylim([-0.1, 1.2])\n",
    "ax.vlines(0, ymin=-0.1, ymax=1.2, linestyle='--')\n",
    "ax.hlines(0, xmin=-5, xmax=5, linestyle='--')\n",
    "ax.legend()\n",
    "plt.savefig(\"../pictures/6.tanh-derivate.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. 多层感知机模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多层感知机就是含有至少一个隐藏层的由全连接层组成的神经网络，且每个隐藏层的输出通过激活函数进行变换。多层感知机的层数和各隐藏层中隐藏单元个数都是超参数。以单隐藏层为例并沿用本节之前定义的符号，多层感知机按以下方式计算输出：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{H} &= \\phi(\\boldsymbol{X} \\boldsymbol{W}_h + \\boldsymbol{b}_h),\\\\\n",
    "\\boldsymbol{O} &= \\boldsymbol{H} \\boldsymbol{W}_o + \\boldsymbol{b}_o,\n",
    "\\end{aligned}\n",
    "$$\n",
    " \n",
    "其中$\\phi$表示激活函数。在分类问题中，我们可以对输出$\\boldsymbol{O}$做softmax运算，并使用softmax回归中的交叉熵损失函数。\n",
    "在回归问题中，我们将输出层的输出个数设为1，并将输出$\\boldsymbol{O}$直接提供给线性回归中使用的平方损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_perceptron(features, layers):\n",
    "    \"\"\"\n",
    "    features: 输入\n",
    "    layers: [(权重矩阵, 偏置向量, 激活函数), ..., ], 如果该层没有激活函数, 则应给None\n",
    "    \"\"\"\n",
    "    y = features  # 初始化输入features\n",
    "    for weight, bias, func in layers:\n",
    "        z = y@weight+bias\n",
    "        if func:\n",
    "            if func.__name__ == 'softmax':\n",
    "                y = func(z, dim=1)\n",
    "            else:\n",
    "                y = func(z)\n",
    "        else:\n",
    "            y = z\n",
    "            \n",
    "    return y.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.randn(1000, 5)\n",
    "layers = [\n",
    "    (torch.randn(5, 10), torch.tensor(0), torch.tanh),\n",
    "    (torch.randn(10, 6), torch.tensor(0), torch.relu),\n",
    "    (torch.randn(6, 5), torch.tensor(0), torch.softmax)\n",
    "]\n",
    "\n",
    "y = multilayer_perceptron(features, layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 5])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 交叉熵损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{crossEnt}=\\sum_iy_i\\log{\\hat{y}_i}$\n",
    "其中，$y$为one-hot向量, $y_i$为各类出现的概率。\n",
    "> 如果y设为类别标签标量，若y=c，直接有$\\text{crossEnt}=\\log{y_c}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y, hat_y):  # 交叉熵\n",
    "    '''\n",
    "    y: one-hot向量或者类别标量\n",
    "    hat_y: softmax之后对应概率向量，多层感知机的输出\n",
    "    '''\n",
    "    if len(y.shape) == 2:\n",
    "        crossEnt = -torch.dot(y.reshape(-1), torch.log10(hat_y.float()).reshape(-1)) / y.shape[0]  # 展开成1维，点积\n",
    "    elif len(y.shape) == 1:\n",
    "        crossEnt = -torch.mean(torch.log10(hat_y[torch.arange(y.shape[0]), y.long()]))\n",
    "    else:\n",
    "        print(\"Wrong format of y!\")\n",
    "    return crossEnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.randint(4, size=(1000,))\n",
    "hot_c = torch.zeros(1000, 5)\n",
    "hot_c[np.arange(1000), c] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5312)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(hot_c, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 案例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Fashion-MNIST](https://gitee.com/mirrors/Fashion-MNIST)是一个替代[MNIST](http://yann.lecun.com/exdb/mnist/)手写数字集的图像数据集。 \n",
    "它是由Zalando（一家德国的时尚科技公司）旗下的研究部门提供。其涵盖了来自10种类别的共7万个不同商品的正面图片。\n",
    "Fashion-MNIST的大小、格式和训练集/测试集划分与原始的MNIST完全一致。60000/10000的训练测试数据划分，28x28的灰度图片。\n",
    "你可以直接用它来测试你的机器学习和深度学习算法性能，且不需要改动任何的代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经典的MNIST数据集包含了大量的手写数字。十几年来，来自机器学习、机器视觉、人工智能、深度学习领域的研究员们把这个数据集作为衡量算法的基准之一。你会在很多的会议，期刊的论文中发现这个数据集的身影。实际上，MNIST数据集已经成为算法作者的必测的数据集之一。有人曾调侃道：\"如果一个算法在MNIST不work, 那么它就根本没法用；而如果它在MNIST上work, 它在其他数据上也可能不work！\"\n",
    "\n",
    "Fashion-MNIST的目的是要成为MNIST数据集的一个直接替代品。作为算法作者，你不需要修改任何的代码，就可以直接使用这个数据集。Fashion-MNIST的图片大小，训练、测试样本数及类别数与经典MNIST完全相同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 加载Fashion-MNIST数据\n",
    ">推荐直接下载数据文件，pytorch有相应方法可以直接下载，但速度实在是感人，且经常断开连接。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['短袖圆领T恤', '裤子', '套衫', '连衣裙', '外套', '凉鞋', '衬衫', '运动鞋','包', '短靴']\n",
    "text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"\n",
    "    Load MNIST data from `path`\n",
    "    \"\"\"\n",
    "    labels_path = os.path.join(path, '%s-labels-idx1-ubyte.gz'% kind)\n",
    "    images_path = os.path.join(path, '%s-images-idx3-ubyte.gz'% kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8, offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8, offset=16).reshape(len(labels), 784)\n",
    "    \n",
    "    features = transforms.ToTensor()(images)  # (h, w, c) -> (c, h, w)\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    return features[0], labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_minibatch_data(batch_size, kind='train', is_one_hot=False, is_mnist=False, path=\"../dataset/\"):\n",
    "    \"\"\"\n",
    "    Prepare mini batch data for training\n",
    "    \"\"\"\n",
    "    if is_mnist:\n",
    "        c_path = path + 'mnist/'\n",
    "    else:\n",
    "        c_path = path + 'fashion_mnist/'\n",
    "    features, labels = load_mnist(c_path, kind=kind)\n",
    "    \n",
    "    if is_one_hot:\n",
    "        hot_labels = torch.zeros(features.shape[0], 10)\n",
    "        x_indices = np.arange(features.shape[0]).tolist()\n",
    "        y_indices = labels.byte().tolist()\n",
    "        hot_labels[x_indices, y_indices] = 1\n",
    "        \n",
    "        dataset = TensorDataset(features, hot_labels)\n",
    "    else:\n",
    "        dataset = TensorDataset(features, labels)\n",
    "\n",
    "    return DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 将标签转换为one-hot向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = load_mnist(path=\"../dataset/fashion_mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_labels = torch.zeros(features.shape[0], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_labels[0, 9] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hot_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_indices = np.arange(features.shape[0]).tolist()\n",
    "y_indices = labels.byte().tolist()\n",
    "hot_labels[x_indices, y_indices] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hot_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, hat_y, is_one_hot=False):\n",
    "    '''\n",
    "    y: 标签, one-hot\n",
    "    hat_y: 标签预测概率, one-hot\n",
    "    is_one_hot: y是否为one-hot形式\n",
    "    '''\n",
    "    if is_one_hot:\n",
    "        precision = torch.sum(torch.max(y, axis=1)[1] == torch.max(hat_y, axis=1)[1]).numpy() / y.shape[0]\n",
    "    else:\n",
    "        precision = torch.sum((y == torch.max(hat_y, axis=1)[1]).byte()).numpy() / y.shape[0]\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 方法1: 随机梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter = load_minibatch_data(batch_size, is_one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
    "W1 = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_hiddens)), dtype=torch.float)\n",
    "b1 = torch.zeros(num_hiddens, dtype=torch.float)\n",
    "W2 = torch.tensor(np.random.normal(0, 0.01, (num_hiddens, num_outputs)), dtype=torch.float)\n",
    "b2 = torch.zeros(num_outputs, dtype=torch.float)\n",
    "\n",
    "params = [W1, W2, b1, b2]\n",
    "for param in params:\n",
    "    param.requires_grad_(requires_grad=True)\n",
    "\n",
    "layers = [\n",
    "    (W1, b1, torch.relu),\n",
    "    (W2, b2, torch.softmax)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 0.3805\n",
      "epoch 2, loss: 0.2741\n",
      "epoch 3, loss: 0.3683\n",
      "epoch 4, loss: 0.2741\n",
      "epoch 5, loss: 0.2853\n",
      "epoch 6, loss: 0.2134\n",
      "epoch 7, loss: 0.2012\n",
      "epoch 8, loss: 0.2177\n",
      "epoch 9, loss: 0.1990\n",
      "epoch 10, loss: 0.2138\n",
      "epoch 11, loss: 0.1655\n",
      "epoch 12, loss: 0.1791\n",
      "epoch 13, loss: 0.1448\n",
      "epoch 14, loss: 0.2116\n",
      "epoch 15, loss: 0.2084\n",
      "epoch 16, loss: 0.1900\n",
      "epoch 17, loss: 0.2357\n",
      "epoch 18, loss: 0.1780\n",
      "epoch 19, loss: 0.1894\n",
      "epoch 20, loss: 0.1809\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 20\n",
    "lr = 0.1\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    for t_x, t_y in train_iter:\n",
    "        hat_y = multilayer_perceptron(t_x, layers)\n",
    "        l = cross_entropy(t_y, hat_y.float())\n",
    "        l.backward()  # 计算损失函数在 W 上的梯度\n",
    "        for param in params:\n",
    "            param.data.sub_(lr*param.grad)\n",
    "            param.grad.data.zero_()\n",
    "    with torch.no_grad():\n",
    "        print(f'epoch {epoch + 1}, loss: {l.numpy():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.8624333333333334\n",
      "Test accuracy: 0.8405\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = load_mnist(path=\"../dataset/fashion_mnist\", kind='train')\n",
    "test_features, test_labels = load_mnist(path=\"../dataset/fashion_mnist\", kind='t10k')\n",
    "\n",
    "print(f'Train accuracy: {accuracy(train_labels, multilayer_perceptron(train_features, layers))}') \n",
    "print(f'Test accuracy: {accuracy(test_labels, multilayer_perceptron(test_features, layers))}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 方法2：`torch.nn.Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module): \n",
    "    def __init__(self, feature_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__() \n",
    "        self.hidden = nn.Linear(feature_size, hidden_size, bias=True)\n",
    "        self.output = nn.Linear(hidden_size, output_size, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.hidden(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
    "net = MLP(num_inputs, num_hiddens, num_outputs)\n",
    "# 初始化网络参数和偏置\n",
    "for params in net.parameters():\n",
    "    nn.init.normal_(params, mean=0, std=0.01)\n",
    "# CrossEntropyLoss包括softmax运算和交叉熵损失计算的函数, 所以输出层不应施加torch.nn.softmax()\n",
    "loss = nn.CrossEntropyLoss()  \n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter2 = load_minibatch_data(batch_size, is_one_hot=False)  # nn.CrossEntropyLoss()接收标量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 0.6035717129707336, train_accuracy~0.7851, test_accuracy~0.7731\n",
      "epoch 2, loss: 0.391079306602478, train_accuracy~0.8359, test_accuracy~0.8212\n",
      "epoch 3, loss: 0.5444250702857971, train_accuracy~0.8507, test_accuracy~0.8384\n",
      "epoch 4, loss: 0.40586695075035095, train_accuracy~0.8488, test_accuracy~0.8352\n",
      "epoch 5, loss: 0.4218519628047943, train_accuracy~0.8569, test_accuracy~0.8400\n",
      "epoch 6, loss: 0.41245242953300476, train_accuracy~0.8484, test_accuracy~0.8332\n",
      "epoch 7, loss: 0.5560639500617981, train_accuracy~0.8111, test_accuracy~0.7982\n",
      "epoch 8, loss: 0.33906814455986023, train_accuracy~0.8602, test_accuracy~0.8448\n",
      "epoch 9, loss: 0.29989343881607056, train_accuracy~0.8676, test_accuracy~0.8509\n",
      "epoch 10, loss: 0.3122353255748749, train_accuracy~0.8751, test_accuracy~0.8589\n",
      "epoch 11, loss: 0.24854837357997894, train_accuracy~0.8850, test_accuracy~0.8674\n",
      "epoch 12, loss: 0.3852939307689667, train_accuracy~0.8699, test_accuracy~0.8494\n",
      "epoch 13, loss: 0.35979029536247253, train_accuracy~0.8559, test_accuracy~0.8363\n",
      "epoch 14, loss: 0.3158089220523834, train_accuracy~0.8763, test_accuracy~0.8578\n",
      "epoch 15, loss: 0.25983795523643494, train_accuracy~0.8918, test_accuracy~0.8688\n",
      "epoch 16, loss: 0.26246389746665955, train_accuracy~0.8954, test_accuracy~0.8725\n",
      "epoch 17, loss: 0.17619794607162476, train_accuracy~0.8858, test_accuracy~0.8596\n",
      "epoch 18, loss: 0.37475302815437317, train_accuracy~0.8886, test_accuracy~0.8651\n",
      "epoch 19, loss: 0.255256325006485, train_accuracy~0.8646, test_accuracy~0.8421\n",
      "epoch 20, loss: 0.2687790095806122, train_accuracy~0.8530, test_accuracy~0.8283\n",
      "epoch 21, loss: 0.29606863856315613, train_accuracy~0.8703, test_accuracy~0.8484\n",
      "epoch 22, loss: 0.26470309495925903, train_accuracy~0.9072, test_accuracy~0.8820\n",
      "epoch 23, loss: 0.2341843992471695, train_accuracy~0.8984, test_accuracy~0.8720\n",
      "epoch 24, loss: 0.27704304456710815, train_accuracy~0.8974, test_accuracy~0.8721\n",
      "epoch 25, loss: 0.43117406964302063, train_accuracy~0.9030, test_accuracy~0.8735\n",
      "epoch 26, loss: 0.20465226471424103, train_accuracy~0.8742, test_accuracy~0.8428\n",
      "epoch 27, loss: 0.22659337520599365, train_accuracy~0.9081, test_accuracy~0.8787\n",
      "epoch 28, loss: 0.25046390295028687, train_accuracy~0.9116, test_accuracy~0.8831\n",
      "epoch 29, loss: 0.2440011352300644, train_accuracy~0.9119, test_accuracy~0.8800\n",
      "epoch 30, loss: 0.15454931557178497, train_accuracy~0.9085, test_accuracy~0.8764\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    for t_x, t_y in train_iter2:\n",
    "        l = loss(net(t_x), t_y)  # 计算当前批量的交叉熵损失\n",
    "        trainer.zero_grad()  # 参数梯度清零\n",
    "        l.backward()  # 反向传播，计算梯度\n",
    "        trainer.step()  # 更新参数\n",
    "        \n",
    "    with torch.no_grad():  # 不计算梯度，加速损失函数的运算\n",
    "        print(f\"epoch {epoch+1}, loss: {l.data.numpy()},\", end=' ')\n",
    "        train_accuracy = accuracy(train_labels, net(train_features))\n",
    "        test_accuracy = accuracy(test_labels, net(test_features))\n",
    "        print(f'train_accuracy: {train_accuracy:.4f}, test_accuracy: {test_accuracy:.4f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 方法3：`torch.nn.Sequential`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
    "net = nn.Sequential(\n",
    "        nn.Linear(num_inputs, num_hiddens),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(num_hiddens, num_outputs), \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in net.parameters():\n",
    "    torch.nn.init.normal_(params, mean=0, std=0.01)\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 0.4193057715892792, train_accuracy: 0.7743, test_accuracy: 0.7585\n",
      "epoch 2, loss: 0.4675431251525879, train_accuracy: 0.8144, test_accuracy: 0.8008\n",
      "epoch 3, loss: 0.5446958541870117, train_accuracy: 0.8357, test_accuracy: 0.8212\n",
      "epoch 4, loss: 0.2699097692966461, train_accuracy: 0.8400, test_accuracy: 0.8250\n",
      "epoch 5, loss: 0.28863516449928284, train_accuracy: 0.8482, test_accuracy: 0.8324\n",
      "epoch 6, loss: 0.30187591910362244, train_accuracy: 0.8483, test_accuracy: 0.8366\n",
      "epoch 7, loss: 0.3859211504459381, train_accuracy: 0.8761, test_accuracy: 0.8586\n",
      "epoch 8, loss: 0.31160327792167664, train_accuracy: 0.8772, test_accuracy: 0.8577\n",
      "epoch 9, loss: 0.20481641590595245, train_accuracy: 0.8780, test_accuracy: 0.8501\n",
      "epoch 10, loss: 0.35441073775291443, train_accuracy: 0.8841, test_accuracy: 0.8627\n",
      "epoch 11, loss: 0.20062850415706635, train_accuracy: 0.8941, test_accuracy: 0.8714\n",
      "epoch 12, loss: 0.3176487386226654, train_accuracy: 0.8596, test_accuracy: 0.8398\n",
      "epoch 13, loss: 0.17333675920963287, train_accuracy: 0.8887, test_accuracy: 0.8674\n",
      "epoch 14, loss: 0.2567870616912842, train_accuracy: 0.8849, test_accuracy: 0.8578\n",
      "epoch 15, loss: 0.22335200011730194, train_accuracy: 0.8962, test_accuracy: 0.8714\n",
      "epoch 16, loss: 0.2955656945705414, train_accuracy: 0.8827, test_accuracy: 0.8585\n",
      "epoch 17, loss: 0.2311239093542099, train_accuracy: 0.9088, test_accuracy: 0.8782\n",
      "epoch 18, loss: 0.32722142338752747, train_accuracy: 0.8430, test_accuracy: 0.8140\n",
      "epoch 19, loss: 0.15865953266620636, train_accuracy: 0.9089, test_accuracy: 0.8738\n",
      "epoch 20, loss: 0.259070485830307, train_accuracy: 0.9028, test_accuracy: 0.8697\n",
      "epoch 21, loss: 0.22159580886363983, train_accuracy: 0.9134, test_accuracy: 0.8804\n",
      "epoch 22, loss: 0.19405318796634674, train_accuracy: 0.8901, test_accuracy: 0.8567\n",
      "epoch 23, loss: 0.17938737571239471, train_accuracy: 0.9133, test_accuracy: 0.8765\n",
      "epoch 24, loss: 0.2439679354429245, train_accuracy: 0.9152, test_accuracy: 0.8800\n",
      "epoch 25, loss: 0.18590782582759857, train_accuracy: 0.8909, test_accuracy: 0.8534\n",
      "epoch 26, loss: 0.21158064901828766, train_accuracy: 0.9080, test_accuracy: 0.8686\n",
      "epoch 27, loss: 0.19622403383255005, train_accuracy: 0.9137, test_accuracy: 0.8788\n",
      "epoch 28, loss: 0.19260156154632568, train_accuracy: 0.9194, test_accuracy: 0.8845\n",
      "epoch 29, loss: 0.20432768762111664, train_accuracy: 0.9031, test_accuracy: 0.8639\n",
      "epoch 30, loss: 0.15298844873905182, train_accuracy: 0.9255, test_accuracy: 0.8829\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    for t_x, t_y in train_iter2:\n",
    "        l = loss(net(t_x), t_y)  # 计算当前批量的交叉熵损失\n",
    "        optimizer.zero_grad()  # 参数梯度清零\n",
    "        l.backward()  # 反向传播，计算梯度\n",
    "        optimizer.step()  # 更新参数\n",
    "        \n",
    "    with torch.no_grad():  # 不计算梯度，加速损失函数的运算\n",
    "        print(f\"epoch {epoch+1}, loss: {l.data.numpy()},\", end=' ')\n",
    "        train_accuracy = accuracy(train_labels, net(train_features))\n",
    "        test_accuracy = accuracy(test_labels, net(test_features))\n",
    "        print(f'train_accuracy: {train_accuracy:.4f}, test_accuracy: {test_accuracy:.4f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 正向传播和计算图\n",
    "\n",
    "基于反向传播（`back-propagation`）算法的自动求梯度极大简化了深度学习模型训练算法的实现。本节我们将使用数学和计算图（`computational graph`）两个方式来描述正向传播和反向传播。具体来说，我们将以带$L_2$范数正则化的含单隐藏层的多层感知机为样例模型解释正向传播和反向传播。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 正向传播\n",
    "\n",
    "正向传播是指对神经网络沿着从输入层到输出层的顺序，依次计算并存储模型的中间变量（包括输出）。为简单起见，假设输入是一个特征为$\\boldsymbol{x} \\in \\mathbb{R}^d$的样本，且不考虑偏差项，那么中间变量\n",
    "\n",
    "$$\\boldsymbol{z} = \\boldsymbol{W}^{(1)} \\boldsymbol{x},$$\n",
    "\n",
    "其中$\\boldsymbol{W}^{(1)} \\in \\mathbb{R}^{h \\times d}$是隐藏层的权重参数。把中间变量$\\boldsymbol{z} \\in \\mathbb{R}^h$输入按元素运算的激活函数$\\phi$后，将得到向量长度为$h$的隐藏层变量\n",
    "\n",
    "$$\\boldsymbol{h} = \\phi (\\boldsymbol{z}).$$\n",
    "\n",
    "隐藏层变量$\\boldsymbol{h}$也是一个中间变量。假设输出层参数只有权重$\\boldsymbol{W}^{(2)} \\in \\mathbb{R}^{q \\times h}$，可以得到向量长度为$q$的输出层变量\n",
    "\n",
    "$$\\boldsymbol{o} = \\boldsymbol{W}^{(2)} \\boldsymbol{h}.$$\n",
    "\n",
    "假设损失函数为$\\ell$，且样本标签为$y$，可以计算出单个数据样本的损失项\n",
    "\n",
    "$$L = \\ell(\\boldsymbol{o}, y).$$\n",
    "\n",
    "根据$L_2$范数正则化的定义，给定超参数$\\lambda$，正则化项即\n",
    "\n",
    "$$s = \\frac{\\lambda}{2} \\left(\\|\\boldsymbol{W}^{(1)}\\|_F^2 + \\|\\boldsymbol{W}^{(2)}\\|_F^2\\right),$$\n",
    "\n",
    "其中矩阵的Frobenius范数等价于将矩阵变平为向量后计算$L_2$范数。最终，模型在给定的数据样本上带正则化的损失为\n",
    "\n",
    "$$J = L + s.$$\n",
    "\n",
    "我们将$J$称为有关给定数据样本的目标函数，并在以下的讨论中简称目标函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y, hat_y):  # 平均平方误差\n",
    "    '''\n",
    "    y: scalar\n",
    "    hat_y: 多层感知机的输出\n",
    "    '''\n",
    "    return torch.mean((hat_y - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y, hat_y):  # 交叉熵\n",
    "    '''\n",
    "    y: one-hot向量\n",
    "    hat_y: softmax之后对应概率向量，多层感知机的输出\n",
    "    '''\n",
    "    crossEnt = -torch.dot(y.reshape(-1), torch.log2(hat_y).reshape(-1)) / y.shape[0]  # 展开成1维，点积\n",
    "    return crossEnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 正向传播的计算图\n",
    "\n",
    "我们通常绘制计算图来可视化运算符和变量在计算中的依赖关系。图2绘制了本节中样例模型正向传播的计算图，其中左下角是输入，右上角是输出。可以看到，图中箭头方向大多是向右和向上，其中方框代表变量，圆圈代表运算符，箭头表示从输入到输出之间的依赖关系。\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"400\" src=\"../pictures/6.2.svg\"/>\n",
    "</div>\n",
    "<div align=center> 图4 正向传播的计算图</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 误差反向传播\n",
    "\n",
    "反向传播指的是计算神经网络参数梯度的方法。总的来说，反向传播依据微积分中的链式法则，沿着从输出层到输入层的顺序，依次计算并存储目标函数有关神经网络各层的中间变量以及参数的梯度。对输入或输出$\\mathsf{X}, \\mathsf{Y}, \\mathsf{Z}$为任意形状张量的函数$\\mathsf{Y}=f(\\mathsf{X})$和$\\mathsf{Z}=g(\\mathsf{Y})$，通过链式法则，我们有\n",
    "\n",
    "$$\\frac{\\partial \\mathsf{Z}}{\\partial \\mathsf{X}} = \\text{prod}\\left(\\frac{\\partial \\mathsf{Z}}{\\partial \\mathsf{Y}}, \\frac{\\partial \\mathsf{Y}}{\\partial \\mathsf{X}}\\right),$$\n",
    "\n",
    "其中$\\text{prod}$运算符将根据两个输入的形状，在必要的操作（如转置和互换输入位置）后对两个输入做乘法。\n",
    "\n",
    "回顾一下本节中样例模型，它的参数是$\\boldsymbol{W}^{(1)}$和$\\boldsymbol{W}^{(2)}$，因此反向传播的目标是计算$\\partial J/\\partial \\boldsymbol{W}^{(1)}$和$\\partial J/\\partial \\boldsymbol{W}^{(2)}$。我们将应用链式法则依次计算各中间变量和参数的梯度，其计算次序与前向传播中相应中间变量的计算次序恰恰相反。首先，分别计算目标函数$J=L+s$有关损失项$L$和正则项$s$的梯度\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial L} = 1, \\quad \\frac{\\partial J}{\\partial s} = 1.$$\n",
    "\n",
    "其次，依据链式法则计算目标函数有关输出层变量的梯度$\\partial J/\\partial \\boldsymbol{o} \\in \\mathbb{R}^q$：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\boldsymbol{o}}\n",
    "= \\text{prod}\\left(\\frac{\\partial J}{\\partial L}, \\frac{\\partial L}{\\partial \\boldsymbol{o}}\\right)\n",
    "= \\frac{\\partial L}{\\partial \\boldsymbol{o}}.\n",
    "$$\n",
    "\n",
    "\n",
    "接下来，计算正则项有关两个参数的梯度：\n",
    "\n",
    "$$\\frac{\\partial s}{\\partial \\boldsymbol{W}^{(1)}} = \\lambda \\boldsymbol{W}^{(1)},\\quad\\frac{\\partial s}{\\partial \\boldsymbol{W}^{(2)}} = \\lambda \\boldsymbol{W}^{(2)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们可以计算最靠近输出层的模型参数的梯度$\\partial J/\\partial \\boldsymbol{W}^{(2)} \\in \\mathbb{R}^{q \\times h}$。依据链式法则，得到\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\boldsymbol{W}^{(2)}}\n",
    "= \\text{prod}\\left(\\frac{\\partial J}{\\partial \\boldsymbol{o}}, \\frac{\\partial \\boldsymbol{o}}{\\partial \\boldsymbol{W}^{(2)}}\\right) + \\text{prod}\\left(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial \\boldsymbol{W}^{(2)}}\\right)\n",
    "= \\frac{\\partial J}{\\partial \\boldsymbol{o}} \\boldsymbol{h}^\\top + \\lambda \\boldsymbol{W}^{(2)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\frac{\\partial \\boldsymbol{o}}{\\partial \\boldsymbol{W}^{(2)}}$为向量对矩阵的求导。\n",
    "> 因为$\\boldsymbol{o}={\\boldsymbol{W}^{(2)}}^\\top \\boldsymbol{h}$, 逐元素计算$\\frac{\\partial \\boldsymbol{o_i}}{\\partial \\boldsymbol{W}^{(2)}_{j,k}}$\n",
    ">\n",
    "> 由于$\\boldsymbol{o_i}={\\boldsymbol{W}^{(2)}}^\\top_{i,1}\\boldsymbol{h}_1+{\\boldsymbol{W}^{(2)}}^\\top_{i,2}\\boldsymbol{h}_2+...+{\\boldsymbol{W}^{(2)}}^\\top_{i,k}\\boldsymbol{h}_k+...+{\\boldsymbol{W}^{(2)}}^\\top_{i,h}\\boldsymbol{h}_h$, 可得$\\frac{\\partial \\boldsymbol{o_i}}{\\partial {\\boldsymbol{W}^{(2)}}^\\top_{i,k}}=\\boldsymbol{h}_k$. \n",
    ">\n",
    "> 即, 当$j=i$时, $\\frac{\\partial \\boldsymbol{o_i}}{\\partial {\\boldsymbol{W}^{(2)}}^\\top_{j,k}}=\\boldsymbol{h}_k$, $\\frac{\\partial \\boldsymbol{o_i}}{\\partial {\\boldsymbol{W}^{(2)}}^\\top_{j:}}=\\boldsymbol{h}$.\n",
    "> \n",
    "> 当$j\\neq i$时, $\\frac{\\partial \\boldsymbol{o_i}}{\\partial {\\boldsymbol{W}^{(2)}}^\\top_{j,k}}=0$, $\\frac{\\partial \\boldsymbol{o_i}}{\\partial {\\boldsymbol{W}^{(2)}}^\\top_{j:}}=\\boldsymbol{0}$.\n",
    ">\n",
    "> 整合成$F=[\\boldsymbol{H}_1, \\boldsymbol{H}_2, ..., \\boldsymbol{H}_q]$, 其中$F$为3维张量, $\\boldsymbol{H}_i=[\\boldsymbol{0}, \\boldsymbol{0}, ..., \\boldsymbol{h^T}, ..., \\boldsymbol{0}]^T$为矩阵, 第i行为$\\boldsymbol{h}$\n",
    "> \n",
    "> F可写成更加紧凑的矩阵形式$G=[\\boldsymbol{h^T}, ..., \\boldsymbol{h^T}, ..., \\boldsymbol{h^T}]^T$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "沿着输出层向隐藏层继续反向传播，隐藏层变量的梯度$\\partial J/\\partial \\boldsymbol{h} \\in \\mathbb{R}^h$可以这样计算：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\boldsymbol{h}}\n",
    "= \\text{prod}\\left(\\frac{\\partial J}{\\partial \\boldsymbol{o}}, \\frac{\\partial \\boldsymbol{o}}{\\partial \\boldsymbol{h}}\\right)\n",
    "= {\\boldsymbol{W}^{(2)}}^\\top \\frac{\\partial J}{\\partial \\boldsymbol{o}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\frac{\\partial \\boldsymbol{o}}{\\partial \\boldsymbol{h}}$为向量对向量求导。因为$\\boldsymbol{o}={\\boldsymbol{W}^{(2)}}^\\top \\boldsymbol{h}$, 即需要计算$\\boldsymbol{o_i}$对$\\boldsymbol{h_j}$中的偏导数。\n",
    "> \n",
    "> 由于$\\boldsymbol{o_i}={\\boldsymbol{W}^{(2)}}^\\top_{i,1}\\boldsymbol{h}_1+{\\boldsymbol{W}^{(2)}}^\\top_{i,1}\\boldsymbol{h}_1+...+{\\boldsymbol{W}^{(2)}}^\\top_{i,j}\\boldsymbol{h}_j+...+{\\boldsymbol{W}^{(2)}}^\\top_{i,h}\\boldsymbol{h}_h$, 可得$\\frac{\\partial \\boldsymbol{o_i}}{\\partial \\boldsymbol{h_j}}={\\boldsymbol{W}^{(2)}}^\\top_{i,j}$.\n",
    ">\n",
    "> 因此, 有$\\frac{\\partial \\boldsymbol{o}}{\\partial \\boldsymbol{h}}={\\boldsymbol{W}^{(2)}}^\\top$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于激活函数$\\phi$是按元素运算的，中间变量$\\boldsymbol{z}$的梯度$\\partial J/\\partial \\boldsymbol{z} \\in \\mathbb{R}^h$的计算需要使用按元素乘法符$\\odot$：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\boldsymbol{z}}\n",
    "= \\text{prod}\\left(\\frac{\\partial J}{\\partial \\boldsymbol{h}}, \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}}\\right)\n",
    "= \\frac{\\partial J}{\\partial \\boldsymbol{h}} \\odot \\phi'\\left(\\boldsymbol{z}\\right).\n",
    "$$\n",
    "\n",
    "最终，我们可以得到最靠近输入层的模型参数的梯度$\\partial J/\\partial \\boldsymbol{W}^{(1)} \\in \\mathbb{R}^{h \\times d}$。依据链式法则，得到\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\boldsymbol{W}^{(1)}}\n",
    "= \\text{prod}\\left(\\frac{\\partial J}{\\partial \\boldsymbol{z}}, \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}^{(1)}}\\right) + \\text{prod}\\left(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial \\boldsymbol{W}^{(1)}}\\right)\n",
    "= \\frac{\\partial J}{\\partial \\boldsymbol{z}} \\boldsymbol{x}^\\top + \\lambda \\boldsymbol{W}^{(1)}\n",
    "= (\\phi'\\left(\\boldsymbol{z}\\right)\\odot {\\boldsymbol{W}^{(2)}}^\\top \\frac{\\partial J}{\\partial \\boldsymbol{o}}) \\boldsymbol{x}^\\top + \\lambda \\boldsymbol{W}^{(1)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_crossentropy_w1(X, y, W1, W2):\n",
    "    '''\n",
    "    交叉熵对w1和w2的导数\n",
    "    X: 输入, m*d\n",
    "    y: 标签, m*o\n",
    "    W1: 输入层至隐藏层权重矩阵, d*h\n",
    "    W2: 隐藏层至输出层权重矩阵, h*o\n",
    "    '''\n",
    "    z = X@W1\n",
    "    h = torch.sigmoid(z)\n",
    "    o = h@W2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 训练深度学习模型\n",
    "在训练深度学习模型时，正向传播和反向传播之间相互依赖。下面我们仍然以本节中的样例模型分别阐述它们之间的依赖关系。\n",
    "\n",
    "一方面，正向传播的计算可能依赖于模型参数的当前值，而这些模型参数是在反向传播的梯度计算后通过优化算法迭代的。例如，计算正则化项$s = (\\lambda/2) \\left(\\|\\boldsymbol{W}^{(1)}\\|_F^2 + \\|\\boldsymbol{W}^{(2)}\\|_F^2\\right)$依赖模型参数$\\boldsymbol{W}^{(1)}$和$\\boldsymbol{W}^{(2)}$的当前值，而这些当前值是优化算法最近一次根据反向传播算出梯度后迭代得到的。\n",
    "\n",
    "另一方面，反向传播的梯度计算可能依赖于各变量的当前值，而这些变量的当前值是通过正向传播计算得到的。举例来说，参数梯度$\\partial J/\\partial \\boldsymbol{W}^{(2)} = (\\partial J / \\partial \\boldsymbol{o}) \\boldsymbol{h}^\\top + \\lambda \\boldsymbol{W}^{(2)}$的计算需要依赖隐藏层变量的当前值$\\boldsymbol{h}$。这个当前值是通过从输入层到输出层的正向传播计算并存储得到的。\n",
    "\n",
    "因此，在模型参数初始化完成后，我们交替地进行正向传播和反向传播，并根据反向传播计算的梯度迭代模型参数。既然我们在反向传播中使用了正向传播中计算得到的中间变量来避免重复计算，那么这个复用也导致正向传播结束后不能立即释放中间变量内存。这也是训练要比预测占用更多内存的一个重要原因。另外需要指出的是，这些中间变量的个数大体上与网络层数线性相关，每个变量的大小跟批量大小和输入个数也是线性相关的，它们是导致较深的神经网络使用较大批量训练时更容易超内存的主要原因。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考资料\n",
    "1. 邱锡鹏. 神经网络与机器学习. 2020.\n",
    "2. [阿斯顿·张、李沐、扎卡里 C. 立顿、亚历山大 J. 斯莫拉等. 动手学深度学习. 2020.](https://github.com/d2l-ai/d2l-zh)\n",
    "3. [动手学深度学习(Pytoch实现)](https://github.com/ShusenTang/Dive-into-DL-PyTorch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
