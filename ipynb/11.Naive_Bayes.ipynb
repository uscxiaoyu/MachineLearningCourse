{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四. 朴素贝叶斯法\n",
    "\n",
    "朴素贝叶斯法(naive Bayes)法是基于贝叶斯定理与特征条件独立假设的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入输出的联合概率分布；然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。朴素贝叶斯实现简单，学习与预测的效率都很高，是一种常用方法。\n",
    "\n",
    "以下内容包括朴素贝叶斯的学习与分类、朴素贝叶斯的参数估计方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 朴素贝叶斯的学习与分类\n",
    "\n",
    "### 1.1 基本方法\n",
    "输入空间$\\chi\\subset \\mathbf{R^n}$为n维向量的集合，输出空间为类标记集合$Y=\\{c_1,c_2,...,c_K\\}$。输入空间为特征向量$x\\in X$，输出类标记$y\\in Y$。$X$是定义在输入空间$\\mathbb{\\chi}$上的随机变量，$Y$是定义在输出空间$\\mathbb{Y}$上的随机变量。$P(X,Y)$是$X$和$Y$的联合概率分布。训练数据集为\n",
    "$$\n",
    "T = \\{(x_1,y_1), (x_2, y_2), ..., (x_N, y_N)\\}\n",
    "$$\n",
    "由$P(X,Y)$独立同分布产生。\n",
    "\n",
    "假定$Y$的先验概率分布为\n",
    "$$\n",
    "P(Y=c_k),k=1,2,...,K\n",
    "$$\n",
    "条件概率分布\n",
    "$$\n",
    "P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k),k=1,2,...,K\n",
    "$$\n",
    "据此学习到联合概率分布$P(X,Y)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "朴素贝叶斯对条件概率分布作了条件独立性的假设，因此被称为“朴素”贝叶斯。具体而言\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(X=x|Y=c_k)&=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k)\\\\\n",
    "&=\\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)\n",
    "\\end{aligned}\n",
    "$$\n",
    "朴素贝叶斯学习到的是生成数据的机制，所以属于生成模型。条件独立假设等于是说用于分类的特征在类确定的条件下是条件独立的。这一假设使朴素贝叶斯变得简单，但有时会牺牲一定的分类准确率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "朴树贝叶斯分类时，对给定的输入x，通过学习到的模型计算后验分布$P(Y=c_k|X=x)$，将后验概率最大的类作为x的类输出。后验概率计算根据贝叶斯定理得出\n",
    "$$\n",
    "P(Y=c_k|X=x)=\\frac{P(X=x|Y=c_k)P(Y=c_k)}{\\sum_kP(X=x|Y=c_k)P(Y=c_k)}\n",
    "$$\n",
    "结合条件独立性假设，有\n",
    "$$\n",
    "P(Y=c_k|X=x)=\\frac{P(Y=c_k)\\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)}{\\sum_kP(Y=c_k)\\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)},k=1,2...,K\n",
    "$$\n",
    "以上即为朴素贝叶斯的基本公式。于是，朴素贝叶斯分类器可表示为\n",
    "$$\n",
    "y=f(x)=\\mathrm{arg}\\max_{c_k}\\frac{P(Y=c_k)\\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)}{\\sum_kP(Y=c_k)\\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)}\n",
    "$$\n",
    "注意到，分母对所有的$c_k$都是相同的，所以以上最大化问题可以简化为\n",
    "$$\n",
    "y=f(x)=\\mathrm{arg}\\max_{c_k}P(Y=c_k)\\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 朴素贝叶斯的参数估计\n",
    "\n",
    "### 2.1 极大似然估计\n",
    "先验概率$P(Y=c_k)$极大似然估计为\n",
    "$$\n",
    "P(Y=c_k)=\\frac{\\sum_{i=1}^{N}I(y_i=c_k)}{N},k=1,2,...,K\n",
    "$$\n",
    "设第j个特征$x^{(j)}$可能取值的集合为$\\{a_{j1},a_{j2},...,a_{jS_j}\\}$，条件概率$P(X^{(j)}=a_{jl}|Y=c_k)$的极大似然估计是\n",
    "$$\n",
    "P(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^{N}I(x_i^{(j)}=a_{jl,y_i=c_k})}{\\sum_{i=1}^NI(y_i=c_k)}\\\\\n",
    "j=1,2,...,n;\\\\\n",
    "l=1,2,...,S_j;\\\\\n",
    "k=1,2,...,K\n",
    "$$\n",
    "上式中，$x_i^{(j)}$是第i个样本的第j个特征；$a_{jl}$是第j个特征可能取的第l个值；I为指示函数。\n",
    "$\\sum_{i=1}^NI(y_i=c_k)$为类别$c_k$的频数；$\\sum_{i=1}^{N}I(x_i^{(j)}=a_{jl,y_i=c_k})$为类别$c_k$中属性$x_i^{(j)}$的值为$a_{jl}$的频数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.2 学习与分类算法\n",
    "\n",
    ">算法4.1  朴素贝叶斯算法（naive Bayes algorithm）\n",
    ">\n",
    ">输入: 训练集$T = \\{(x_1,y_1), (x_2, y_2), ..., (x_N, y_N)\\}$, 其中$x_i=(x_i^{(1)},x_i^{(2)},...,x_i^{(n)})^T$，$x_i^{(j)}$表示第i个样本的第j个特征，$x_i^{(j)}\\in \\{a_{j1},a_{j2},...,a_{jn}\\}$，$a_{jl}$是第j个特征可能取的第l个值，$j=1,2,...,n;l=1,2,...,S_j;y_i\\in\\{c_1,c_2,...,c_K\\}$；实例$x$;\n",
    ">\n",
    ">输出：实例$x$的分类。\n",
    ">\n",
    ">算法过程：\n",
    ">\n",
    ">(1) 计算先验概率及条件概率\n",
    ">$$\n",
    "P(Y=c_k)=\\frac{\\sum_{i=1}^{N}I(y_i=c_k)}{N},k=1,2,...,K\\\\\n",
    "P(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^{N}I(x_i^{(j)}=a_{jl,y_i=c_k})}{\\sum_{i=1}^NI(y_i=c_k)}\\\\\n",
    "j=1,2,...,n;\\\\\n",
    "l=1,2,...,S_j;\\\\\n",
    "k=1,2,...,K\n",
    "$$\n",
    ">(2) 对于给定的实例$x_i=(x_i^{(1)},x_i^{(2)},...,x_i^{(n)})^T$计算\n",
    ">$$\n",
    "P(Y=c_k)\\prod_jP(X^{(j)}=x^{(j)}|Y=c_k),k=1,2,...,K\n",
    "$$\n",
    ">(3) 确定实例$x$的类\n",
    ">$$\n",
    "y=f(x)=\\mathrm{arg}\\max_{c_k}P(Y=c_k)\\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython import display\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例4.1数据集\n",
    "X = np.array([\n",
    "    ['1', 'S'],\n",
    "    ['1', 'M'],\n",
    "    ['1', 'M'],\n",
    "    ['1', 'S'],\n",
    "    ['1', 'S'],\n",
    "    ['2', 'S'],\n",
    "    ['2', 'M'],\n",
    "    ['2', 'M'],\n",
    "    ['2', 'L'],\n",
    "    ['2', 'L'],\n",
    "    ['3', 'L'],\n",
    "    ['3', 'M'],\n",
    "    ['3', 'M'],\n",
    "    ['3', 'L'],\n",
    "    ['3', 'L']])\n",
    "y = np.array(['-1', '-1', '1', '1', '-1', '-1', '-1', '1', '1', '1', '1', '1', '1', '1', '-1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 朴素贝叶斯训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naiveBayes_ml(X, y):\n",
    "    Xy = np.concatenate([X, y.reshape(-1, 1)], axis=1)\n",
    "    freq_y = {c: np.sum(y == c) for c in np.unique(y)}\n",
    "    p_y = {f'y={c}': np.sum(y == c) / len(y) for c in set(y)}  # p(y)\n",
    "    freq_xy = {}  # p(x_i, y) -> (i, x, y): frequency\n",
    "    for i in range(X.shape[1]):\n",
    "        for value in Xy[:, [i,-1]]:\n",
    "            key = tuple([i, *value])\n",
    "            freq_xy[key] = freq_xy.get(key, 0) + 1\n",
    "\n",
    "    p_xy = {f\"x{key[0]}={key[1]}|y={key[2]}\": freq_xy[key] / freq_y[key[-1]] for key in freq_xy} # p(x|y)\n",
    "    return {**p_y, **p_xy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = train_naiveBayes_ml(X, y)\n",
    "cates = set(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 根据训练结果构造分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naiveBayes_classifier(x, P=P, cates=cates):\n",
    "    p_y_x = []\n",
    "    for c in cates:\n",
    "        likeli = np.prod([P[f\"y={c}\"], *[P[f\"x{i}={x[i]}|y={c}\"] for i in range(len(x))]])\n",
    "        p_y_x.append([c, likeli])\n",
    "    \n",
    "    return p_y_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', 'S']对应的类别为:-1\n"
     ]
    }
   ],
   "source": [
    "x = ['2', 'S']\n",
    "p_y_x = naiveBayes_classifier(x)\n",
    "predict_y = max(p_y_x, key=lambda x: x[1])[0]\n",
    "print(f'{x}对应的类别为:{predict_y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 贝叶斯估计\n",
    "\n",
    "用极大似然估计可能会出现所要估计的概率值为0的情况，将影响到后验概率的计算结果。解决这一问题的方法之一是采用贝叶斯估计。具体而言，条件概率的贝叶斯估计是\n",
    "$$\n",
    "P_{\\lambda}(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^N I(x_i^{(j)}=a_{jl}, y_i=c_k)+\\lambda}{\\sum_{i=1}^N I(y_i=c_k)+S_j\\lambda}\n",
    "$$\n",
    "式中$\\lambda\\geq 0$, $S_j$为属性j取值的可能数量。\n",
    "> $\\lambda$为概率更新中先验概率的权重系数\n",
    "\n",
    "等价于在随机变量各个取值的频数上赋予一个正数$\\lambda> 0$。当$\\lambda=0$时即为极大似然估计。常取$\\lambda=1$，这时称为拉普拉斯平滑。\n",
    "> 在无任何可用样本时，$P(X^{(j)}=a_{jl}|Y=c_k) = \\frac{1}{S_j}$，即各个属性的出现是等概率的。\n",
    "\n",
    "显然，对于任何$l=1,2,...,S_j,k=1,2,...,K$，有\n",
    "$$\n",
    "P_{\\lambda}(X^{(j)}=a_{jl}|Y=c_k)>0\\\\\n",
    "\\sum_{l=1}^{S_j}P_{\\lambda}(X^{(j)}=a_{jl}|Y=c_k)=1\n",
    "$$\n",
    "即$P_{\\lambda}(X^{(j)})$是一种概率分布。类别的先验概率的贝叶斯估计是\n",
    "$$\n",
    "P_{\\lambda}(Y=c_k)=\\frac{\\sum_{i=1}^N I(y_i=c_k)+\\lambda}{N+K\\lambda}\n",
    "$$\n",
    "式中$N$为样本数量，$K$为类别取值的可能数量。\n",
    "> 在无任何可用样本时，$P_{\\lambda}(Y=c_k)=\\frac{1}{K}$，即各类样本的出现是等概率的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [['1', '2', '3'], ['S', 'M', 'L'], ['-1', '1']]  # 属性和类别的取值范围\n",
    "def train_naiveBayes_nb(X, y, lamb=1, values=values):\n",
    "    Xy = np.concatenate([X, y.reshape(-1, 1)], axis=1)\n",
    "    freq_y = {c: np.sum(y == c) for c in values[-1]}  # 样本中y的频数\n",
    "    all_c = len(y) + len(values[-1]) * lamb\n",
    "    p_y = {f'y={c}': (freq_y[c] + lamb) / all_c for c in freq_y}  # p(y)\n",
    "    keys = []\n",
    "    for i in range(len(values) - 1):  # 某一子属性值和某一类别值的组合 (i, value, c)\n",
    "        for v in values[i]:\n",
    "            for y in values[-1]:\n",
    "                keys.append((i, v, y))\n",
    "    \n",
    "    freq_xy = {}  # 样本频数 p(x_i, y) -> (i, x, y): frequency\n",
    "    for k in keys:\n",
    "        freq_xy[k] = np.sum([1 for value in Xy[:, [k[0], -1]] if value.tolist() == [k[1], k[2]]])\n",
    "\n",
    "    p_xy = {f\"x{key[0]}={key[1]}|y={key[2]}\": (freq_xy[key] + lamb) / (freq_y[key[-1]] + len(values[key[0]])*lamb) \n",
    "            for key in freq_xy} # p(x|y)\n",
    "    return {**p_y, **p_xy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_P = train_naiveBayes_nb(X, y, lamb=1)  # 贝叶斯估计值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', 'S']对应的类别为:-1\n"
     ]
    }
   ],
   "source": [
    "x = ['2', 'S']\n",
    "p_y_x = naiveBayes_classifier(x, P=nb_P)\n",
    "predict_y = max(p_y_x, key=lambda x: x[1])[0]\n",
    "print(f'{x}对应的类别为:{predict_y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', 0.0326797385620915], ['-1', 0.06100217864923746]]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_y_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `scikit-learn`实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_NB = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['-1', '-1', '1', '1', '-1', '-1', '-1', '1', '1', '1', '1', '1',\n",
       "       '1', '1', '-1'], dtype='<U2')"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
