{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNTrainer:\n",
    "    '''\n",
    "    DDQN的训练器\n",
    "    '''\n",
    "    def __init__(self, env, policy_net, target_net, optimizer, device='cpu'):\n",
    "        self.env = env\n",
    "        self.policy_net = policy_net  # policy_net的参数会被target_net的参数\n",
    "        self.target_net = target_net  # target_net的参数会被policy_net的参数\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.memory = deque(maxlen=10000)  # 经验回放池\n",
    "        self.batch_size = 128  # batch大小\n",
    "        self.gamma = 0.999  # 折扣因子\n",
    "        self.epsilon = 0.9  # epsilon-greedy策略中的epsilon\n",
    "        self.epsilon_decay = 0.995  # epsilon的衰减率\n",
    "        self.min_epsilon = 0.05  # epsilon的最小值\n",
    "        self.num_episodes = 1000  # 训练的回合数\n",
    "\n",
    "    def select_action(self, state):  # epsilon-greedy策略\n",
    "        if random.random() > self.epsilon:\n",
    "            with torch.no_grad():\n",
    "                return self.policy_net(state).max(1)[1].view(1, 1).to(self.device)\n",
    "        else:\n",
    "            return torch.tensor([[random.randrange(self.env.action_space.n)]], device=self.device, dtype=torch.long)\n",
    "\n",
    "    def optimize_model(self):  # 利用Q-learning更新网络参数\n",
    "        if len(self.memory) < self.batch_size:  # 如果记忆中的样本数量小于batch_size，不优化，直接返回\n",
    "            return\n",
    "        \n",
    "        transitions = random.sample(self.memory, self.batch_size)  # 从记忆中随机采样\n",
    "        batch = tuple(zip(*transitions))\n",
    "\n",
    "        # 非最终状态的mask: 以下两行代码的目的是创建一个布尔类型的张量 non_final_mask，\n",
    "        # 用于表示批处理中的状态是否为非最终状态。\n",
    "        # 它通过对批处理中的每个状态进行检查，如果状态不为 None，则对应的张量元素为 True，否则为 False。\n",
    "        # 这个张量将在强化学习的深度 Q 网络 (DQN) 中使用，用于选择下一个动作和计算目标 Q 值。\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch[3])), device=self.device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch[3] if s is not None])\n",
    "\n",
    "        # 将batch转换为张量\n",
    "        state_batch = torch.cat(batch[0])  # 将batch中的state拼接成一个张量\n",
    "        action_batch = torch.cat(batch[1])  # 将batch中的action拼接成一个张量\n",
    "        reward_batch = torch.cat(batch[2])  # 将batch中的reward拼接成一个张量\n",
    "\n",
    "        # 计算hat_q的值\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "        # 计算Q(s_{t+1}, a)的值\n",
    "        next_state_values = torch.zeros(self.batch_size, device=self.device)\n",
    "        next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()\n",
    "        # 计算hat_y的值\n",
    "        expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
    "\n",
    "        # 计算损失\n",
    "        loss = F.mse_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # 更新参数\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "    def train(self):  # 训练\n",
    "        for i_episode in range(self.num_episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            state = torch.tensor([state], device=self.device, dtype=torch.float32)  # 直接转换为张量\n",
    "            sum_reward = 0\n",
    "            for _ in range(1000):  # 假设每个回合的最大步数为1000\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done, _, _ = self.env.step(action.item())  # 执行动作\n",
    "                reward = torch.tensor([reward], device=self.device, dtype=torch.float32)\n",
    "                if done:\n",
    "                    next_state = None\n",
    "                else:\n",
    "                    next_state = torch.tensor([next_state], device=self.device, dtype=torch.float32)\n",
    "                \n",
    "                self.memory.append((state, action, reward, next_state))\n",
    "                state = next_state if not done else None\n",
    "                self.optimize_model()\n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "                sum_reward += reward.item()\n",
    "            \n",
    "            # 更新target_net的参数\n",
    "            if i_episode % 10 == 0:\n",
    "                self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "            \n",
    "            # 打印每个回合的reward\n",
    "            if (i_episode + 1) % 50 == 0:\n",
    "                print('Episode {} reward: {}'.format(i_episode + 1, sum_reward))\n",
    "            \n",
    "            # 更新epsilon\n",
    "            self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cartpole游戏环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.00906778,  0.01233419, -0.01864976,  0.00012788], dtype=float32), {})\n",
      "1\n",
      "[ 0.00931446  0.20771858 -0.0186472  -0.29838043] 1.0 False False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoyu/miniforge3/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "# Setting up the environment and the trainer\n",
    "env = gym.make('CartPole-v1')\n",
    "state = env.reset()\n",
    "print(state)\n",
    "\n",
    "action = env.action_space.sample()\n",
    "print(action)\n",
    "\n",
    "next_state, reward, done, truancated, _ = env.step(env.action_space.sample())\n",
    "print(next_state, reward, done, truancated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 训练agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s7/5sc_9jgx26b11xl1803v3c040000gn/T/ipykernel_1288/2224593507.py:64: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1678454847243/work/torch/csrc/utils/tensor_new.cpp:248.)\n",
      "  state = torch.tensor([state], device=self.device, dtype=torch.float32)  # 直接转换为张量\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50 reward: 36.0\n",
      "Episode 100 reward: 59.0\n",
      "Episode 150 reward: 56.0\n",
      "Episode 200 reward: 240.0\n",
      "Episode 250 reward: 141.0\n",
      "Episode 300 reward: 90.0\n",
      "Episode 350 reward: 276.0\n",
      "Episode 400 reward: 161.0\n",
      "Episode 450 reward: 284.0\n",
      "Episode 500 reward: 343.0\n",
      "Episode 550 reward: 148.0\n",
      "Episode 600 reward: 1000.0\n",
      "Episode 650 reward: 129.0\n",
      "Episode 700 reward: 136.0\n",
      "Episode 750 reward: 343.0\n",
      "Episode 800 reward: 1000.0\n",
      "Episode 850 reward: 464.0\n",
      "Episode 900 reward: 482.0\n",
      "Episode 950 reward: 241.0\n",
      "Episode 1000 reward: 89.0\n",
      "Episode 1050 reward: 103.0\n",
      "Episode 1100 reward: 93.0\n",
      "Episode 1150 reward: 94.0\n",
      "Episode 1200 reward: 97.0\n",
      "Episode 1250 reward: 483.0\n",
      "Episode 1300 reward: 301.0\n",
      "Episode 1350 reward: 106.0\n",
      "Episode 1400 reward: 345.0\n",
      "Episode 1450 reward: 405.0\n",
      "Episode 1500 reward: 324.0\n",
      "Episode 1550 reward: 93.0\n",
      "Episode 1600 reward: 54.0\n",
      "Episode 1650 reward: 501.0\n",
      "Episode 1700 reward: 103.0\n",
      "Episode 1750 reward: 243.0\n",
      "Episode 1800 reward: 1000.0\n",
      "Episode 1850 reward: 1000.0\n",
      "Episode 1900 reward: 11.0\n",
      "Episode 1950 reward: 1000.0\n",
      "Episode 2000 reward: 477.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = env.action_space.n\n",
    "policy_net = DQN(input_dim, output_dim).to('cpu')\n",
    "target_net = DQN(input_dim, output_dim).to('cpu')\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "\n",
    "trainer = DQNTrainer(env, policy_net, target_net, optimizer)\n",
    "trainer.train()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 测试agent的表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, policy_net, num_episodes=100):\n",
    "    policy_net.eval()  # 设置为评估模式\n",
    "    total_rewards = []\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = torch.tensor([state], dtype=torch.float32)\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "            # 选择最佳动作\n",
    "            action = policy_net(state).max(1)[1].view(1, 1)\n",
    "            next_state, reward, done, truancated, _ = env.step(action.item())\n",
    "            episode_reward += reward\n",
    "            state = torch.tensor([next_state], dtype=torch.float32)\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "    \n",
    "    avg_reward = sum(total_rewards) / len(total_rewards)\n",
    "    print(f'平均奖励：{avg_reward:.2f}')\n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均奖励：203.97\n"
     ]
    }
   ],
   "source": [
    "test_rewards = evaluate_agent(env, policy_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 对决网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对决网络结构（Dueling Network Architecture），分别估计状态值（V）和优势函数（A）。在对决网络中，网络的输出被分解为两个部分：一个是单一的状态值 \\( V(s) \\)，另一个是每个动作的优势 \\( A(s, a) \\)。最终的动作值 \\( Q(s, a) \\) 通过合并这两部分得到：\\( Q(s, a) = V(s) + (A(s, a) - \\text{mean}(A(s, a))) \\)，其中 \\(\\text{mean}(A(s, a))\\) 是所有动作的平均优势，用来稳定训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        # Common feature layer\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        # Value stream\n",
    "        self.fc_value = nn.Linear(64, 1)\n",
    "        # Advantage stream\n",
    "        self.fc_advantage = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Common layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # Value and Advantage streams\n",
    "        value = self.fc_value(x)\n",
    "        advantage = self.fc_advantage(x)\n",
    "        # Subtract the mean of the advantage to stabilize training\n",
    "        Q = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "        \n",
    "        return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 说明：\n",
    "- **共享层（Shared Layers）**：`fc1` 和 `fc2` 用于从输入状态提取特征，这些特征被用于后续的价值（V）和优势（A）计算。\n",
    "- **价值流（Value Stream）**：一个输出层 `fc_value`，用于计算状态值 \\( V(s) \\)，这是一个标量，表示在给定状态下的预期回报。\n",
    "- **优势流（Advantage Stream）**：一个输出层 `fc_advantage`，输出每个动作的优势值 \\( A(s, a) \\)。优势函数表示选择每个动作相对于平均水平的额外价值。\n",
    "- **合并Q值（Combining Q Values）**：在输出时，动作的总Q值由状态值和优势值合成，通过从每个动作的优势中减去所有动作优势的平均值来保持数值稳定。\n",
    "\n",
    "这种架构可以更有效地学习在不同状态下哪些动作更重要，因为它区分了状态的整体价值和每个动作相对于其他动作的额外价值。这通常可以在需要评估大量潜在动作的复杂环境中提高性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50 reward: 26.0\n",
      "Episode 100 reward: 45.0\n",
      "Episode 150 reward: 90.0\n",
      "Episode 200 reward: 65.0\n",
      "Episode 250 reward: 29.0\n",
      "Episode 300 reward: 120.0\n",
      "Episode 350 reward: 101.0\n",
      "Episode 400 reward: 613.0\n",
      "Episode 450 reward: 1000.0\n",
      "Episode 500 reward: 127.0\n",
      "Episode 550 reward: 122.0\n",
      "Episode 600 reward: 1000.0\n",
      "Episode 650 reward: 213.0\n",
      "Episode 700 reward: 234.0\n",
      "Episode 750 reward: 40.0\n",
      "Episode 800 reward: 543.0\n",
      "Episode 850 reward: 156.0\n",
      "Episode 900 reward: 97.0\n",
      "Episode 950 reward: 34.0\n",
      "Episode 1000 reward: 230.0\n",
      "Episode 1050 reward: 107.0\n",
      "Episode 1100 reward: 129.0\n",
      "Episode 1150 reward: 1000.0\n",
      "Episode 1200 reward: 134.0\n",
      "Episode 1250 reward: 303.0\n",
      "Episode 1300 reward: 23.0\n",
      "Episode 1350 reward: 12.0\n",
      "Episode 1400 reward: 169.0\n",
      "Episode 1450 reward: 293.0\n",
      "Episode 1500 reward: 1000.0\n",
      "Episode 1550 reward: 237.0\n",
      "Episode 1600 reward: 1000.0\n",
      "Episode 1650 reward: 1000.0\n",
      "Episode 1700 reward: 1000.0\n",
      "Episode 1750 reward: 86.0\n",
      "Episode 1800 reward: 44.0\n",
      "Episode 1850 reward: 48.0\n",
      "Episode 1900 reward: 32.0\n",
      "Episode 1950 reward: 121.0\n",
      "Episode 2000 reward: 112.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = env.action_space.n\n",
    "policy_net = DuelingDQN(input_dim, output_dim).to('cpu')  # policy_net\n",
    "target_net = DuelingDQN(input_dim, output_dim).to('cpu')  # target_net\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "\n",
    "trainer = DQNTrainer(env, policy_net, target_net, optimizer)\n",
    "trainer.train()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均奖励：115.56\n"
     ]
    }
   ],
   "source": [
    "test_rewards = evaluate_agent(env, policy_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 噪声网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了将传统的DQN模型改造为噪声`DQN（Noisy DQN）`，我们需要引入带有参数化噪声的网络层。这样的层可以直接整合到策略网络中，以代替传统的`ε-greedy`策略进行探索。在`Noisy DQN`中，网络本身会预测在决策过程中应该添加的噪声量，这样可以更有效地探索动作空间而不依赖于外部的随机决策过程。\n",
    "\n",
    "- 实现Noisy Layer\n",
    "\n",
    "首先，我们定义一个噪声层，该层将在每次调用时生成新的噪声。我们可以通过扩展torch.nn.Linear来创建这样的层，使其包含噪声参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class NoisyLinear(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, sigma_init=0.017, bias=True):\n",
    "        # 先调用super初始化基础功能\n",
    "        super(NoisyLinear, self).__init__(in_features, out_features, bias=bias)\n",
    "\n",
    "        # 然后添加sigma参数和epsilon缓存\n",
    "        self.sigma_weight = nn.Parameter(torch.full((out_features, in_features), sigma_init))\n",
    "        self.register_buffer(\"epsilon_weight\", torch.zeros(out_features, in_features))\n",
    "        if bias:\n",
    "            self.sigma_bias = nn.Parameter(torch.full((out_features,), sigma_init))\n",
    "            self.register_buffer(\"epsilon_bias\", torch.zeros(out_features))\n",
    "\n",
    "        # 重置所有参数\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        super(NoisyLinear, self).reset_parameters()\n",
    "        std = 0.5 / math.sqrt(self.weight.size(1))\n",
    "        nn.init.uniform_(self.weight, -std, std)\n",
    "        if self.bias is not None:\n",
    "            nn.init.uniform_(self.bias, -std, std)\n",
    "            \n",
    "        nn.init.constant_(self.sigma_weight, 0.017)\n",
    "        if self.bias is not None:\n",
    "            nn.init.constant_(self.sigma_bias, 0.017)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.epsilon_weight.normal_()\n",
    "        self.epsilon_bias.normal_()\n",
    "        \n",
    "        return F.linear(x, self.weight + self.sigma_weight * self.epsilon_weight,\n",
    "                        self.bias + self.sigma_bias * self.epsilon_bias if self.bias is not None else None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 集成噪声层道DQN模型中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyDQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(NoisyDQN, self).__init__()\n",
    "        # 使用NoisyLinear替换标准全连接层\n",
    "        self.fc1 = NoisyLinear(input_dim, 64)\n",
    "        self.fc2 = NoisyLinear(64, 64)\n",
    "        self.fc3 = NoisyLinear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "    def reset_noise(self):\n",
    "        \"\"\"重置所有噪声层的噪声参数\"\"\"\n",
    "        self.fc1.reset_parameters()\n",
    "        self.fc2.reset_parameters()\n",
    "        self.fc3.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyDQNTrainer:\n",
    "    def __init__(self, env, policy_net, target_net, optimizer, device='cpu'):\n",
    "        self.env = env\n",
    "        self.policy_net = policy_net\n",
    "        self.target_net = target_net\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.batch_size = 128\n",
    "        self.gamma = 0.999\n",
    "        self.num_episodes = 5000\n",
    "\n",
    "    def select_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            return self.policy_net(state).max(1)[1].view(1, 1).to(self.device)\n",
    "\n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        transitions = random.sample(self.memory, self.batch_size)\n",
    "        batch = tuple(zip(*transitions))\n",
    "\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch[3])), device=self.device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch[3] if s is not None])\n",
    "\n",
    "        state_batch = torch.cat(batch[0])\n",
    "        action_batch = torch.cat(batch[1])\n",
    "        reward_batch = torch.cat(batch[2])\n",
    "\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "        next_state_values = torch.zeros(self.batch_size, device=self.device)\n",
    "        next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()\n",
    "        expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
    "\n",
    "        loss = F.mse_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def train(self):\n",
    "        for i_episode in range(self.num_episodes):\n",
    "            self.policy_net.reset_noise()  # Reset noise at the beginning of each episode\n",
    "            state = self.env.reset()\n",
    "            state = torch.tensor([state], device=self.device, dtype=torch.float32)\n",
    "            for t in range(1000):  # Assume max steps per episode is 1000\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action.item())\n",
    "                reward = torch.tensor([reward], device=self.device, dtype=torch.float32)\n",
    "                \n",
    "                if done:\n",
    "                    next_state = None\n",
    "                else:\n",
    "                    next_state = torch.tensor([next_state], device=self.device, dtype=torch.float32)\n",
    "                \n",
    "                self.memory.append((state, action, reward, next_state))\n",
    "                state = next_state if not done else None\n",
    "                self.optimize_model()\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            # Update the target network\n",
    "            if i_episode % 10 == 0:\n",
    "                self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "            \n",
    "            # Print episode stats\n",
    "            if (i_episode + 1) % 100 == 0:\n",
    "                print(f'Episode {i_episode + 1} completed')\n",
    "\n",
    "    def evaluate(self, num_episodes=10):\n",
    "        self.policy_net.eval()  # Set the network to evaluation mode\n",
    "        total_rewards = []\n",
    "        for i_episode in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            state = torch.tensor([state], device=self.device, dtype=torch.float32)\n",
    "            total_reward = 0\n",
    "            while True:\n",
    "                action = self.select_action(state)\n",
    "                state, reward, done, _ = self.env.step(action.item())\n",
    "                total_reward += reward\n",
    "                state = torch.tensor([state], device=self.device, dtype=torch.float32)\n",
    "                if done:\n",
    "                    break\n",
    "            total_rewards.append(total_reward)\n",
    "            print(f'Episode {i_episode + 1}: Total Reward = {total_reward}')\n",
    "        self.policy_net.train()  # Set the network back to train mode\n",
    "        return total_rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoisyLinear' object has no attribute 'sigma_weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/s7/5sc_9jgx26b11xl1803v3c040000gn/T/ipykernel_28826/3540867907.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moutput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpolicy_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNoisyDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtarget_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNoisyDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtarget_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/s7/5sc_9jgx26b11xl1803v3c040000gn/T/ipykernel_28826/4130329543.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dim, output_dim)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNoisyDQN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m# 使用NoisyLinear替换标准全连接层\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNoisyLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNoisyLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNoisyLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/s7/5sc_9jgx26b11xl1803v3c040000gn/T/ipykernel_28826/1689998742.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_features, out_features, sigma_init, bias)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.017\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# 先调用super初始化基础功能\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNoisyLinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# 然后添加sigma参数和epsilon缓存\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bias'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/s7/5sc_9jgx26b11xl1803v3c040000gn/T/ipykernel_28826/1689998742.py\u001b[0m in \u001b[0;36mreset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.017\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma_bias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.017\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1614\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1615\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoisyLinear' object has no attribute 'sigma_weight'"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = env.action_space.n\n",
    "\n",
    "policy_net = NoisyDQN(input_dim, output_dim).to('cpu')\n",
    "target_net = NoisyDQN(input_dim, output_dim).to('cpu')\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "trainer = NoisyDQNTrainer(env, policy_net, target_net, optimizer)\n",
    "trainer.train()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
