---
marp: true
# size: 4:3
paginate: true
headingDivider: 0
# header: '**ç¬¬4ç«  çº¿æ€§æ¨¡å‹**'
---
<!-- fit -->
# è¯¯å·®åå‘ä¼ æ’­ç®—æ³•

---
# ä¸»è¦å†…å®¹

- å¼•è¨€
- è¯¯å·®åå‘ä¼ æ’­çš„4ä¸ªæ–¹ç¨‹
- è¯¯å·®åå‘ä¼ æ’­ç®—æ³•
    - ç®—æ³•è¿‡ç¨‹
    - åŸºäº`pytorch`å®ç°
- å‚æ•°åˆå§‹åŒ–
- æ­£åˆ™åŒ–
- èåˆæƒé‡è¡°å‡å’Œdropoutçš„å‰é¦ˆç¥ç»ç½‘ç»œå®ç°

---
# å¼•è¨€

- ç¥ç»â½¹ç»œä¸»è¦ä½¿â½¤æ¢¯åº¦ä¸‹é™ç®—æ³•å­¦ä¹ æƒé‡å’Œåç½®ã€‚ä½†æ˜¯ï¼Œè¿™â¾¥è¿˜ç•™ä¸‹äº†â¼€ä¸ªé—®é¢˜ï¼šæˆ‘ä»¬å¹¶æ²¡æœ‰è®¨è®ºå¦‚ä½•è®¡ç®—ä»£ä»·å‡½æ•°çš„æ¢¯åº¦ã€‚è¿™æ˜¯å¾ˆâ¼¤çš„ç¼ºå¤±ï¼æˆ‘ä»¬æ¥ä¸‹æ¥å­¦ä¹ è®¡ç®—è¿™äº›æ¢¯åº¦çš„å¿«é€Ÿç®—æ³•ï¼Œä¹Ÿå°±æ˜¯**åå‘ä¼ æ’­ï¼ˆ`Back propagation`ï¼‰**ã€‚

- åå‘ä¼ æ’­ç®—æ³•æœ€åˆåœ¨1970å¹´ä»£è¢«æåŠï¼Œä½†æ˜¯â¼ˆä»¬ç›´åˆ°`D. Rumelhart, G. Hinton & R. Williams (1986)` çš„è‘—å[è®ºâ½‚](https://www.nature.com/articles/323533a0)ä¸­æ‰è®¤è¯†åˆ°è¿™ä¸ªç®—æ³•çš„é‡è¦æ€§ã€‚è¿™ç¯‡è®ºâ½‚æè¿°äº†å¯¹â¼€äº›ç¥ç»â½¹ç»œåå‘ä¼ æ’­è¦â½ä¼ ç»Ÿçš„â½…æ³•æ›´å¿«ï¼Œè¿™ä½¿å¾—ä½¿â½¤ç¥ç»â½¹ç»œæ¥è§£å†³ä¹‹å‰â½†æ³•å®Œæˆçš„é—®é¢˜å˜å¾—å¯â¾ã€‚

- ç°åœ¨ï¼Œåå‘ä¼ æ’­ç®—æ³•å·²ç»æ˜¯ç¥ç»â½¹ç»œå­¦ä¹ çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚

---
# å¼•è¨€
- ä¸ºäº†åˆ©ç”¨æ¢¯åº¦ä¸‹é™æ³•è®­ç»ƒç¥ç»ç½‘ç»œï¼Œå¿…é¡»é¦–å…ˆè®¡ç®—ç¥ç»ç½‘ç»œä¸­æŸå¤±å‡½æ•°å…³äºæƒé‡å’Œåç½®çš„æ¢¯åº¦ã€‚

- å¦‚ä½•è®¡ç®—è¿™äº›æ¢¯åº¦ï¼Ÿæœ‰ä»¥ä¸‹ä¸¤ç§æ–¹æ³•
    - æ•°å€¼æ–¹æ³•
    $$
    \frac{\partial C}{\partial w_{ij}}\approx \frac{C(w_{ij}+\eta\Delta e_{ij}) - C(w_{ij})}{\eta}
    $$
    å…¶ä¸­$\eta>0$æ˜¯ä¸€ä¸ªå¾ˆå°çš„æ­£æ•°ï¼Œè€Œ$e_{ij}$æ˜¯å•ä½å‘é‡ã€‚è¿™ç§æ–¹æ³•éœ€è¦é‡å¤å¤šæ¬¡è¿è¡Œå‰å‘ä¼ æ’­è®¡ç®—$C$, æ•ˆç‡å¤ªä½ã€‚
    - ç¬¦å·å¾®åˆ†ï¼šé“¾å¼æ³•åˆ™

---
# å¼•è¨€

- åå‘ä¼ æ’­ç®—æ³•æ¶‰åŠä¸¤ä¸ªè¿‡ç¨‹çš„è¿­ä»£:
    - ä¿¡æ¯ç»ç”±ç¥ç»å…ƒçš„å‰å‘ä¼ é€’
    - è¯¯å·®ç»ç”±ç¥ç»å…ƒçš„åå‘ä¼ é€’
    - æ ¸å¿ƒæŠ€æœ¯æ˜¯åŸºäºé“¾å¼æ³•åˆ™ä»¥åŠä½¿ç”¨ä¸­é—´å˜é‡*è¯¯å·®é¡¹*

- åå‘ä¼ æ’­ç®—æ³•çš„åº”ç”¨
    - å‰é¦ˆç¥ç»ç½‘ç»œï¼šå¤šå±‚æ„ŸçŸ¥æœºã€å·ç§¯ç¥ç»ç½‘ç»œ
    - è®°å¿†ç¥ç»ç½‘ç»œï¼šå¾ªç¯ç¥ç»ç½‘ç»œã€LSTM
    - å›¾ç¥ç»ç½‘ç»œ
    - å¤§è¯­è¨€æ¨¡å‹: `Transformer, BERT, GPT`

---
# è¯¯å·®åå‘ä¼ æ’­çš„4ä¸ªæ–¹ç¨‹

- å‡è®¾è¾“å…¥å±‚è¡¨ç¤ºä¸º$x$ï¼Œè¾“å…¥å±‚åˆ°ç¬¬1ä¸ªéšè—å±‚çš„æƒé‡çŸ©é˜µè¡¨ç¤ºä¸º$w^1$ï¼Œç¬¬1ä¸ªéšè—å±‚çš„å‡€å€¼ä¸º$z^1$ï¼Œæ¿€æ´»å€¼ä¸º$a^1$ï¼›è¾“å‡ºå±‚çš„å‡€å€¼ä¸º$z^L$, æ¿€æ´»å€¼ä¸º$a^L$ï¼Œå³éšè—å±‚åŠ è¾“å‡ºå±‚çš„æ•°é‡ä¸º$L$ã€‚ç¬¬$l$å±‚çš„ç¥ç»å…ƒä¸ªæ•°ä¸º$M_l$ã€‚

- $w_{jk}^l$è¡¨ç¤ºç¬¬$(l-1)$å±‚çš„ç¬¬$k$ä¸ªç¥ç»å…ƒåˆ°$l$å±‚çš„ç¬¬$j$ä¸ªç¥ç»å…ƒçš„è¿æ¥ä¸Šçš„æƒé‡ï¼Œ$l-1$å’Œ$l$å±‚é—´çš„æƒé‡çŸ©é˜µä¸º$w^l\in \mathbb{R}^{\mathbf{M}_l\times \mathbf{M}_{l-1}}$ã€‚

- $b_j^l$è¡¨ç¤ºç¬¬$l-1$å±‚åˆ°ç¬¬$l$å±‚ç¬¬$j$ä¸ªç¥ç»å…ƒçš„åç½®ï¼Œ$l$å±‚çš„åç½®å‘é‡è¡¨ç¤ºä¸º$b^l\in \mathbb{R}^{\mathbf{M}_l}$ã€‚

- $z_j^l$è¡¨ç¤ºç¬¬$l$å±‚ç¬¬$j$ä¸ªç¥ç»å…ƒçš„å‡€å€¼ï¼Œ$l$å±‚çš„å‡€å€¼å‘é‡è¡¨ç¤ºä¸º$z^l\in \mathbb{R}^{\mathbf{M}_l}$ã€‚

- $\sigma^l$è¡¨ç¤ºåº”ç”¨äºç¬¬$l$å±‚ç¥ç»å…ƒå‡€å€¼çš„æ¿€æ´»å‡½æ•°ã€‚

- $a_j^l$è¡¨ç¤ºç¬¬$l$å±‚ç¬¬$j$ä¸ªç¥ç»å…ƒçš„æ¿€æ´»å€¼ï¼Œ$l$å±‚çš„æ¿€æ´»å€¼è¡¨ç¤ºä¸º$a^l\in \mathbb{R}^{\mathbf{M}_l}$ã€‚

---
![bg right:90% fit](./pictures/7.back_propagation.svg)


---
# è¯¯å·®åå‘ä¼ æ’­çš„4ä¸ªæ–¹ç¨‹

$$
\begin{aligned}
\delta^L&=\frac{\partial{C}}{\partial{a^L}}\odot R'(z^L)  \text{\ \ \ \ \ \ \ \ (BP.1)}\\
\delta^l&=((w^{l+1})^T\delta^{l+1})\odot\sigma'(z^l) \text{\ \ \ \ \ \ \ \ (BP.2)}\\
\frac{\partial{C}}{\partial{b_j^l}}&=\delta_j^l \text{ or } \frac{\partial{C}}{\partial{b^l}}=\delta^l \in \mathbb{R}^{M_l} \text{\ \ \ \ \ \ \ \ (BP.3)}\\
\frac{\partial{C}}{\partial{w_{jk}^l}}&=a_k^{l-1}\delta_j^l \text{ or } \frac{\partial{C}}{\partial{w^l}}=\delta^l (a^{l-1})^T\in \mathbb{R}^{M_l\times M_{l-1}}\text{\ \ \ \ \ \ \ \ (BP.4)}
\end{aligned}
$$

---
# è¯¯å·®åå‘ä¼ æ’­çš„4ä¸ªæ–¹ç¨‹

- åå‘ä¼ æ’­å…¶å®æ˜¯å¯¹æƒé‡å’Œåç½®å˜åŒ–å½±å“ä»£ä»·å‡½æ•°è¿‡ç¨‹çš„ç†è§£ã€‚æœ€ç»ˆç›®æ ‡å…¶å®å°±æ˜¯è®¡ç®—åå¯¼æ•°$\frac{\partial{C}}{\partial{w^l_{jk}}}$ å’Œ$\frac{\partial{C}}{\partial{b^l}}$ã€‚

- ä½†æ˜¯ä¸ºäº†è®¡ç®—è¿™äº›å€¼ï¼Œæˆ‘ä»¬â¾¸å…ˆå¼•â¼Šâ¼€ä¸ªä¸­é—´é‡ï¼Œ$\delta_j^l$ï¼Œè¿™ä¸ªæˆ‘ä»¬ç§°ä¸ºç¬¬$l$å±‚ç¬¬$j$ä¸ªç¥ç»å…ƒä¸Šçš„è¯¯å·®ã€‚
>ä»è®¡ç®—ä¸Šæ¥è®²ï¼Œ$\delta_j^l$åœ¨ä¸‹ä¸€å±‚çš„æƒé‡å’Œåç½®æ¢¯åº¦çš„è®¡ç®—ä¸­é‡å¤åˆ©ç”¨ï¼Œå› æ­¤å°†å…¶å®šä¹‰å¹¶ä¿å­˜ä¸‹æ¥å¯è§„é¿é‡å¤è®¡ç®—ï¼Œæå¤§å‡å°‘è®¡ç®—é‡

- åå‘ä¼ æ’­å°†ç»™å‡ºè®¡ç®—è¯¯å·®$\delta_j^l$çš„æµç¨‹ï¼Œç„¶åå°†å…¶å…³è”åˆ°è®¡ç®—$\frac{\partial{C}}{\partial{w^l_{jk}}}$ å’Œ$\frac{\partial{C}}{\partial{b^l}}$ä¸Šã€‚


---
# è¯¯å·®åå‘ä¼ æ’­çš„4ä¸ªæ–¹ç¨‹

- å‡å®šåœ¨å‰é¦ˆä¿¡æ¯ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œ$l$å±‚ç¬¬$i$ä¸ªç¥ç»å…ƒçš„å‡€å€¼$z_i^l$å¢åŠ äº†$\Delta z_i^l$ï¼Œåˆ™æœ€ç»ˆæŸå¤±å‡½æ•°çš„å¢é‡åº”ä¸º$\frac{\partial C}{\partial z_i^l}\Delta z_i^l$ã€‚

- **å¯¹$\frac{\partial C}{\partial z_j^l}$æ˜¯$l$å±‚ç¥ç»å…ƒ$j$çš„è¯¯å·®åº¦é‡çš„å¯å‘å¼è®¤è¯†**: å‡è®¾$\frac{\partial C}{\partial z_i^l}$å–å¾ˆå¤§çš„å€¼ï¼ˆæˆ–æ­£æˆ–è´Ÿï¼‰ï¼Œé‚£ä¹ˆå¯ä»¥é€šè¿‡é€‰æ‹©ä¸$\frac{\partial C}{\partial z_i^l}$ç›¸åç¬¦å·çš„$\Delta z_i^l$æ¥é™ä½ä»£ä»·ã€‚ç›¸åï¼Œå¦‚æœ$\frac{\partial C}{\partial z_i^l}$æ¥è¿‘0ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¹¶ä¸èƒ½é€šè¿‡æ‰°åŠ¨å¸¦æƒè¾“â¼Š$z^l_j$æ¥æ”¹å–„å¤ªå¤šä»£ä»·ã€‚

- æŒ‰ç…§ä¸Šâ¾¯çš„æè¿°ï¼Œæˆ‘ä»¬å®šä¹‰$l$å±‚çš„ç¬¬$j$ä¸ªç¥ç»å…ƒä¸Šçš„è¯¯å·®$\delta_i^l$ä¸ºï¼š
$$
\delta_j^l = \frac{\partial C}{\partial z_j^l}
$$

---
ç¥ç»å…ƒçš„è¯¯å·®é¡¹
![bg right:90% fit](./pictures/7.error_z.svg)

---
# è¯¯å·®åå‘ä¼ æ’­çš„4ä¸ªæ–¹ç¨‹

## *æŒ‰ç…§æˆ‘ä»¬é€šå¸¸çš„æƒ¯ä¾‹ï¼Œæˆ‘ä»¬ä½¿â½¤$\delta^l$è¡¨â½°å…³è”äº$l$å±‚çš„è¯¯å·®å‘é‡ã€‚*

## *åå‘ä¼ æ’­ä¼šæä¾›ç»™æˆ‘ä»¬â¼€ç§è®¡ç®—æ¯å±‚çš„$\delta^l$çš„â½…æ³•ï¼Œç„¶åå°†è¿™äº›è¯¯å·®å’Œæœ€ç»ˆæˆ‘ä»¬éœ€è¦çš„é‡$\frac{\partial C}{\partial w_{jk}^l}$å’Œ$\frac{\partial C}{\partial b_j^l}$è”ç³»èµ·æ¥ã€‚*

---
# è¯¯å·®åå‘ä¼ æ’­çš„4ä¸ªæ–¹ç¨‹
- `(BP.1)`. è¾“å‡ºå±‚çš„è¯¯å·®é¡¹$\delta^L$, æ¯ä¸ªå…ƒç´ å®šä¹‰å¦‚ä¸‹ï¼š
$$
\delta_j^L=\frac{\partial C}{\partial a_j^L}\sigma'(z_j^L)
$$

>å³å¼ç¬¬â¼€ä¸ªé¡¹$\frac{\partial C}{\partial a_j^L}$è¡¨â½°Céšç€jå±‚è¾“å‡ºæ¿€æ´»å€¼çš„å˜åŒ–â½½å˜åŒ–çš„é€Ÿåº¦ã€‚å‡å¦‚$C$ä¸å¤ªä¾èµ–â¼€ä¸ªç‰¹å®šçš„è¾“å‡ºç¥ç»å…ƒjï¼Œé‚£ä¹ˆ$\frac{\partial C}{\partial a_j^L}$å°±ä¼šå¾ˆâ¼©ã€‚å³å¼ç¬¬â¼†é¡¹$\sigma'(z_j^L)$åˆ»ç”»äº†åœ¨$z_j^L$å¤„æ¿€æ´»å‡½æ•°$\sigma$å˜åŒ–çš„é€Ÿåº¦ã€‚ä¸Šå¼ä¹Ÿå¯ä»¥é‡æ–°å†™æˆçŸ©é˜µå½¢å¼
>$$
\delta^L=\nabla_aC\odot R'(z^L)
>$$
>
>ä¾‹å¦‚ï¼Œå¦‚æœæŸå¤±å‡½æ•°ä¸ºè¯¯å·®å¹³æ–¹å’Œæ—¶ï¼Œæˆ‘ä»¬æœ‰$\nabla_aC=(a^L-y)$ï¼Œå› æ­¤å¯å¾—$\delta_L=(a^L-y)\odot \sigma'(z^L)$

---
# è¯¯å·®åå‘ä¼ æ’­çš„4ä¸ªæ–¹ç¨‹
- `(BP.2)`. ä½¿ç”¨$l+1$å±‚è¯¯å·®$\delta^{l+1}$è®¡ç®—$l$å±‚çš„è¯¯å·®$\delta^{l}$: 
$$
\delta^l=((w^{l+1})^T\delta^{l+1})\odot\sigma'(z^l)
$$

>å‡è®¾å·²çŸ¥$l+1$å±‚çš„è¯¯å·®$\delta^{l+1}$ã€‚å½“åº”â½¤è½¬ç½®çš„æƒé‡çŸ©é˜µ$(w^{l+1})^T$ ï¼Œå¯ä»¥å‡­ç›´è§‰åœ°æŠŠå®ƒçœ‹ä½œæ˜¯åœ¨æ²¿ç€â½¹ç»œåå‘ç§»åŠ¨è¯¯å·®ï¼Œç»™äº†åº¦é‡$l$å±‚è¾“å‡ºçš„è¯¯å·®â½…æ³•ã€‚ç„¶åï¼Œè¿›â¾`Hadamard` ä¹˜ç§¯è¿ç®—$\odot\sigma'(z^l)$ï¼Œä»¥ä½¿è¯¯å·®é€šè¿‡$l$å±‚çš„æ¿€æ´»å‡½æ•°åå‘ä¼ é€’å›æ¥ï¼Œå¹¶ç»™å‡ºåœ¨ç¬¬$l$å±‚çš„å¸¦æƒè¾“â¼Šçš„è¯¯å·®$\delta$ã€‚

- **é€šè¿‡ç»„åˆ`(BP.1)` å’Œ`(BP.2)`ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—ä»»ä½•å±‚çš„è¯¯å·®$\delta^l$ã€‚â¾¸å…ˆä½¿â½¤`(BP.1)` è®¡ç®—$\delta^L$ï¼Œç„¶ååº”â½¤â½…ç¨‹`(BP.2)` æ¥è®¡ç®—$\delta^{L-1}$ï¼Œç„¶åå†æ¬¡â½¤â½…ç¨‹`(BP.2)`æ¥è®¡ç®—$\delta^{L-2}$ï¼Œå¦‚æ­¤â¼€æ­¥â¼€æ­¥åœ°åå‘ä¼ æ’­å®Œæ•´ä¸ªâ½¹ç»œã€‚**

---
# è¯¯å·®åå‘ä¼ æ’­çš„4ä¸ªæ–¹ç¨‹
- `(BP.3)`. ä»£ä»·å‡½æ•°å…³äºâ½¹ç»œä¸­ä»»æ„åç½®çš„æ”¹å˜ç‡:
$$
\frac{\partial{C}}{\partial{b_j^l}}=\delta_j^l \\
\text{ or } \\
\frac{\partial{C}}{\partial{b^l}}=\delta^l \in \mathbb{R}^{M_l}
$$

---
æƒé‡å’Œåç½®æ¢¯åº¦çš„è®¡ç®—
![bg right:90% fit](./pictures/7.bp3-4.svg)

---
# è¯¯å·®åå‘ä¼ æ’­çš„4ä¸ªæ–¹ç¨‹
- `(BP.4)`. ä»£ä»·å‡½æ•°å…³äºæƒé‡çš„æ”¹å˜ç‡
$$
\frac{\partial{C}}{\partial{w_{jk}^l}}=a_k^{l-1}\delta_j^l \\
\text{ or } \\
\frac{\partial{C}}{\partial{w^l}}=\delta^l (a^{l-1})^T\in \mathbb{R}^{M_l\times M_{l-1}}
$$

>å½“æ¿€æ´»å€¼$a_k^{l-1}$å¾ˆâ¼©ï¼Œä¾‹å¦‚$a_k^{l-1}\simeq 0$æ—¶ï¼Œæ¢¯åº¦$\frac{\partial{C}}{\partial{w_{jk}^l}}$ä¹Ÿä¼šè¶‹å‘å¾ˆâ¼©ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å°±è¯´æƒé‡ç¼“æ…¢å­¦ä¹ ï¼Œè¡¨â½°åœ¨æ¢¯åº¦ä¸‹é™çš„æ—¶å€™ï¼Œè¿™ä¸ªæƒé‡ä¸ä¼šæ”¹å˜å¤ªå¤šã€‚æ¢â¾”ä¹‹ï¼Œ`(BP.4)`çš„â¼€ä¸ªç»“æœå°±æ˜¯æ¥â¾ƒä½æ¿€æ´»å€¼ç¥ç»å…ƒçš„æƒé‡å­¦ä¹ ä¼šâ¾®å¸¸ç¼“æ…¢ã€‚

---
è®¡ç®—å›¾
![bg right:90% fit](./pictures/7.computational_graph.svg)

---
# æ¡ˆä¾‹

## äº¤å‰ç†µé£é™©å‡½æ•°
- äº¤å‰ç†µé£é™©å‡½æ•°$\mathbf{R(z^L, y)}=-y\cdot\log{a^L}$å…³äº$z^L$çš„æ¢¯åº¦ä¸º
$$\begin{aligned}
\mathrm{softmax}(z^L) &=  \left(\frac{exp(z^l_1)}{\sum_{j=1}^{M_L}exp(z^l_j)}, ... , \frac{exp(z^l_{M_L})}{\sum_{j=1}^{M_L}exp(z^l_j)}\right)\\
a^L&=\mathrm{softmax}(z^L), \frac{\partial{C}}{\partial{a_i^L}}=-y_i\frac{1}{a_i^L}\\
\frac{\partial{C}}{\partial{a^L}} &= -y\odot (a^L)^{-1} = (-y_1\frac{1}{a_1^L}, -y_2\frac{1}{a_2^L}, ..., -y_{M_L}\frac{1}{a_{M_L}^L})
\end{aligned}
$$


---
# æ¡ˆä¾‹
## äº¤å‰ç†µé£é™©å‡½æ•°
- å½“$i=j$æ—¶, æœ‰$\frac{\partial{a_i}}{\partial{z_j}}=a_i(1-a_i)$; å½“$i\neq j$æ—¶, æœ‰$\frac{\partial{a_i}}{\partial{z_j}}=-a_ia_j$, å› æ­¤æœ‰

$$
\begin{aligned}
\frac{\partial{a^L}}{\partial{z^L}} &= \begin{bmatrix} 
a_1^L(1-a_1^L) & -a_1^La_2^L & ... & -a_1^La_j^L & ... & -a_1^La_{M_L}^L \\ 
-a_2^La_1^L & a_2^L(1-a_2^L) & ... & a_2^La_j^L & ... & -a_2^La_{M_L}^L \\
... & ... & ... & ... & ... & ....\\
-a_{M_L}^La_1^L & -a_{M_L}^La_2^L & ... & -a_{M_L}^La_j^L & ... & a_{M_L}^L(1-a_{M_L}^L)
\end{bmatrix} \\
&= (\frac{\partial{a^L}}{\partial{z^L_1}}, \frac{\partial{a^L}}{\partial{z^L_2}}, ..., \frac{\partial{a^L}}{\partial{z^L_{M_L}}})
\end{aligned}
$$
> è‹¥$i = j$, åˆ™æœ‰$\frac{\partial \mathrm{softmax}(x_i)}{\partial x_j}=\mathrm{softmax}(x_i)(1-\mathrm{softmax}(x_i))$; å¦åˆ™, $\frac{\partial \mathrm{softmax}(x_i)}{\partial x_j}=-\mathrm{softmax}(x_i)\mathrm{softmax}(x_j)$


---
# æ¡ˆä¾‹
- å› æ­¤ï¼Œç»“åˆé“¾å¼æ³•åˆ™ï¼Œæœ‰
$$
\begin{aligned}
\frac{\partial{C}}{\partial{z^L}} &= \frac{\partial{C}}{\partial{a^L}} \frac{\partial{a^L}}{\partial{z^L}} \\
&= (\frac{\partial{C}}{\partial{a^L}}\frac{\partial{a^L}}{\partial{z^L_1}}, \frac{\partial{C}}{\partial{a^L}}\frac{\partial{a^L}}{\partial{z^L_2}}, ..., \frac{\partial{C}}{\partial{a^L}}\frac{\partial{a^L}}{\partial{z^L_{M_L}}}) 
\end{aligned}
$$

- å‡å®š$y_i$=1, åˆ™æœ‰$\frac{\partial{C}}{\partial{z^L}}=(a^L_1, a^L_2, ..., a^L_{i}-1, ..., a^L_{M_L})$, å› æ­¤è¾“å‡ºå±‚çš„è¯¯å·®é¡¹$\delta^L$å¯ä»¥å†™æˆ: 
$$\delta^L=\frac{\partial{C}}{\partial{z^L}}=a^L-y$$

>  

---
# ç»ƒä¹ : è¯·ç»“åˆé“¾å¼æ³•åˆ™ï¼Œæ¨å¯¼`(BP.1)~(BP.4)`.

---
# è¯¯å·®åå‘ä¼ æ’­ç®—æ³•
åå‘ä¼ æ’­æ–¹ç¨‹ç»™å‡ºäº†ä¸€ç§è®¡ç®—ä»£ä»·å‡½æ•°æ¢¯åº¦çš„æ–¹æ³•ï¼Œå¯ä»¥ç”¨ä»¥ä¸‹ç®—æ³•æè¿°:

**è¾“å…¥: è®­ç»ƒé›†`{features, labels}`, è®­ç»ƒå›åˆæ•°`max_epochs`, å­¦ä¹ ç‡`lr`, æ‰¹é‡å¤§å°`batch_size`**
**è¾“å‡º: è®­ç»ƒå¥½çš„å‰é¦ˆç¥ç»ç½‘ç»œ**
**ç®—æ³•è¿‡ç¨‹:**
- åˆå§‹åŒ–å½“å‰å›åˆ`epoch=1`, 
- å¦‚æœ`epoch <= max_epochs`, æ‰§è¡Œä»¥ä¸‹æ“ä½œ
    - æ‰“ä¹±è®­ç»ƒé›†çš„æ’åº
    - ç”±å‰é€æ‰¹å–å‡º`batch_size`ä¸ªæ ·æœ¬ï¼Œç„¶ååšä»¥ä¸‹è®¡ç®—ï¼Œç›´åˆ°å–å®Œæ‰€æœ‰æ ·æœ¬ä¸ºæ­¢
        - å‰é¦ˆè®¡ç®—æ¯ä¸€å±‚çš„å‡€å€¼$z^l$å’Œæ¿€æ´»å€¼$a^l$ï¼Œç›´åˆ°æœ€åä¸€å±‚
        - åå‘ä¼ æ’­è®¡ç®—æ¯ä¸€å±‚çš„è¯¯å·®$\delta^l$(**å…¬å¼`BP1, BP2`**)
        - è®¡ç®—æŸå¤±å‡½æ•°å¯¹å„å±‚é—´æƒé‡çŸ©é˜µ$w^l$å’Œåç½®å‘é‡$b^l$çš„åå¯¼æ•°(**å…¬å¼`BP3, BP4`**)

---
# è¯¯å·®åå‘ä¼ æ’­ç®—æ³•

**ç®—æ³•è¿‡ç¨‹:**
- å¦‚æœ`epoch <= max_epochs`, æ‰§è¡Œä»¥ä¸‹æ“ä½œ
...
    - ç”±å‰é€æ‰¹å–å‡º`batch_size`ä¸ªæ ·æœ¬ï¼Œç„¶ååšä»¥ä¸‹è®¡ç®—ï¼Œç›´åˆ°å–å®Œæ‰€æœ‰æ ·æœ¬ä¸ºæ­¢
        ...
        - æ›´æ–°æƒé‡çŸ©é˜µå’Œæƒé‡çŸ©é˜µ
        $$
        \begin{aligned}
        w^l &:= w^l - lr*\frac{\delta^l(a^{(l-1)})^T}{\mathrm{batch\_size}} \\
        b^l &:= b^l - lr*\mathrm{mean}(\delta^l, axis=1)
        \end{aligned}
        $$
    - æ›´æ–°$epoch := epoch + 1$
> æ³¨æ„: `(BP.1)~(BP.4)`æ˜¯å•ä¸€æ ·æœ¬çš„æ›´æ–°ï¼Œé’ˆå¯¹å°æ‰¹é‡æ ·æœ¬éœ€å¯¹è¯¯å·®é¡¹å‘é‡å–å¹³å‡ã€‚

---
# è¯¯å·®åå‘ä¼ æ’­ç®—æ³•
```python
class FNN:
    def __init__(self, features, labels, params, batch_size=256):
        '''
        features: ç‰¹å¾
        labels: æ ‡ç­¾
        paramså…ƒç´ : (æƒé‡çŸ©é˜µ, åç½®å‘é‡, æ¿€æ´»å‡½æ•°, æ¿€æ´»å‡½æ•°çš„å¯¼æ•°)
        æ³¨æ„: æƒé‡çš„å½¢å¼ä¸º (M_{l}, M_{l-1}), åç½®çš„å½¢å¼ä¸º (1, M_{l})
        '''
        self.features = features
        self.labels = labels
        self.params = params
        self.train_iter = self.data_loader(batch_size=256)
```

---
# è¯¯å·®åå‘ä¼ æ’­ç®—æ³•
```python
class FNN:
    ...
    def data_loader(self, batch_size, is_one_hot=True):
        '''
        æ„å»ºå°æ‰¹é‡æ•°æ®é›†
        '''
        if is_one_hot:
            hot_labels = torch.zeros(self.features.shape[0], 10)
            x_indices = np.arange(self.features.shape[0]).tolist()
            y_indices = labels.byte().tolist()
            hot_labels[x_indices, y_indices] = 1
            dataset = TensorDataset(self.features, hot_labels)
        else:
            dataset = TensorDataset(self.features, self.labels)

        return DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)
```

---
# è¯¯å·®åå‘ä¼ æ’­ç®—æ³•
```python
class FNN:
    ...
    def forward(self, X):
        '''
        ç¥ç»å…ƒå‰é¦ˆä¼ é€’ä¿¡æ¯
        '''
        self.z_list, self.a_list = [], [X]  # è®°å½•å„å±‚çš„å‡€å€¼å’Œæ¿€æ´»å€¼
        y = X  # åˆå§‹åŒ–è¾“å…¥features
        for weight, bias, func, _ in self.params:
            # (N, M_{l-1}) @ (M_{l-1}, M_{l}) + (1, M_{l}), broadcast
            z = y@torch.transpose(weight, 0, 1) + bias.reshape(1, -1)  
            if func:
                if func.__name__ == 'softmax':
                    y = func(z, dim=1)
                else:
                    y = func(z)
            else:
                y = z
                
            self.z_list.append(z)
            self.a_list.append(y)
        return y.double()
```

---
# è¯¯å·®åå‘ä¼ æ’­ç®—æ³•
```python
class FNN:
    ...
    def cross_entropy(self, X, y):
        '''
        labels: one-hotå½¢å¼
        hat_y: softmaxä¹‹åå¯¹åº”æ¦‚ç‡å‘é‡ï¼Œå¤šå±‚æ„ŸçŸ¥æœºçš„è¾“å‡º
        '''
        hat_y = self.forward(X)
        if len(y.shape) == 2:
            # å±•å¼€æˆ1ç»´ï¼Œç‚¹ç§¯
            crossEnt = -torch.dot(y.reshape(-1),torch.log10(hat_y.float()).reshape(-1))/y.shape[0] 
        elif len(y.shape) == 1:
            crossEnt = -torch.mean(torch.log10(hat_y[torch.arange(y.shape[0]), y.long()]))
        else:
            print("Wrong format of y!")
        return crossEnt
```

---
# è¯¯å·®åå‘ä¼ æ’­ç®—æ³•
```python
class FNN:
    ...
    def cal_neuron_errors(self, y):
        '''
        è®¡ç®—ç¥ç»å…ƒçš„è¯¯å·®
        '''
        # è¾“å‡ºå±‚è¯¯å·®
        error_L = self.a_list[-1] - y
        self.error_list = [error_L]
        for i in range(len(self.params)-1):
            weight = self.params[-i-1][0]  # æƒé‡çŸ©é˜µ
            der_f = self.params[-i-1][-1]  # å¯¼æ•°
            error_up = self.error_list[-1]  # ä¸Šä¸€å±‚çš„è¯¯å·®
            z = self.z_list[-i-2]  # å½“å‰å±‚çš„å‡€å€¼
            #  (N, M_{l})@(M_{l}, M_{l-1}) = (N, M_{l-1})
            error = error_up@weight * der_f(z)
            self.error_list.append(error)
            
        self.error_list.reverse()
```

---
# è¯¯å·®åå‘ä¼ æ’­ç®—æ³•
```python
class FNN:
    ...
    def cal_params_partial(self):
        '''
        è®¡ç®—æŸå¤±å‡½æ•°å…³äºæƒé‡å’Œåç½®çš„åå¯¼æ•°
        '''
        self.der_weight_list = []
        self.der_bias_list = []
        for i in range(len(self.params)):
            a_out = self.a_list[i]
            error_in = self.error_list[i]
            # ä»¥ä¸‹è®¡ç®—å‡ºæ¥çš„æ˜¯æ¯ä¸ªæ ·æœ¬å¯¹åº”çš„der_weightæ„æˆçš„çŸ©é˜µï¼Œå½’çº¦æˆ1ç»´ï¼Œå¯é‡‡ç”¨å‡å€¼æˆ–æ±‚å’Œçš„å½¢å¼
            # (M_{l}, N) @ (N, M{l-1})
            der_weight = torch.transpose(error_in, 0, 1)@a_out / self.a_list[0].shape[0]  
            der_bias = torch.mean(torch.transpose(error_in, 0, 1), axis=1)  # (M_{l}, N)
            self.der_weight_list.append(der_weight)
            self.der_bias_list.append(der_bias)

```
---
# è¯¯å·®åå‘ä¼ æ’­ç®—æ³•
```python
class FNN:
    ...
    def backward(self, y):
        '''
        è¯¯å·®åå‘ä¼ æ’­ç®—æ³•å®ç°
        '''
        self.cal_neuron_errors(y)
        self.cal_params_partial()
    
    def accuracy(self, y, hat_y, is_one_hot=False):
        '''
        y: æ ‡ç­¾, one-hot
        hat_y: æ ‡ç­¾é¢„æµ‹æ¦‚ç‡, one-hot
        is_one_hot: yæ˜¯å¦ä¸ºone-hotå½¢å¼
        '''
        if is_one_hot:
            precision = torch.sum(torch.max(y, axis=1)[1]== 
                torch.max(hat_y, axis=1)[1]).numpy() / y.shape[0]
        else:
            precision = torch.sum((y == 
                torch.max(hat_y, axis=1)[1]).byte()).numpy() / y.shape[0]
        return precision

```

---
# è¯¯å·®åå‘ä¼ æ’­ç®—æ³•
```python
class FNN:
    ...
    def minibatch_sgd_trainer(self, max_epochs=10, lr=0.1):
        '''
        è®­ç»ƒ
        '''
        for epoch in range(max_epochs):
            for X, y in self.train_iter:
                self.forward(X)  # å‰å‘ä¼ æ’­
                self.backward(y)  # è¯¯å·®åå‘ä¼ æ’­
                for i in range(len(self.params)):
                    self.params[i][0] -= lr*self.der_weight_list[i]
                    self.params[i][1] -= lr*self.der_bias_list[i]
            
            loss = self.cross_entropy(self.features, self.labels)
            accu = self.accuracy(self.labels, self.forward(self.features))
            print(f"ç¬¬{epoch+1}ä¸ªå›åˆ, è®­ç»ƒé›†äº¤å‰ç†µæŸå¤±ä¸º:{loss:.4f}, åˆ†ç±»å‡†ç¡®ç‡{accu:.4f}")

```

---
# è¯¯å·®åå‘ä¼ æ’­ç®—æ³•
```python
def d_relu(x):
    '''
    reluæ¿€æ´»å‡½æ•°çš„å¯¼æ•°
    '''
    d = torch.zeros_like(x)
    d[x > 0] = 1
    return d

def d_softmax(x):
    '''
    softmaxæ¿€æ´»å‡½æ•°çš„å¯¼æ•°
    '''
    d = torch.softmax(x, dim=1)
    return d*(1-d)
```

---
# å‚æ•°åˆå§‹åŒ–

ç¥ç»ç½‘ç»œçš„å‚æ•°å­¦ä¹ æ˜¯ä¸€ä¸ªéå‡¸ä¼˜åŒ–é—®é¢˜ï¼å½“ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•æ¥è¿›è¡Œä¼˜åŒ–ç½‘ç»œå‚æ•°æ—¶ï¼Œå‚æ•°åˆå§‹å€¼çš„é€‰å–ååˆ†å…³é”®ï¼Œå…³ç³»åˆ°ç½‘ç»œçš„ä¼˜åŒ–æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ï¼å‚æ•°åˆå§‹åŒ–çš„æ–¹å¼é€šå¸¸æœ‰ä»¥ä¸‹ä¸‰ç§ï¼š

- é¢„è®­ç»ƒåˆå§‹åŒ–ï¼šä¸åŒçš„å‚æ•°åˆå§‹å€¼ä¼šæ”¶æ•›åˆ°ä¸åŒçš„å±€éƒ¨æœ€ä¼˜è§£ï¼è™½ç„¶è¿™äº›å±€éƒ¨æœ€ä¼˜è§£åœ¨è®­ç»ƒé›†ä¸Šçš„æŸå¤±æ¯”è¾ƒæ¥è¿‘ï¼Œä½†æ˜¯å®ƒä»¬çš„æ³›åŒ–èƒ½åŠ›å·®å¼‚å¾ˆå¤§ï¼ä¸€ä¸ªå¥½çš„åˆå§‹å€¼ä¼šä½¿å¾—ç½‘ç»œæ”¶æ•›åˆ°ä¸€ä¸ªæ³›åŒ–èƒ½åŠ›é«˜çš„å±€éƒ¨æœ€ä¼˜è§£ï¼é€šå¸¸æƒ…å†µä¸‹ï¼Œä¸€ä¸ªå·²ç»åœ¨å¤§è§„æ¨¡æ•°æ®ä¸Šè®­ç»ƒè¿‡çš„æ¨¡å‹å¯ä»¥æä¾›ä¸€ä¸ªå¥½çš„å‚æ•°åˆå§‹å€¼ï¼Œè¿™ç§åˆå§‹åŒ–æ–¹æ³•ç§°ä¸º**é¢„è®­ç»ƒåˆå§‹åŒ–ï¼ˆ`Pre-trained Initialization`ï¼‰**ï¼é¢„è®­ç»ƒä»»åŠ¡å¯ä»¥ä¸ºç›‘ç£å­¦ä¹ æˆ–æ— ç›‘ç£å­¦ä¹ ä»»åŠ¡ï¼ç”±äºæ— ç›‘ç£å­¦ä¹ ä»»åŠ¡æ›´å®¹æ˜“è·å–å¤§è§„æ¨¡çš„è®­ç»ƒæ•°æ®ï¼Œå› æ­¤è¢«å¹¿æ³›é‡‡ç”¨ï¼é¢„è®­ç»ƒæ¨¡å‹åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šçš„å­¦ä¹ è¿‡ç¨‹ä¹Ÿç§°ä¸º**ç²¾è°ƒï¼ˆ`Fine-Tuning`ï¼‰**ï¼

---
# å‚æ•°åˆå§‹åŒ–

- éšæœºåˆå§‹åŒ–ï¼šåœ¨çº¿æ€§æ¨¡å‹çš„è®­ç»ƒï¼ˆæ¯”å¦‚æ„ŸçŸ¥æœºå’Œ`Logistic` å›å½’ï¼‰ä¸­ï¼Œæˆ‘ä»¬ä¸€èˆ¬å°†å‚æ•°å…¨éƒ¨åˆå§‹åŒ–ä¸º0ï¼ä½†æ˜¯è¿™åœ¨ç¥ç»ç½‘ç»œçš„è®­ç»ƒä¸­ä¼šå­˜åœ¨ä¸€äº›é—®é¢˜ï¼å› ä¸ºå¦‚æœå‚æ•°éƒ½ä¸º0ï¼Œåœ¨ç¬¬ä¸€éå‰å‘è®¡ç®—æ—¶ï¼Œæ‰€æœ‰çš„éšè—å±‚ç¥ç»å…ƒçš„æ¿€æ´»å€¼éƒ½ç›¸åŒï¼›åœ¨åå‘ä¼ æ’­æ—¶ï¼Œæ‰€æœ‰æƒé‡çš„æ›´æ–°ä¹Ÿéƒ½ç›¸åŒï¼Œè¿™æ ·ä¼šå¯¼è‡´éšè—å±‚ç¥ç»å…ƒæ²¡æœ‰åŒºåˆ†æ€§ï¼è¿™ç§ç°è±¡ä¹Ÿç§°ä¸º**å¯¹ç§°æƒé‡ç°è±¡**ï¼ä¸ºäº†æ‰“ç ´è¿™ä¸ªå¹³è¡¡ï¼Œæ¯”è¾ƒå¥½çš„æ–¹å¼æ˜¯å¯¹æ¯ä¸ªå‚æ•°éƒ½**éšæœºåˆå§‹åŒ–ï¼ˆ`Random Initialization`ï¼‰**ï¼Œä½¿å¾—ä¸åŒç¥ç»å…ƒä¹‹é—´çš„åŒºåˆ†æ€§æ›´å¥½ï¼
- å›ºå®šå€¼åˆå§‹åŒ–ï¼šå¯¹äºä¸€äº›ç‰¹æ®Šçš„å‚æ•°ï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®ç»éªŒç”¨ä¸€ä¸ªç‰¹æ®Šçš„å›ºå®šå€¼æ¥è¿›è¡Œåˆå§‹åŒ–ï¼æ¯”å¦‚åç½®ï¼ˆ`Bias`ï¼‰é€šå¸¸ç”¨0æ¥åˆå§‹åŒ–ï¼Œä½†æ˜¯æœ‰æ—¶å¯ä»¥è®¾ç½®æŸäº›ç»éªŒå€¼ä»¥æé«˜ä¼˜åŒ–æ•ˆç‡ï¼å¯¹äºä½¿ç”¨`ReLU` çš„ç¥ç»å…ƒï¼Œæœ‰æ—¶ä¹Ÿå¯ä»¥å°†åç½®è®¾ä¸º0.01ï¼Œä½¿å¾—`ReLU` ç¥ç»å…ƒåœ¨è®­ç»ƒåˆæœŸæ›´å®¹æ˜“æ¿€æ´»ï¼Œä»è€Œè·å¾—ä¸€å®šçš„æ¢¯åº¦æ¥è¿›è¡Œè¯¯å·®åå‘ä¼ æ’­ï¼

---
# å‚æ•°åˆå§‹åŒ–

è™½ç„¶é¢„è®­ç»ƒåˆå§‹åŒ–é€šå¸¸å…·æœ‰æ›´å¥½çš„æ”¶æ•›æ€§å’Œæ³›åŒ–æ€§ï¼Œä½†æ˜¯çµæ´»æ€§ä¸å¤Ÿï¼Œä¸èƒ½åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šä»»æ„åœ°è°ƒæ•´ç½‘ç»œç»“æ„ï¼

å› æ­¤ï¼Œå¥½çš„éšæœºåˆå§‹åŒ–æ–¹æ³•å¯¹è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹æ¥è¯´ä¾ç„¶ååˆ†é‡è¦ï¼è¿™é‡Œæˆ‘ä»¬ä»‹ç»ä¸‰ç±»å¸¸ç”¨çš„éšæœºåˆå§‹åŒ–æ–¹æ³•ï¼š
  - åŸºäºå›ºå®šæ–¹å·®çš„å‚æ•°åˆå§‹åŒ–
  - åŸºäºæ–¹å·®ç¼©æ”¾çš„å‚æ•°åˆå§‹åŒ–
  - æ­£äº¤åˆå§‹åŒ–æ–¹æ³•ï¼

---
# å‚æ•°åˆå§‹åŒ–
## 1. åŸºäºå›ºå®šæ–¹å·®çš„å‚æ•°åˆå§‹åŒ–

- é«˜æ–¯åˆ†å¸ƒåˆå§‹åŒ–ï¼šä½¿ç”¨ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒ$N(0,\sigma^2)$å¯¹æ¯ä¸ªå‚æ•°è¿›è¡Œéšæœºåˆå§‹åŒ–ï¼
- å‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–ï¼šåœ¨ä¸€ä¸ªç»™å®šçš„åŒºé—´$[-r, r]$å†…é‡‡ç”¨å‡åŒ€åˆ†å¸ƒæ¥åˆå§‹åŒ–å‚æ•°ï¼å‡è®¾éšæœºå˜é‡xåœ¨åŒºé—´$[a, b]$å†…å‡åŒ€åˆ†å¸ƒï¼Œåˆ™å…¶æ–¹å·®ä¸º$var(x)=\frac{(b-a)^2}{12}$. å› æ­¤ï¼Œè‹¥ä½¿ç”¨åŒºé—´ä¸º$[-r, r]$çš„å‡åˆ†åˆ†å¸ƒæ¥é‡‡æ ·ï¼Œå¹¶æ»¡è¶³$var(x)=\sigma^2$æ—¶ï¼Œåˆ™ğ‘Ÿçš„å–å€¼ä¸º$r=\sqrt{3\sigma^2}$

---
# å‚æ•°åˆå§‹åŒ–
## 1. åŸºäºå›ºå®šæ–¹å·®çš„å‚æ•°åˆå§‹åŒ–
```python
def uniform_init(m, limit=0.1):
    if isinstance(m, nn.Linear):
        init.uniform_(m.weight, -limit, limit)
        if m.bias is not None:
            init.uniform_(m.bias, -limit, limit)

def normal_init(m, mean=0.0, std=0.05):
    if isinstance(m, nn.Linear):
        init.normal_(m.weight, mean, std)
        if m.bias is not None:
            init.normal_(m.bias, mean, std)

```


---
# å‚æ•°åˆå§‹åŒ–
## 2. åŸºäºæ–¹å·®ç¼©æ”¾çš„å‚æ•°åˆå§‹åŒ–

| åˆå§‹åŒ–æ–¹æ³• | æ¿€æ´»å‡½æ•° | å‡åŒ€åˆ†å¸ƒ$[-r, r]$ | é«˜æ–¯åˆ†å¸ƒ $N(0, \sigma^2)$ |
| :----: | :----: | :----: | :----: |
| Xavier | Logistic | $r=4\sqrt{\frac{6}{M_{l-1}+M_l}}$ | $\sigma^2=16\times \frac{2}{M_{l-1}+M_l}$ |
| Xavier | tanh | $r=\sqrt{\frac{6}{M_{l-1}+M_l}}$ | $\sigma^2=\frac{2}{M_{l-1}+M_l}$ |
| He | reLu | $r=\sqrt{\frac{6}{M_{l-1}}}$ | $\sigma^2=\frac{2}{M_{l-1}}$ |

---
# å‚æ•°åˆå§‹åŒ–
## 2. åŸºäºæ–¹å·®ç¼©æ”¾çš„å‚æ•°åˆå§‹åŒ–
```python
def xavier_uniform_init(m):
    if isinstance(m, nn.Linear):
        init.xavier_uniform_(m.weight)
        if m.bias is not None:
            init.zeros_(m.bias)

def xavier_normal_init(m):
    if isinstance(m, nn.Linear):
        init.xavier_normal_(m.weight)
        if m.bias is not None:
            init.zeros_(m.bias)

```

---
# å‚æ•°åˆå§‹åŒ–
## 2. åŸºäºæ–¹å·®ç¼©æ”¾çš„å‚æ•°åˆå§‹åŒ–
```python
def he_uniform_init(m):
    if isinstance(m, nn.Linear):
        init.kaiming_uniform_(m.weight, nonlinearity='relu')
        if m.bias is not None:
            init.zeros_(m.bias)

def he_normal_init(m):
    if isinstance(m, nn.Linear):
        init.kaiming_normal_(m.weight, nonlinearity='relu')
        if m.bias is not None:
            init.zeros_(m.bias)
```


---
# å‚æ•°åˆå§‹åŒ–
## 3. æ­£äº¤åˆå§‹åŒ–
æ­£äº¤åˆå§‹åŒ–æ˜¯ä¸€ç§æƒé‡åˆå§‹åŒ–ç­–ç•¥ï¼Œå¸¸ç”¨äºæ·±åº¦å­¦ä¹ ä¸­çš„å„ç§ç½‘ç»œç»“æ„ï¼Œç‰¹åˆ«æ˜¯åœ¨å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ä¸­éå¸¸å—æ¬¢è¿ã€‚æ­£äº¤åˆå§‹åŒ–çš„ç›®çš„æ˜¯ä½¿å¾—æƒé‡çŸ©é˜µ $W$ ä¸­çš„è¡Œå‘é‡æˆ–åˆ—å‘é‡éƒ½æ˜¯æ­£äº¤çš„ï¼Œå³ $W^T W = I$æˆ–$W W^T = I$ï¼Œå…¶ä¸­ $I$æ˜¯å•ä½çŸ©é˜µã€‚è¿™æ ·çš„ç‰¹æ€§æœ‰å‡ ä¸ªå…³é”®ä¼˜ç‚¹ï¼š
1. **ä¿æŒç‰¹å¾ç‹¬ç«‹**ï¼šæ­£äº¤çŸ©é˜µçš„è¿™ç§ç‰¹æ€§å¸®åŠ©ç½‘ç»œæ¯å±‚è¾“å‡ºçš„ç‰¹å¾ä¿æŒç‹¬ç«‹ï¼Œå‡å°‘äº†ç‰¹å¾é—´çš„å†—ä½™ï¼Œæé«˜äº†å­¦ä¹ æ•ˆç‡ã€‚
2. **ç¼“è§£æ¢¯åº¦é—®é¢˜**ï¼šåœ¨å¾ªç¯ç¥ç»ç½‘ç»œä¸­ï¼Œç”±äºæƒé‡çŸ©é˜µåœ¨æ¯ä¸ªæ—¶é—´æ­¥éƒ½å‚ä¸è®¡ç®—ï¼Œæ­£äº¤æ€§èƒ½æœ‰æ•ˆé˜²æ­¢æ¢¯åº¦åœ¨åå‘ä¼ æ’­ä¸­çš„æ¶ˆå¤±æˆ–çˆ†ç‚¸ï¼Œè¿™æ˜¯å› ä¸ºæ­£äº¤çŸ©é˜µçš„ç‰¹å¾å€¼ä¸º1ï¼Œè¿™ä¿æŒäº†æ¢¯åº¦æµåŠ¨çš„ç¨³å®šæ€§ã€‚
3. **ç¨³å®šçš„æ¿€æ´»åˆ†å¸ƒ**ï¼šç”±äºæ­£äº¤çŸ©é˜µçš„ä¹˜ç§¯ä»æ˜¯æ­£äº¤çš„ï¼Œè¿™å¸®åŠ©ç½‘ç»œåœ¨å‰å‘ä¼ æ’­ä¸­ä¿æŒæ¿€æ´»åˆ†å¸ƒçš„ç¨³å®šæ€§ï¼Œä»è€Œæœ‰åŠ©äºç½‘ç»œçš„å¿«é€Ÿå’Œç¨³å®šè®­ç»ƒã€‚

---
# å‚æ•°åˆå§‹åŒ–
## 3. æ­£äº¤åˆå§‹åŒ–
æ­£äº¤åˆå§‹åŒ–ç‰¹åˆ«é€‚ç”¨äºé‚£äº›æ¿€æ´»å‡½æ•°çš„è¾“å‡ºå¯ä»¥å¹¿æ³›åˆ†å¸ƒä¸”ä¸æ˜“é¥±å’Œçš„ç½‘ç»œå±‚ï¼Œå¦‚ReLUæ¿€æ´»å‡½æ•°é…åˆçš„å±‚ã€‚å…¶åˆå§‹åŒ–æ­¥éª¤é€šå¸¸å¦‚ä¸‹ï¼š
1. **ç”ŸæˆéšæœºçŸ©é˜µ**ï¼šé¦–å…ˆç”Ÿæˆä¸€ä¸ªå½¢çŠ¶ä¸º $(d_{\text{in}}, d_{\text{out}})$çš„éšæœºçŸ©é˜µï¼Œé€šå¸¸å…ƒç´ æ¥è‡ªæ ‡å‡†æ­£æ€åˆ†å¸ƒã€‚
2. **åº”ç”¨SVD**ï¼šå¯¹è¿™ä¸ªéšæœºçŸ©é˜µåº”ç”¨å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰ï¼Œå¾—åˆ° $U$, $\Sigma$, $V^T$ã€‚
3. **æ„é€ æ­£äº¤çŸ©é˜µ**ï¼š
   - å¦‚æœ $d_{\text{in}} < d_{\text{out}}$ï¼Œä½¿ç”¨ $U$ä½œä¸ºæƒé‡çŸ©é˜µï¼Œå› ä¸º $U$çš„åˆ—æ˜¯æ­£äº¤çš„ã€‚
   - å¦‚æœ $d_{\text{in}} \geq d_{\text{out}}$ï¼Œä½¿ç”¨ $V$ï¼ˆæ¥è‡ª $V^T$çš„è½¬ç½®ï¼‰ä½œä¸ºæƒé‡çŸ©é˜µï¼Œå› ä¸º $V$çš„è¡Œæ˜¯æ­£äº¤çš„ã€‚

---
# å‚æ•°åˆå§‹åŒ–
## 3. æ­£äº¤åˆå§‹åŒ–

åœ¨PyTorchä¸­ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨`torch.nn.init.orthogonal_`æ¥åº”ç”¨æ­£äº¤åˆå§‹åŒ–ï¼š

```python
# å®šä¹‰ä¸€ä¸ªçº¿æ€§å±‚
linear = nn.Linear(20, 30)
# åº”ç”¨æ­£äº¤åˆå§‹åŒ–
torch.nn.init.orthogonal_(linear.weight)
# æ‰“å°æƒé‡æŸ¥çœ‹æ•ˆæœ
print("Weights after orthogonal initialization:")
print(linear.weight)
```

---
# æ­£åˆ™åŒ–

- éšç€è®­ç»ƒå›åˆæ•°çš„å¢åŠ ï¼Œç†è®ºä¸Šæ¥è®²ï¼Œè¶³å¤Ÿå¤šç¥ç»å…ƒçš„å‰é¦ˆç¥ç»ç½‘ç»œå¯ä»¥å­¦ä¹ åœ¨è®­ç»ƒé›†ä¸­å‡ ä¹æ‰€æœ‰æ ·æœ¬çš„ç‰¹å¾ï¼Œä»è€Œå¯¹è®­ç»ƒé›†æœ‰ç€å¾ˆé«˜çš„é¢„æµ‹æ­£ç¡®ç‡ã€‚ç„¶è€Œï¼Œéœ€è¦è­¦æƒ•çš„æ—¶å€™ï¼Œæ ·æœ¬ä¸­çš„è¯¯å·®ä¹ŸåŒæ ·è¢«å­¦ä¹ åˆ°äº†ï¼Œä»è€Œå¯¹ç°å®äº§ç”Ÿäº†é”™è¯¯çš„è®¤è¯†ã€‚é€ æˆçš„åæœæ˜¯ï¼Œå¯¹äºæ–°çš„æ ·æœ¬ï¼Œé¢„æµ‹æ­£ç¡®ç‡å¹¶ä¸é«˜ï¼Œç”šè‡³æ¯”ä¸€äº›ç®€å•çš„æ¨¡å‹è¿˜è¦å·®ã€‚

- å› æ­¤ï¼Œå¦‚ä½•æ§åˆ¶æ¨¡å‹ä¸å­¦ä¹ æ ·æœ¬ä¸­çš„è¯¯å·®æ˜¯ä¸€ä¸ªå¾ˆé‡è¦çš„é—®é¢˜ï¼Œç›®å‰é€šå¸¸æœ‰ä»¥ä¸‹åŠæ³•
    - $â„“_1$å’Œ$â„“_2$æ­£åˆ™åŒ–
    - æƒé‡è¡°å‡
    - æå‰ç»ˆæ­¢
    - ä¸¢å¼ƒæ³•

---
# æ­£åˆ™åŒ–
## 1. $â„“_1$å’Œ$â„“_2$æ­£åˆ™åŒ–`Normalization`

- $â„“_1$ å’Œ$â„“_2$ æ­£åˆ™åŒ–æ˜¯æœºå™¨å­¦ä¹ ä¸­æœ€å¸¸ç”¨çš„æ­£åˆ™åŒ–æ–¹æ³•ï¼Œé€šè¿‡çº¦æŸå‚æ•°çš„â„“1 å’Œâ„“2èŒƒæ•°æ¥å‡å°æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®é›†ä¸Šçš„è¿‡æ‹Ÿåˆç°è±¡.
$$
\theta^*=\text{arg}\min \frac{1}{N}\sum_{n=1}^N \mathbf{L}(y^{(n)},f(x^{(n)};\theta))+\lambda \mathrm{â„“_p}(\theta)
$$
- å…¶ä¸­$\mathbf{L(\cdot)}$ä¸ºæŸå¤±å‡½æ•°ï¼Œ$N$ä¸ºè®­ç»ƒæ ·æœ¬ï¼Œ$f(\cdot)$ä¸ºå¾…å­¦ä¹ çš„ç¥ç»ç½‘ç»œï¼Œ$\theta$ä¸ºå‚æ•°ï¼Œ$â„“_p$ä¸ºèŒƒæ•°å‡½æ•°ï¼Œ$ğ‘$ çš„å–å€¼é€šå¸¸ä¸º`{1, 2}` ä»£è¡¨$â„“_1$ å’Œ$â„“_2$ èŒƒæ•°ï¼Œ$ğœ†$ä¸ºæ­£åˆ™åŒ–ç³»æ•°ï¼

---
# æ­£åˆ™åŒ–
## 2. æƒé‡è¡°å‡`Weight Decay`
- æƒé‡è¡°å‡ï¼ˆ`Weight Decay`ï¼‰æ˜¯ä¸€ç§æœ‰æ•ˆçš„æ­£åˆ™åŒ–æ–¹æ³•(`Hanson et al., 1989`)ï¼Œåœ¨æ¯æ¬¡å‚æ•°æ›´æ–°æ—¶ï¼Œå¼•å…¥ä¸€ä¸ªè¡°å‡ç³»æ•°ï¼
$$
\begin{aligned}
W^l &:= (1-\beta)W^l - \alpha \frac{\partial C}{\partial W^l}\\
b^l &:= (1-\beta)b^l - \alpha \frac{\partial C}{\partial b^l}
\end{aligned}
$$

- å…¶ä¸­$\alpha$ä¸ºå­¦ä¹ ç‡ï¼Œ$\beta$ä¸ºæƒé‡è¡°å‡ç³»æ•°ï¼Œä¸€èˆ¬å–å€¼æ¯”è¾ƒå°ï¼Œæ¯”å¦‚0.0005ï¼åœ¨æ ‡å‡†çš„éšæœºæ¢¯åº¦ä¸‹é™ä¸­ï¼Œæƒé‡è¡°å‡æ­£åˆ™åŒ–å’Œ$â„“_2$æ­£åˆ™åŒ–çš„æ•ˆæœç›¸åŒï¼å› æ­¤ï¼Œæƒé‡è¡°å‡åœ¨ä¸€äº›æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­é€šè¿‡$â„“_2$æ­£åˆ™åŒ–æ¥å®ç°ï¼ä½†æ˜¯ï¼Œè¾ƒä¸ºå¤æ‚çš„ä¼˜åŒ–æ–¹æ³•ï¼ˆæ¯”å¦‚`Adam`ï¼‰ä¸­ï¼Œæƒé‡è¡°å‡æ­£åˆ™åŒ–å’Œ$â„“_2$æ­£åˆ™åŒ–å¹¶ä¸ç­‰ä»·.

---
# æ­£åˆ™åŒ–
## 3. æå‰ç»ˆæ­¢`Early Stop`

- æå‰åœæ­¢ï¼ˆ`Early Stop`ï¼‰å¯¹äºæ·±åº¦ç¥ç»ç½‘ç»œæ¥è¯´æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ­£åˆ™åŒ–æ–¹æ³•ï¼ç”±äºæ·±åº¦ç¥ç»ç½‘ç»œçš„æ‹Ÿåˆèƒ½åŠ›éå¸¸å¼ºï¼Œå› æ­¤æ¯”è¾ƒå®¹æ˜“åœ¨è®­ç»ƒé›†ä¸Šè¿‡æ‹Ÿåˆï¼

- åœ¨ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•è¿›è¡Œä¼˜åŒ–æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ä¸ªå’Œè®­ç»ƒé›†ç‹¬ç«‹çš„æ ·æœ¬é›†åˆï¼Œç§°ä¸ºéªŒè¯é›†ï¼ˆ`Validation Set`ï¼‰ï¼Œå¹¶ç”¨éªŒè¯é›†ä¸Šçš„é”™è¯¯æ¥ä»£æ›¿æœŸæœ›é”™è¯¯ï¼

- å½“éªŒè¯é›†ä¸Šçš„é”™è¯¯ç‡ä¸å†ä¸‹é™ï¼Œå°±åœæ­¢è¿­ä»£ï¼ç„¶è€Œåœ¨å®é™…æ“ä½œä¸­ï¼ŒéªŒè¯é›†ä¸Šçš„é”™è¯¯ç‡å˜åŒ–æ›²çº¿å¾ˆå¯èƒ½æ˜¯å…ˆå‡é«˜å†é™ä½ï¼å› æ­¤ï¼Œæå‰åœæ­¢çš„å…·ä½“åœæ­¢æ ‡å‡†éœ€è¦æ ¹æ®å®é™…ä»»åŠ¡è¿›è¡Œä¼˜åŒ–(`Prechelt, 1998`)ï¼

---
# æ­£åˆ™åŒ–
## 4. ä¸¢å¼ƒæ³•`Dropout`

- å½“è®­ç»ƒä¸€ä¸ªæ·±åº¦ç¥ç»ç½‘ç»œæ—¶ï¼Œ æˆ‘ä»¬å¯ä»¥éšæœºä¸¢å¼ƒä¸€éƒ¨åˆ†ç¥ç»å…ƒï¼ˆåŒæ—¶ä¸¢å¼ƒå…¶å¯¹åº”çš„è¿æ¥è¾¹ï¼‰æ¥é¿å…è¿‡æ‹Ÿåˆï¼Œè¿™ç§æ–¹æ³•ç§°ä¸ºä¸¢å¼ƒæ³•ï¼ˆ`Dropout Method`ï¼‰[`Srivastava et al., 2014`]ï¼æ¯æ¬¡é€‰æ‹©ä¸¢å¼ƒçš„ç¥ç»å…ƒæ˜¯éšæœºçš„ï¼

- æœ€ç®€å•çš„æ–¹æ³•æ˜¯è®¾ç½®ä¸€ä¸ªå›ºå®šçš„æ¦‚ç‡ğ‘ï¼å¯¹æ¯ä¸€ä¸ªç¥ç»å…ƒéƒ½ä»¥æ¦‚ç‡ğ‘ æ¥åˆ¤å®šè¦ä¸è¦ä¿ç•™, è‹¥$p=1$ï¼Œåˆ™æ„å‘³ç€éœ€è¦ä¿ç•™è¯¥ç¥ç»å…ƒï¼›è‹¥$p=0$, åˆ™æ„å‘³ç€ä¸ä¿ç•™è¯¥ç¥ç»å…ƒï¼

- é€šè¿‡æ¯æ¬¡éšæœºè®­ç»ƒç¥ç»ç½‘ç»œçš„ä¸€å°éƒ¨åˆ†ï¼Œ`Dropout`èƒ½å¤Ÿè®©ä¸€å¼ å¤§ç½‘ç»œåƒå°ç½‘ç»œä¸€æ ·è¿›è¡Œå­¦ä¹ â€”â€”è¾ƒå°çš„ç¥ç»ç½‘ç»œä¸å®¹æ˜“å‘ç”Ÿè¿‡æ‹Ÿåˆã€‚

- `Dropout`æ˜¯ä¸€ç§è®­ç»ƒä¸€ç³»åˆ—ç½‘ç»œå¹¶å¯¹å…¶è¿›è¡Œå¹³å‡çš„å½¢å¼ã€‚

> ä¸¢å¼ƒæ³•é€šå¸¸è¢«è®¤ä¸ºæ˜¯å¯¹å¤§å¤šæ•°ç¥ç»ç½‘ç»œæ¥è¯´æœ€é‡è¦ä¸”æœ€å…ˆè¿›çš„æ­£åˆ™åŒ–æŠ€æœ¯

---
![bg right:90% fit](./pictures/7.dropout.jpg)

---
# æ­£åˆ™åŒ–
## 4. ä¸¢å¼ƒæ³•`Dropout`

- å¯¹äºä¸€ä¸ªç¥ç»å±‚$y=f(Wx + b)$ï¼Œæˆ‘ä»¬å¯ä»¥å¼•å…¥ä¸€ä¸ªæ©è”½å‡½æ•°$mask(â‹…)$ ä½¿å¾—$y=f(W\times mask(x) + b)$ï¼æ©è”½å‡½æ•°$mask(â‹…)$çš„å®šä¹‰ä¸º
$$
\mathrm{mask}(x)=\begin{cases} m \odot x \text{  è®­ç»ƒæ—¶}\\
px \text{  æµ‹è¯•æ—¶}\\
\end{cases}

$$
æˆ–è€…
$$
\mathrm{mask}(x)=\begin{cases} m \odot \frac{x}{p} \text{  è®­ç»ƒæ—¶}\\
x \text{  æµ‹è¯•æ—¶}\\
\end{cases}
$$

---
# æ­£åˆ™åŒ–
## 4. ä¸¢å¼ƒæ³•`Dropout`
```python
# Dropoutæ¦‚ç‡è®¾ç½®ä¸º0.5
dropout = nn.Dropout(p=0.1)
# å‡è®¾æœ‰ä¸€å±‚çš„è¾“å‡º
layer_output = torch.randn(4, 5)  # éšæœºç”Ÿæˆä¸€äº›æ•°æ®
# åº”ç”¨Dropout
output_during_training = dropout(layer_output)
# è¾“å‡ºæŸ¥çœ‹
print("Output with Dropout during training:")
print(output_during_training)
```

---
# æ­£åˆ™åŒ–
## 4. ä¸¢å¼ƒæ³•`Dropout`
```python
class MyModel(nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()
        self.layer1 = nn.Linear(784, 256)
        self.dropout1 = nn.Dropout(0.5)
        self.layer2 = nn.Linear(256, 10)
    
    def forward(self, x):
        x = torch.relu(self.layer1(x))
        x = self.dropout1(x)  # åœ¨ç¬¬ä¸€å±‚ååº”ç”¨Dropout
        x = self.layer2(x)
        return x

# æ³¨æ„ï¼šæ¨¡å‹è®­ç»ƒæ—¶ä¼šåº”ç”¨Dropoutï¼Œæµ‹è¯•æ—¶éœ€å…³é—­Dropout
model = MyModel()
model.train()  # ç¡®ä¿æ˜¯è®­ç»ƒæ¨¡å¼
```


---
# èåˆæƒé‡è¡°å‡å’Œdropoutçš„å‰é¦ˆç¥ç»ç½‘ç»œå®ç°
```python
class FNN2:
    ...
    def mask(self, X, p):
        '''
        X: è¾“å…¥
        p: ç¥ç»å…ƒçš„ä¿ç•™æ¦‚ç‡
        è‹¥è¾“å…¥Xé™¤ä»¥æ¦‚ç‡pï¼Œåˆ™ä½¿Xçš„æœŸæœ›ä¿æŒä¸å˜, é¢„æµ‹æ—¶, ç¥ç»ç½‘ç»œçš„æƒé‡ä¸ç”¨è½¬æ¢ï¼›
        è‹¥è®­ç»ƒæ—¶æŒ‰æ­£å¸¸çš„è¾“å…¥è®­ç»ƒXï¼Œé¢„æµ‹æ—¶ï¼Œç¥ç»ç½‘ç»œä¸­å¯¹åº”çš„æƒé‡éœ€ä¹˜p
        '''
        if p == 0:
            return torch.zeros_like(X)
        elif p == 1:
            return X
        else:
            prob = p*torch.ones_like(X)
            return X*torch.bernoulli(prob)/p
```

---
# èåˆæƒé‡è¡°å‡å’Œ`dropout`çš„å‰é¦ˆç¥ç»ç½‘ç»œå®ç°
```python
class FNN2:
    ...    
    def train_forward(self, X):
        '''
        è®­ç»ƒç”¨ç¥ç»å…ƒå‰é¦ˆä¼ é€’ä¿¡æ¯: ä¾ç…§æ¦‚ç‡péšæœºå…³é—­ä¸€äº›ç¥ç»å…ƒ
        '''
        y = X  # åˆå§‹åŒ–è¾“å…¥features
        mask_y = self.mask(y, self.prob_dropout[0])
        self.z_list, self.a_list = [], [y]  # è®°å½•å„å±‚çš„å‡€å€¼å’Œæ¿€æ´»å€¼
        for i, (weight, bias, func, _) in enumerate(self.params, start=1):
            p = self.prob_dropout[i]
            # (N, M_{l-1}) @ (M_{l-1}, M_{l}) + (1, M_{l}), broadcast
            z = y@torch.transpose(weight, 0, 1) + bias.reshape(1, -1)  
            mask_z = self.mask(z, p)
            if func:
                if func.__name__ == 'softmax':
                    y = func(mask_z, dim=1)
                else:
                    y = func(mask_z)
            else:
                y = mask_z
            self.z_list.append(mask_z)
            self.a_list.append(y)
        return y.double()
```

---
# èåˆæƒé‡è¡°å‡å’Œ`dropout`çš„å‰é¦ˆç¥ç»ç½‘ç»œå®ç°
```python
class FNN2:
    ...    
    def predict_forward(self, X):
        '''
        é¢„æµ‹ç”¨ç¥ç»å…ƒå‰é¦ˆä¼ é€’ä¿¡æ¯: ä½¿ç”¨æ‰€æœ‰ç¥ç»å…ƒ
        '''
        y = X  # åˆå§‹åŒ–è¾“å…¥features
        for i, (weight, bias, func, _) in enumerate(self.params):
            z = y@torch.transpose(weight, 0, 1) + bias.reshape(1, -1)
            if func:
                if func.__name__ == 'softmax':
                    y = func(z, dim=1)
                else:
                    y = func(z)
            else:
                y = z

        return y.double()
```

---
# èåˆæƒé‡è¡°å‡å’Œ`dropout`çš„å‰é¦ˆç¥ç»ç½‘ç»œå®ç°
```python
class FNN2:
    ...
    def minibatch_sgd_trainer(self, max_epochs=10, lr=0.1, decay=0.0005):
        '''
        è®­ç»ƒ
        lr: å­¦ä¹ ç‡
        decay: æƒé‡è¡°å‡ç³»æ•°
        '''
        for epoch in range(max_epochs):
            for X, y in self.train_iter:
                self.train_forward(X)  # å‰å‘ä¼ æ’­
                self.backward(y)  # è¯¯å·®åå‘ä¼ æ’­
                for i in range(len(self.params)):
                    self.params[i][0] = (1 - decay)*self.params[i][0] - lr*self.der_weight_list[i]
                    self.params[i][1] = (1 - decay)*self.params[i][1] - lr*self.der_bias_list[i]
            
            hat_labels = self.predict_forward(self.features)
            loss = self.cross_entropy(self.labels, hat_labels)
            accu = self.accuracy(self.labels, hat_labels)
            print(f"ç¬¬{epoch+1}ä¸ªå›åˆ, è®­ç»ƒé›†äº¤å‰ç†µæŸå¤±ä¸º:{loss:.4f}, åˆ†ç±»å‡†ç¡®ç‡{accu:.4f}")
```


---
## å‚è€ƒèµ„æ–™
1. é‚±é”¡é¹. ç¥ç»ç½‘ç»œä¸æœºå™¨å­¦ä¹ . 2020.
2. [é˜¿æ–¯é¡¿Â·å¼ ã€ææ²ã€æ‰å¡é‡Œ C. ç«‹é¡¿ã€äºšå†å±±å¤§ J. æ–¯è«æ‹‰ç­‰. åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ . 2020.](https://github.com/d2l-ai/d2l-zh)
3. Christopher M. Bishop. Pattern recognition and machine learning. 2006.
4. Michael Nielsen. Neural network and deep learning. 2016.